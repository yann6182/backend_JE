"""
Script d'import DPGF complet am√©lior√©:
- D√©tection automatique du client
- Import de lots
- Import des sections et sous-sections
- Import des √©l√©ments d'ouvrage
- D√©tection dynamique des colonnes de prix et quantit√©s
- Gestion intelligente des erreurs et des doublons
- Classification avanc√©e avec l'API Google Gemini (optionnelle)
"""

import argparse
import sys
import json
import os
import re
import hashlib
import pickle
from typing import Optional, Dict, List, Tuple, Generator
from datetime import date
from pathlib import Path
import pandas as pd
import requests
from tqdm import tqdm

# Import conditionnel de l'API Gemini
try:
    import google.generativeai as genai
    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False
    print("‚ö†Ô∏è Module google.generativeai non disponible. L'analyse avanc√©e par IA ne sera pas utilis√©e.")

# Configuration de l'encodage pour √©viter les erreurs avec les caract√®res sp√©ciaux
if sys.platform.startswith('win'):
    import codecs
    sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer, 'strict')
    sys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer, 'strict')


class ImportStats:
    """Statistiques d'import"""
    def __init__(self):
        self.total_rows = 0
        self.sections_created = 0
        self.elements_created = 0
        self.sections_reused = 0
        self.lots_created = 0
        self.lots_reused = 0
        self.errors = 0
        self.gemini_calls = 0
        self.cache_hits = 0


class ClientDetector:
    """D√©tecteur automatique du nom du client"""
    
    def __init__(self):
        # Patterns pour extraire le client du nom de fichier
        self.filename_patterns = [
            r'DPGF[_\-\s]*([A-Z][A-Za-z\s&\'\.]+?)[_\-\s]*Lot',
            r'([A-Z][A-Za-z\s&\'\.]+?)[\-_\s]*DPGF',
            r'Client[_\-\s]*([A-Z][A-Za-z\s&\'\.]+)',
            r'([A-Z]{2,}[\s&][A-Z\s\'\.]+)',  # Acronymes + mots
            r'^((?:[A-Z][a-zA-Z\'\.]+\s*)+)',  # S√©quence de mots capitalis√©s au d√©but
            r'[\\/]([A-Z][A-Za-z\s&\'\.]+?)[\\/][^\\\/]+\.xlsx$', # Client dans le chemin du dossier
            r'(?:projet|chantier)[_\-\s]+([A-Z][A-Za-z\s&\'\.]+)',  # Pattern apr√®s "projet" ou "chantier"
            r'_([A-Z][a-z]{2,}(?:[A-Z][a-z]+)+)_', # Nom en camelCase entour√© de underscores
        ]
        
        # Patterns pour d√©tecter un client dans le contenu
        self.content_patterns = [
            r'(?:client|ma√Ætre d\'ouvrage|maitre d\'ouvrage|donneur d\'ordre)[^\w\n]{1,5}([A-Z][A-Za-z\s&\'\.]{2,})',
            r'(?:pour|destin√© √†|r√©alis√© pour)[^\w\n]{1,5}([A-Z][A-ZaZ\s&\'\.]{2,})',
            r'(?:soci√©t√©|entreprise|groupe)[^\w\n]{1,5}([A-Z][A-ZaZ\s&\'\.]{2,})',
            r'^([A-Z][A-z]+(?:[\s\-][A-Z][A-z]+){1,3})\s*$', # Ligne avec uniquement un nom capitalis√©
            r'Projet\s*(?:pour|de|avec)?\s*(?:la|le)?\s*([A-Z][A-Za-z\s&\'\.]{2,})',
            r'Chantier\s*(?:de|pour)?\s*([A-Z][A-ZaZ\s&\'\.]{2,})',
            r'(?:SA|SAS|SARL|GROUP|HABITAT)\s+([A-Z][A-ZaZ\s&\'\.]{2,})',
            r'([A-Z][A-ZaZ\s&\'\.]{2,})\s+(?:SA|SAS|SARL|GROUP|HABITAT)'
        ]
        
        # Mots-cl√©s √† ignorer dans la d√©tection
        self.ignore_words = {'LOT', 'DPGF', 'NOVEMBRE', 'DECEMBRE', 'JANVIER', 'FEVRIER', 'MARS', 'AVRIL', 'MAI', 'JUIN', 
                           'JUILLET', 'AOUT', 'SEPTEMBRE', 'OCTOBRE', 'DCE', 'CONSTRUCTION', 'TRAVAUX', 'BATIMENT',
                           'APPEL', 'OFFRE', 'MARCHE', 'MAITRISE', 'OEUVRE', 'PROJET', 'CHANTIER', 'RENOVATION',
                           'REHABILITATION', 'DEVIS', 'ESTIMATION', 'PRIX', 'DOCUMENT', 'BORDEREAU', 'QUANTITATIF',
                           'REFERENCE', 'DESCRIPTION', 'CODE', 'UNITE', 'TOTAL', 'EUROS', 'EUR', 'HT', 'TTC'}
    
    def detect_from_filename(self, file_path: str) -> Optional[str]:
        """D√©tecte le client depuis le nom de fichier"""
        filename = Path(file_path).stem
        print(f"Analyse du nom de fichier: {filename}")
        
        for pattern in self.filename_patterns:
            match = re.search(pattern, filename, re.IGNORECASE)
            if match:
                client_name = match.group(1).strip()
                # Nettoyer et valider
                client_name = self._clean_client_name(client_name)
                if client_name and len(client_name) > 3:
                    print(f"Client d√©tect√© dans le nom de fichier: {client_name}")
                    return client_name
        
        return None
        
    def detect_from_excel_header(self, file_path: str) -> Optional[str]:
        """D√©tecte le client dans les 15 premi√®res lignes du fichier Excel"""
        try:
            # Lire seulement les premi√®res lignes (augment√© √† 15 pour une meilleure couverture)
            df = pd.read_excel(file_path, engine='openpyxl', nrows=15, header=None)
            
            print("Analyse des premi√®res lignes du fichier...")
            
            # 1. D'abord chercher des mots-cl√©s sp√©cifiques comme "Client:", "Ma√Ætre d'ouvrage:"
            for row_idx in range(min(15, len(df))):
                row_text = " ".join([str(val).strip() for val in df.iloc[row_idx].values if pd.notna(val)])
                
                for pattern in self.content_patterns:
                    match = re.search(pattern, row_text, re.IGNORECASE)
                    if match:
                        client_name = match.group(1).strip()
                        client_name = self._clean_client_name(client_name)
                        if client_name and len(client_name) > 2:
                            print(f"Client d√©tect√© avec pattern sp√©cifique (ligne {row_idx}): {client_name}")
                            return client_name
            
            # 2. Chercher dans toutes les cellules des premi√®res lignes
            for row_idx in range(min(15, len(df))):
                for col_idx in range(min(8, len(df.columns))):  # Augment√© √† 8 colonnes
                    cell_value = df.iloc[row_idx, col_idx]
                    
                    if pd.notna(cell_value):
                        cell_text = str(cell_value).strip()
                        
                        # Chercher des patterns de nom de client
                        client = self._extract_client_from_text(cell_text)
                        if client:
                            print(f"Client d√©tect√© dans la cellule [{row_idx},{col_idx}]: {client}")
                            return client
            
            # 3. Recherche de texte libre avec des noms d'entreprises connus
            known_companies = [
                "CDC HABITAT", "VINCI", "BOUYGUES", "EIFFAGE", "AXA", "BNP", "ICADE", "NEXITY", 
                "KAUFMAN", "COGEDIM", "PITCH", "PICHET", "AMETIS", "ADIM", "SOGEPROM", "MARIGNAN",
                "DEMATHIEU BARD", "ALTAREA", "CREDIT AGRICOLE", "SOCIETE GENERALE", "CARREFOUR", 
                "LECLERC", "AUCHAN", "LEROY MERLIN", "CASTORAMA", "LIDL", "ALDI", "COLAS"
            ]
            
            for row_idx in range(min(15, len(df))):
                row_text = " ".join([str(val).strip() for val in df.iloc[row_idx].values if pd.notna(val)])
                for company in known_companies:
                    if company in row_text.upper():
                        print(f"Entreprise connue d√©tect√©e (ligne {row_idx}): {company}")
                        return company
            
            return None
        except Exception as e:
            print(f"Erreur lors de l'analyse de l'en-t√™te: {e}")
            return None
            
    def _extract_client_from_text(self, text: str) -> Optional[str]:
        """Extrait un nom de client depuis un texte"""
        # Patterns pour identifier un client
        client_patterns = [
            r'^([A-Z]{2,}(?:\s+[A-Z&\'\.]+)*)\s*$',  # Acronymes en majuscules
            r'([A-Z][a-z]+(?:\s+[A-Z][a-z\'\.]+)*)\s*(?:HABITAT|GROUP|COMPANY|SA|SAS|SARL|SCI|IMMOBILIER)',
            r'((?:[A-Z]{2,}\s*)+)(?:HABITAT|GROUP|IMMOBILIER)',  # CDC HABITAT, BNP GROUP, etc.
            r'(?:^|\s)([A-Z][a-zA-Z\'\.]+(?:\s+[A-Z][a-zA-Z\'\.]+){1,3})(?:\s|$)',  # Mots capitalis√©s (2-4 mots)
            r'(?:^|\s)([A-Z]{2,}(?:\s*[A-Z]{2,}){0,2})(?:\s|$)',  # Acronymes 2-3 lettres
            r'(?:soci√©t√©|entreprise|groupe|client|constructeur)\s+([A-Z][a-zA-Z\'\.]+(?:\s+[A-Z][a-zA-Z\'\.]+){0,3})',
            r'[Pp]our\s+(?:le\s+compte\s+de\s+)?([A-Z][a-zA-Z\'\.]+(?:\s+[A-Z][a-zA-Z\'\.]+){0,3})',
            r'[Aa]dresse\s*:?\s*(?:[^,]+,\s*)?([A-Z][a-zA-Z\'\.]+(?:\s+[A-Z][a-zA-Z\'\.]+){1,3})',
            r'(?:^|\n)\s*([A-Z][a-zA-Z\'\.]+(?:\s+[A-Z][a-zA-Z\'\.]+){1,2})\s*(?:$|\n)', # Nom isol√© sur une ligne
        ]
        
        text = text.strip()
        for pattern in client_patterns:
            match = re.search(pattern, text)
            if match:
                client_name = match.group(1).strip()
                client_name = self._clean_client_name(client_name)
                
                # Valider que c'est un vrai nom de client
                if (len(client_name) >= 3 and 
                    not any(word.upper() in self.ignore_words for word in client_name.split()) and
                    any(c.isalpha() for c in client_name) and
                    len([w for w in client_name.split() if len(w) > 1]) > 0):  # Au moins un mot de plus d'une lettre
                    return client_name
        
        # D√©tecter les noms d'entreprises connus m√™me sans pattern
        known_companies = [
            "CDC HABITAT", "VINCI", "BOUYGUES", "EIFFAGE", "AXA", "BNP", "ICADE", "NEXITY", 
            "KAUFMAN", "COGEDIM", "PITCH", "PICHET", "AMETIS", "ADIM", "SOGEPROM", "MARIGNAN",
            "DEMATHIEU BARD", "ALTAREA", "CREDIT AGRICOLE", "SOCIETE GENERALE", "CARREFOUR", 
            "LECLERC", "AUCHAN", "LEROY MERLIN", "CASTORAMA", "LIDL", "ALDI", "COLAS"
        ]
        for company in known_companies:
            if company in text.upper():
                return company
                
        return None
    
    def _clean_client_name(self, name: str) -> str:
        """Nettoie un nom de client"""
        # Supprimer caract√®res ind√©sirables
        name = re.sub(r'[_\-\.]+', ' ', name)
        name = re.sub(r'\s+', ' ', name)
        name = name.strip()
        
        # Supprimer mots parasites
        words = name.split()
        cleaned_words = [w for w in words if w.upper() not in self.ignore_words]
        
        return ' '.join(cleaned_words)
        
    def detect_client(self, file_path: str, include_file_suffix: bool = True) -> Optional[str]:
        """
        D√©tection compl√®te du client (nom de fichier + contenu)
        
        Args:
            file_path: Chemin du fichier Excel
            include_file_suffix: Si True, ajoute un suffixe du nom de fichier au client pour le rendre unique
        """
        print(f"üîç D√©tection automatique du client pour: {file_path}")
        filename = Path(file_path).stem
        
        client_candidates = []
        
        # 1. Essayer depuis le contenu du fichier (priorit√© au contenu car plus fiable)
        client_from_content = self.detect_from_excel_header(file_path)
        if client_from_content:
            client_candidates.append(("contenu", client_from_content))
        
        # 2. Essayer depuis le nom de fichier
        client_from_filename = self.detect_from_filename(file_path)
        if client_from_filename:
            client_candidates.append(("nom_fichier", client_from_filename))
        
        # 3. Essayer avec des mots uniques du nom de fichier
        unique_words = []
        if len(filename) > 5:
            words = re.findall(r'[A-Za-z]{4,}', filename)
            for word in words:
                if word.upper() not in self.ignore_words and word not in ["DPGF", "Lot", "LOT"]:
                    if len(word) >= 4:
                        unique_words.append(word)
        
        # S√©lection du meilleur candidat
        if client_candidates:
            # Priorit√© au client d√©tect√© dans le contenu
            source, client = next((s, c) for s, c in client_candidates if s == "contenu") \
                            if any(s == "contenu" for s, _ in client_candidates) \
                            else client_candidates[0]
            
            # G√©n√©rer un identifiant unique bas√© sur le nom de fichier
            if include_file_suffix:
                # Extraire un identifiant significatif du nom de fichier
                # Pr√©f√©rer un num√©ro de lot s'il existe
                lot_match = re.search(r'Lot\s*(\d+)', filename, re.IGNORECASE)
                if lot_match:
                    file_id = f"Lot{lot_match.group(1)}"
                else:
                    # Sinon utiliser un mot unique ou les premiers caract√®res
                    if unique_words:
                        file_id = unique_words[0][:8]
                    else:
                        file_id = filename.split()[0][:8]  # Premiers caract√®res du premier mot
                
                # Ajouter l'identifiant uniquement s'il n'est pas d√©j√† dans le nom du client
                if file_id.upper() not in client.upper():
                    client = f"{client} ({file_id})"
            
            print(f"‚úì Client d√©tect√© (source: {source}): {client}")
            return client
        
        # 4. Dernier recours: utiliser une partie du nom de fichier
        if unique_words:
            client = unique_words[0].capitalize()
            print(f"‚ö†Ô∏è Utilisation d'un mot unique du nom de fichier comme client: {client}")
            return client
        
        print("‚ö†Ô∏è Aucun client d√©tect√© automatiquement")
        return None


class GeminiCache:
    """Cache intelligent pour les r√©ponses Gemini"""
    
    def __init__(self, cache_dir: str = "cache"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        self.cache_file = self.cache_dir / "gemini_patterns.pkl"
        self.patterns = self._load_cache()
    
    def _load_cache(self) -> Dict:
        """Charge le cache depuis le disque"""
        if self.cache_file.exists():
            try:
                with open(self.cache_file, 'rb') as f:
                    return pickle.load(f)
            except:
                pass
        return {}
    
    def _save_cache(self):
        """Sauvegarde le cache sur disque"""
        with open(self.cache_file, 'wb') as f:
            pickle.dump(self.patterns, f)
    
    def _get_pattern_hash(self, rows: List[str]) -> str:
        """G√©n√®re un hash pour un pattern de lignes"""
        # Normaliser les lignes (enlever espaces, casse)
        normalized = []
        for row in rows:
            normalized.append(''.join(row.lower().split()))
        
        pattern = '|'.join(normalized)
        return hashlib.md5(pattern.encode()).hexdigest()
    
    def get(self, rows: List[str]) -> Optional[List[Dict]]:
        """R√©cup√®re une classification depuis le cache"""
        pattern_hash = self._get_pattern_hash(rows)
        return self.patterns.get(pattern_hash)
    
    def set(self, rows: List[str], classification: List[Dict]):
        """Met en cache une classification"""
        pattern_hash = self._get_pattern_hash(rows)
        self.patterns[pattern_hash] = classification
        self._save_cache()


class ExcelParser:
    """Analyse les fichiers Excel DPGF avec d√©tection de colonnes am√©lior√©e"""
    
    def __init__(self, file_path: str):
        self.file_path = file_path
        self.df = pd.read_excel(file_path, engine='openpyxl', header=None)
        # Colonnes d√©tect√©es (indices)
        self.col_designation = None
        self.col_unite = None
        self.col_quantite = None
        self.col_prix_unitaire = None
        self.col_prix_total = None
        self.headers_detected = False
    
    def find_lot_headers(self) -> List[Tuple[str, str]]:
        """
        Recherche dans les 15 premi√®res lignes les intitul√©s de lot au format
        ¬´ LOT <num√©ro> ‚Äì <libell√©> ¬ª (maj/min indiff√©rent).
        
        Returns:
            Liste de tuples (numero_lot, nom_lot)
        """
        lots = []
        pattern = re.compile(r'lot\s+([^\s‚Äì-]+)\s*[‚Äì-]\s*(.+)', re.IGNORECASE)
        
        # Parcourir les 15 premi√®res lignes
        for i in range(min(15, len(self.df))):
            for col in self.df.columns:
                cell_value = self.df.iloc[i, col]
                if pd.notna(cell_value):
                    cell_str = str(cell_value).strip()
                    match = pattern.search(cell_str)
                    if match:
                        numero_lot = match.group(1).strip()
                        nom_lot = match.group(2).strip()
                        lots.append((numero_lot, nom_lot))
        
        # Si aucun lot trouv√© dans le contenu, essayer depuis le nom de fichier
        if not lots:
            filename_lot = self.extract_lot_from_filename()
            if filename_lot:
                lots.append(filename_lot)
        
        return lots
    
    def extract_lot_from_filename(self) -> Optional[Tuple[str, str]]:
        """
        Extrait le num√©ro et nom du lot depuis le nom du fichier.
        
        Returns:
            Tuple (numero_lot, nom_lot) ou None si non trouv√©
        """
        filename = Path(self.file_path).stem
        
        # Patterns pour d√©tecter un lot dans le nom de fichier
        patterns = [
            # LOT 06 - DPGF - METALLERIE
            r'lot\s*(\d+)\s*-\s*(?:dpgf|devis)\s*-\s*([\w\s-]+)',
            # LOT 06 - METALLERIE
            r'lot\s*(\d+)\s*-\s*([\w\s-]+)',
            # DPGF Lot 6 - M√©tallerie
            r'dpgf\s*lot\s*(\d+)\s*-\s*([\w\s-]+)',
            # Lot06-M√©tallerie
            r'lot\s*(\d+)[_\-\s]+([\w\s-]+)',
            # Lot6
            r'lot\s*(\d+)',
            # 06 M√©tallerie
            r'(\d{1,2})\s+([\w\s-]+)',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, filename, re.IGNORECASE)
            if match:
                try:
                    numero_lot = match.group(1).strip()
                    # Si on a un deuxi√®me groupe de capture, c'est le nom du lot
                    if len(match.groups()) > 1 and match.group(2):
                        nom_lot = match.group(2).strip()
                    else:
                        nom_lot = f"Lot {numero_lot}"
                    
                    print(f"‚úì Lot d√©tect√© depuis le nom du fichier: {numero_lot} - {nom_lot}")
                    return (numero_lot, nom_lot)
                except:
                    pass
                    
        # Essai de derni√®re chance: chercher juste un nombre dans le nom de fichier
        digit_match = re.search(r'(\d{1,2})', filename)
        if digit_match:
            numero = digit_match.group(1)
            if 1 <= int(numero) <= 99:  # Les num√©ros de lot sont g√©n√©ralement entre 1 et 99
                print(f"‚úì Num√©ro de lot d√©tect√© depuis le nom de fichier: {numero}")
                return (numero, f"Lot {numero}")
        
        return None
        
    def detect_project_name(self, client_name: str = None) -> str:
        """
        Extrait le nom de projet avec une strat√©gie intelligente pour garantir l'unicit√©.
        
        Args:
            client_name: Nom du client pour l'inclure dans le nom du projet
            
        Returns:
            Nom du projet (str)
        """
        potential_names = []
        filename = Path(self.file_path).stem
        
        # 1. Chercher dans les premi√®res cellules
        for i in range(min(5, len(self.df))):
            for col in range(min(3, len(self.df.columns))):
                cell_value = self.df.iloc[i, col]
                if pd.notna(cell_value):
                    value = str(cell_value).strip()
                    if len(value) > 5 and not any(w in value.upper() for w in ['DPGF', 'QUANTITATIF', 'BORDEREAU']):
                        potential_names.append(value)
        
        # 2. Extraire des infos pertinentes du nom de fichier
        file_info = []
        
        # Extraire le num√©ro et nom de lot
        lot_match = re.search(r'Lot\s*(\d+)[^\w]*([\w\s\-]+?)(?:\.xlsx|$)', filename, re.IGNORECASE)
        if lot_match:
            lot_num = lot_match.group(1)
            lot_name = lot_match.group(2).strip()
            if lot_name:
                file_info.append(f"Lot {lot_num} - {lot_name}")
            else:
                file_info.append(f"Lot {lot_num}")
        
        # 3. Construire un nom de projet distinctif
        project_parts = []
        
        # Inclure le nom du client s'il est fourni
        if client_name and len(client_name) > 2:
            project_parts.append(client_name)
        
        # Ajouter la meilleure info de contenu 
        if potential_names:
            best_name = max(potential_names, key=len)[:50]  # Limiter la longueur
            if best_name and not any(best_name.lower() in part.lower() for part in project_parts):
                project_parts.append(best_name)
        
        # Ajouter l'info du fichier
        if file_info:
            best_file_info = file_info[0]
            if not any(best_file_info.lower() in part.lower() for part in project_parts):
                project_parts.append(best_file_info)
        
        # Si on n'a pas assez d'infos, utiliser le nom de fichier
        if not project_parts or len(' - '.join(project_parts)) < 10:
            return filename
            
        # Construire le nom final
        return ' - '.join(project_parts)
    
    def find_header_row(self) -> Optional[int]:
        """
        Trouve la ligne d'en-t√™te contenant D√©signation/Quantit√©/Prix unitaire/Prix total.
        
        Returns:
            Index de la ligne d'en-t√™te ou None si non trouv√©e
        """
        # Patterns pour reconna√Ætre les en-t√™tes (fran√ßais et autres variations)
        header_patterns = {
            'designation': [r'd√©signation', r'libell√©', r'description', r'prestation', r'article', r'd√©tail', r'ouvrage', r'intitul√©', r'nature'],
            'unite': [r'unit√©', r'u\.', r'un\.', r'un$', r'unit√© de mesure', r'mesure'],
            'quantite': [r'quantit√©', r'qt√©\.?', r'qt\.?', r'quant\.?', r'qte'],
            'prix_unitaire': [r'prix\s*(?:unitaire|unit\.?)(?:\s*h\.?t\.?)?', r'p\.u\.(?:\s*h\.?t\.?)?', r'pu(?:\s*h\.?t\.?)?'],
            'prix_total': [r'prix\s*(?:total|tot\.?)(?:\s*h\.?t\.?)?', r'montant(?:\s*h\.?t\.?)?', r'p\.t\.(?:\s*h\.?t\.?)?', r'pt(?:\s*h\.?t\.?)?', r'total(?:\s*h\.?t\.?)?']
        }
        
        best_row = None
        best_score = 0
        
        # Parcourir les 30 premi√®res lignes pour chercher les en-t√™tes
        for i in range(min(30, len(self.df))):
            row_values = [str(val).strip().lower() if pd.notna(val) else "" for val in self.df.iloc[i].values]
            row_text = " ".join(row_values)
            
            # Compter le nombre de patterns correspondants dans cette ligne
            score = 0
            found_patterns = {k: False for k in header_patterns.keys()}
            
            for col_name, patterns in header_patterns.items():
                # Chercher chaque pattern dans toute la ligne d'abord
                for pattern in patterns:
                    if re.search(pattern, row_text, re.IGNORECASE):
                        found_patterns[col_name] = True
                        score += 1
                        break
                
                # Si le pattern n'est pas trouv√© dans la ligne enti√®re, chercher dans chaque cellule
                if not found_patterns[col_name]:
                    for col_idx, cell_text in enumerate(row_values):
                        for pattern in patterns:
                            if re.search(f"^{pattern}$", cell_text, re.IGNORECASE):
                                found_patterns[col_name] = True
                                score += 1
                                break
                        if found_patterns[col_name]:
                            break
            
            # Si on a trouv√© au moins 3 des 5 en-t√™tes attendus, c'est probablement la bonne ligne
            if score >= 3:
                if score > best_score:
                    best_score = score
                    best_row = i
            
            # Si on a trouv√© tous les en-t√™tes, on arr√™te la recherche
            if score == 5:
                print(f"‚úì Ligne d'en-t√™te trouv√©e (ligne {i+1}): score parfait")
                return i
        
        if best_row is not None:
            print(f"‚úì Ligne d'en-t√™te trouv√©e (ligne {best_row+1}): score {best_score}/5")
        else:
            print("‚ö†Ô∏è Aucune ligne d'en-t√™te trouv√©e, l'analyse pourrait √™tre moins pr√©cise")
            
        return best_row
    
    def detect_column_indices(self, header_row_idx: Optional[int]) -> Dict[str, Optional[int]]:
        """
        D√©termine l'indice des colonnes importantes en se basant sur l'en-t√™te
        
        Args:
            header_row_idx: Indice de la ligne d'en-t√™te
            
        Returns:
            Dictionnaire avec les indices des colonnes
        """
        # Initialiser les indices √† None
        column_indices = {
            'designation': None,
            'unite': None,
            'quantite': None,
            'prix_unitaire': None,
            'prix_total': None
        }
        
        # Si on n'a pas d'en-t√™te, on essaie une approche par d√©faut
        if header_row_idx is None:
            print("‚ö†Ô∏è Utilisation des indices de colonne par d√©faut")
            column_indices['designation'] = 0
            
            # Pour les autres colonnes, on examine quelques lignes pour trouver des nombres
            num_rows = min(20, len(self.df))
            num_cols = min(10, len(self.df.columns))
            
            # Compter combien de valeurs num√©riques on a dans chaque colonne
            num_counts = {col: 0 for col in range(1, num_cols)}
            for row in range(5, num_rows):  # Commencer apr√®s les potentiels en-t√™tes
                for col in range(1, num_cols):
                    if pd.notna(self.df.iloc[row, col]):
                        try:
                            # Tester si la valeur est num√©rique (entier ou d√©cimal)
                            val_str = str(self.df.iloc[row, col]).replace(',', '.')
                            float(val_str)
                            num_counts[col] += 1
                        except ValueError:
                            pass
            
            # Les colonnes avec le plus de valeurs num√©riques sont probablement quantit√©/prix
            numeric_cols = sorted([(col, count) for col, count in num_counts.items() if count > 3], 
                                  key=lambda x: x[1], reverse=True)
            
            if len(numeric_cols) >= 3:
                # Supposer un ordre typique: unit√©, quantit√©, prix unitaire, prix total
                column_indices['unite'] = 1  # Souvent juste avant les nombres
                column_indices['quantite'] = numeric_cols[0][0]
                column_indices['prix_unitaire'] = numeric_cols[1][0]
                column_indices['prix_total'] = numeric_cols[2][0]
            elif len(numeric_cols) == 2:
                # Au minimum, on a besoin de quantit√© et prix
                column_indices['quantite'] = numeric_cols[0][0]
                column_indices['prix_unitaire'] = numeric_cols[1][0]
            
            print(f"D√©tection colonnes par d√©faut: {column_indices}")
            return column_indices
        
        # Si on a un en-t√™te, on cherche les correspondances avec des patterns connus
        header_row = [str(val).strip().lower() if pd.notna(val) else "" for val in self.df.iloc[header_row_idx].values]
        
        # Patterns pour chaque type de colonne
        patterns = {
            'designation': [r'd√©signation', r'libell√©', r'description', r'prestation', r'article', r'd√©tail', r'ouvrage', r'intitul√©', r'nature'],
            'unite': [r'unit√©', r'u\.', r'un\.', r'un$', r'unit√© de mesure', r'mesure'],
            'quantite': [r'quantit√©', r'qt√©\.?', r'qt\.?', r'quant\.?', r'qte'],
            'prix_unitaire': [r'prix\s*(?:unitaire|unit\.?)(?:\s*h\.?t\.?)?', r'p\.u\.(?:\s*h\.?t\.?)?', r'pu(?:\s*h\.?t\.?)?'],
            'prix_total': [r'prix\s*(?:total|tot\.?)(?:\s*h\.?t\.?)?', r'montant(?:\s*h\.?t\.?)?', r'p\.t\.(?:\s*h\.?t\.?)?', r'pt(?:\s*h\.?t\.?)?', r'total(?:\s*h\.?t\.?)?']
        }
        
        # Chercher chaque pattern dans les cellules de la ligne d'en-t√™te
        for col_name, col_patterns in patterns.items():
            for col_idx, cell_text in enumerate(header_row):
                cell_text = cell_text.lower()
                for pattern in col_patterns:
                    if re.search(pattern, cell_text, re.IGNORECASE):
                        column_indices[col_name] = col_idx
                        print(f"Colonne '{col_name}' d√©tect√©e: indice {col_idx}, valeur: '{cell_text}'")
                        break
                if column_indices[col_name] is not None:
                    break
        
        # Si la d√©signation n'est pas trouv√©e, on peut supposer que c'est la premi√®re colonne
        if column_indices['designation'] is None:
            column_indices['designation'] = 0
            print(f"‚ö†Ô∏è Colonne 'designation' non d√©tect√©e, suppos√©e √™tre √† l'indice 0")
        
        # V√©rifications et inf√©rences des colonnes non d√©tect√©es
        # Si on a trouv√© prix unitaire et quantit√© mais pas prix total, on cherche apr√®s prix unitaire
        if column_indices['prix_unitaire'] is not None and column_indices['quantite'] is not None and column_indices['prix_total'] is None:
            if column_indices['prix_unitaire'] + 1 < len(header_row):
                column_indices['prix_total'] = column_indices['prix_unitaire'] + 1
                print(f"‚ö†Ô∏è Colonne 'prix_total' non d√©tect√©e, suppos√©e √™tre √† l'indice {column_indices['prix_total']}")
        
        # Si on a trouv√© prix total et quantit√© mais pas prix unitaire, on cherche avant prix total
        if column_indices['prix_total'] is not None and column_indices['quantite'] is not None and column_indices['prix_unitaire'] is None:
            if column_indices['prix_total'] - 1 >= 0:
                column_indices['prix_unitaire'] = column_indices['prix_total'] - 1
                print(f"‚ö†Ô∏è Colonne 'prix_unitaire' non d√©tect√©e, suppos√©e √™tre √† l'indice {column_indices['prix_unitaire']}")
        
        # Si on a trouv√© prix unitaire mais pas quantit√©, on cherche juste avant
        if column_indices['prix_unitaire'] is not None and column_indices['quantite'] is None:
            if column_indices['prix_unitaire'] - 1 >= 0:
                column_indices['quantite'] = column_indices['prix_unitaire'] - 1
                print(f"‚ö†Ô∏è Colonne 'quantite' non d√©tect√©e, suppos√©e √™tre √† l'indice {column_indices['quantite']}")
        
        # Si l'unit√© n'est pas trouv√©e, on peut supposer qu'elle est entre d√©signation et quantit√©
        if column_indices['unite'] is None and column_indices['designation'] is not None and column_indices['quantite'] is not None:
            if column_indices['quantite'] > column_indices['designation'] + 1:
                column_indices['unite'] = column_indices['quantite'] - 1
                print(f"‚ö†Ô∏è Colonne 'unite' non d√©tect√©e, suppos√©e √™tre √† l'indice {column_indices['unite']}")
        
        # Afficher les r√©sultats de la d√©tection
        print(f"‚úì Indices des colonnes d√©tect√©s: {column_indices}")
        
        # Stocker dans l'instance
        self.col_designation = column_indices['designation']
        self.col_unite = column_indices['unite']
        self.col_quantite = column_indices['quantite']
        self.col_prix_unitaire = column_indices['prix_unitaire']
        self.col_prix_total = column_indices['prix_total']
        self.headers_detected = True
        
        return column_indices
    
    def safe_convert_to_float(self, value) -> float:
        """
        Convertit une valeur en float de fa√ßon s√©curis√©e, en g√©rant les formats europ√©ens.
        
        Args:
            value: Valeur √† convertir (str, int, float, etc.)
            
        Returns:
            Valeur convertie en float, ou 0.0 si erreur
        """
        if pd.isna(value):
            return 0.0
        
        if isinstance(value, (int, float)):
            return float(value)
        
        try:
            # Nettoyer la valeur
            val_str = str(value).strip()
            
            # Supprimer les symboles mon√©taires et autres caract√®res
            val_str = re.sub(r'[‚Ç¨$¬£\s]', '', val_str)
            
            # Remplacer les virgules par des points (format europ√©en)
            if ',' in val_str and '.' not in val_str:
                val_str = val_str.replace(',', '.')
            
            # Traiter les cas comme "1.234,56" (format europ√©en) -> "1234.56"
            if '.' in val_str and ',' in val_str:
                if val_str.find('.') < val_str.find(','):
                    val_str = val_str.replace('.', '')
                    val_str = val_str.replace(',', '.')
            
            # Convertir en float
            return float(val_str)
        except (ValueError, TypeError):
            # Si la conversion √©choue, on retourne 0
            print(f"‚ö†Ô∏è Impossible de convertir en nombre: '{value}'")
            return 0.0
    
    def detect_sections_and_elements(self, header_row: Optional[int] = None) -> List[Dict]:
        """
        D√©tecte les sections et √©l√©ments d'ouvrage √† partir de la ligne d'en-t√™te.
        Cette version utilise la d√©tection dynamique des colonnes.
        
        Args:
            header_row: Index de la ligne d'en-t√™te
            
        Returns:
            Liste de dictionnaires avec 'type' ('section' ou 'element') et 'data'
        """
        results = []
        
        # Trouver la ligne d'en-t√™te si non sp√©cifi√©e
        if header_row is None:
            header_row = self.find_header_row()
        
        # D√©tecter les indices des colonnes
        if not self.headers_detected:
            self.detect_column_indices(header_row)
        
        # Si on n'a pas pu d√©tecter les colonnes essentielles, on utilise des valeurs par d√©faut
        if self.col_designation is None:
            self.col_designation = 0
        
        print(f"Colonnes utilis√©es: d√©signation={self.col_designation}, unit√©={self.col_unite}, "
              f"quantit√©={self.col_quantite}, prix unitaire={self.col_prix_unitaire}, prix total={self.col_prix_total}")
        
        section_pattern = re.compile(r'^(\d+(?:\.\d+)*)\s+(.*)')
        title_pattern = re.compile(r'^([A-Z][A-Z\s\d\.]+)$')  # Pour les titres en majuscules sans num√©ro
        current_section = None
        
        # Si header_row est None (pas trouv√©), commencer depuis le d√©but
        start_row = header_row + 1 if header_row is not None else 0
        
        for i in range(start_row, len(self.df)):
            row = self.df.iloc[i]
            
            # Ignorer les lignes vides
            if all(pd.isna(val) for val in row.values):
                continue
            
            # V√©rifier si c'est une section (texte en d√©but de ligne avec num√©ro ou en majuscules)
            if pd.notna(row.iloc[self.col_designation]):
                cell_text = str(row.iloc[self.col_designation]).strip()
                
                # Essayer de d√©tecter une section avec num√©ro (ex: "1.2 Section Title")
                match = section_pattern.match(cell_text)
                
                if match:
                    numero_section = match.group(1).strip()
                    titre_section = match.group(2).strip()
                    
                    # Calculer le niveau hi√©rarchique
                    niveau = numero_section.count('.') + 1
                    
                    # Stocker la section
                    current_section = {
                        'numero_section': numero_section,
                        'titre_section': titre_section,
                        'niveau_hierarchique': niveau
                    }
                    
                    results.append({
                        'type': 'section',
                        'data': current_section,
                        'row': i
                    })
                    continue
                
                # Deuxi√®me test: est-ce un titre en majuscules sans num√©ro?
                if len(cell_text) > 3 and title_pattern.match(cell_text):
                    # C'est probablement un titre de section en majuscules
                    titre_section = cell_text
                    
                    # G√©n√©rer un num√©ro unique pour cette section
                    # En utilisant un hash simple pour avoir un num√©ro unique
                    section_hash = abs(hash(titre_section)) % 10000
                    numero_section = f"S{section_hash}"
                    
                    # Stocker la section
                    current_section = {
                        'numero_section': numero_section,
                        'titre_section': titre_section,
                        'niveau_hierarchique': 1  # Section de premier niveau par d√©faut
                    }
                    
                    results.append({
                        'type': 'section',
                        'data': current_section,
                        'row': i
                    })
                    continue
                
                # Si on a une section active, et qu'il y a des donn√©es de prix et quantit√©, c'est un √©l√©ment
                if current_section:
                    has_price_data = False
                    if self.col_prix_total is not None and self.col_prix_total < len(row) and pd.notna(row.iloc[self.col_prix_total]):
                        has_price_data = True
                    elif self.col_prix_unitaire is not None and self.col_quantite is not None:
                        if (self.col_prix_unitaire < len(row) and pd.notna(row.iloc[self.col_prix_unitaire]) and 
                            self.col_quantite < len(row) and pd.notna(row.iloc[self.col_quantite])):
                            has_price_data = True
                    
                    if has_price_data:
                        # C'est un √©l√©ment
                        designation = cell_text
                        
                        # R√©cup√©rer l'unit√© si disponible
                        unite = ""
                        if self.col_unite is not None and self.col_unite < len(row) and pd.notna(row.iloc[self.col_unite]):
                            unite = str(row.iloc[self.col_unite])
                        
                        # R√©cup√©rer quantit√© et prix avec conversion s√©curis√©e
                        quantite = 0.0
                        if self.col_quantite is not None and self.col_quantite < len(row) and pd.notna(row.iloc[self.col_quantite]):
                            quantite = self.safe_convert_to_float(row.iloc[self.col_quantite])
                        
                        prix_unitaire = 0.0
                        if self.col_prix_unitaire is not None and self.col_prix_unitaire < len(row) and pd.notna(row.iloc[self.col_prix_unitaire]):
                            prix_unitaire = self.safe_convert_to_float(row.iloc[self.col_prix_unitaire])
                        
                        prix_total = 0.0
                        if self.col_prix_total is not None and self.col_prix_total < len(row) and pd.notna(row.iloc[self.col_prix_total]):
                            prix_total = self.safe_convert_to_float(row.iloc[self.col_prix_total])
                        elif quantite > 0 and prix_unitaire > 0:
                            # Calculer le prix total si non disponible
                            prix_total = quantite * prix_unitaire
                        
                        # Si prix total est disponible mais pas prix unitaire ou quantit√©, essayer de calculer
                        if prix_total > 0:
                            if prix_unitaire == 0 and quantite > 0:
                                prix_unitaire = prix_total / quantite
                            elif quantite == 0 and prix_unitaire > 0:
                                quantite = prix_total / prix_unitaire
                        
                        results.append({
                            'type': 'element',
                            'data': {
                                'designation_exacte': designation,
                                'unite': unite[:10],  # Limiter √† 10 caract√®res
                                'quantite': quantite,
                                'prix_unitaire_ht': prix_unitaire,
                                'prix_total_ht': prix_total,
                            },
                            'row': i
                        })
                        continue
        
        print(f"Total √©l√©ments/sections d√©tect√©s: {len(results)}")
        return results


class GeminiProcessor:
    """Traitement des donn√©es avec l'API Gemini"""
    
    def __init__(self, api_key: str, chunk_size: int = 20):
        if not GEMINI_AVAILABLE:
            raise ImportError("Le module google.generativeai n'est pas disponible")
        
        self.api_key = api_key
        self.chunk_size = chunk_size
        self.cache = GeminiCache()
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel('gemini-1.5-flash')
        self.stats = ImportStats()
    
    def read_excel_chunks(self, file_path: str) -> Generator[pd.DataFrame, None, None]:
        """Lit un fichier Excel par chunks pour √©conomiser la m√©moire"""
        print(f"Lecture du fichier par chunks de {self.chunk_size} lignes...")
        
        # Lire par chunks
        skip_rows = 0
        while True:
            try:
                chunk = pd.read_excel(
                    file_path, 
                    engine='openpyxl',
                    skiprows=range(1, skip_rows + 1) if skip_rows > 0 else None,
                    nrows=self.chunk_size
                )
                
                if chunk.empty:
                    break
                
                yield chunk
                skip_rows += self.chunk_size
                
            except Exception as e:
                print(f"Erreur lecture chunk √† partir de la ligne {skip_rows}: {e}")
                break
    
    def classify_chunk(self, df_chunk: pd.DataFrame, chunk_offset: int = 0) -> List[Dict]:
        """Classifie un chunk avec Gemini + cache"""
        
        # Pr√©parer les donn√©es du chunk
        chunk_rows = []
        for i, row in df_chunk.iterrows():
            row_values = [str(val) if pd.notna(val) else "" for val in row.values]
            if any(val.strip() for val in row_values):
                chunk_rows.append(f"Ligne {chunk_offset + i}: {row_values}")
        
        if not chunk_rows:
            return []
        
        # V√©rifier le cache
        cached_result = self.cache.get(chunk_rows)
        if cached_result:
            self.stats.cache_hits += 1
            print(f"Cache hit pour chunk de {len(chunk_rows)} lignes")
            # Ajuster les num√©ros de ligne pour le chunk actuel
            for item in cached_result:
                item['row'] += chunk_offset
            return cached_result
        
        # Appel Gemini si pas en cache
        self.stats.gemini_calls += 1
        result = self._call_gemini_api(chunk_rows, chunk_offset)
        
        if result:
            # Mettre en cache (avec les num√©ros de ligne relatifs)
            cache_result = []
            for item in result:
                cache_item = item.copy()
                cache_item['row'] -= chunk_offset  
                cache_result.append(cache_item)
            self.cache.set(chunk_rows, cache_result)
        
        return result
    
    def _call_gemini_api(self, chunk_rows: List[str], chunk_offset: int) -> List[Dict]:
        """Appel direct √† l'API Gemini"""
        data_text = "\n".join(chunk_rows)
        
        prompt = f"""
        Analyse ce chunk de fichier Excel DPGF ligne par ligne.
        
        Classifie chaque ligne comme :
        - "section" : Titre de section (ex: "2.9 FERRURES", "LOT 06")
        - "element" : √âl√©ment d'ouvrage avec prix/quantit√© 
        - "ignore" : Ligne vide, en-t√™te, non pertinente
        
        Pour les SECTIONS, extrais :
        - numero_section, titre_section, niveau_hierarchique
        
        Pour les ELEMENTS, extrais :
        - designation_exacte (OBLIGATOIRE - m√™me si vide mettre "Description manquante")
        - unite, quantite, prix_unitaire_ht, prix_total_ht
        
        Donn√©es :
        {data_text}
        
        R√©ponds en JSON : [{{"row": N, "type": "section|element|ignore", "data": {{...}}}}]
        """
        
        try:
            response = self.model.generate_content(prompt)
            response_text = response.text.strip()
            
            # Nettoyer le JSON
            if response_text.startswith('```json'):
                response_text = response_text.split('```json')[1].split('```')[0]
            elif response_text.startswith('```'):
                response_text = response_text.split('```')[1]
            
            result = json.loads(response_text.strip())
            print(f"Gemini a classifi√© {len(result)} lignes du chunk")
            return result
            
        except Exception as e:
            print(f"Erreur Gemini pour chunk: {e}")
            return []


class DPGFImporter:
    """Importeur complet de DPGF avec d√©tection intelligente des colonnes"""
    
    def __init__(self, base_url: str = "http://127.0.0.1:8000", gemini_key: Optional[str] = None, 
                 use_gemini: bool = False, chunk_size: int = 20, debug: bool = False):
        self.base_url = base_url
        self.stats = ImportStats()
        self.gemini_key = gemini_key
        self.use_gemini = use_gemini and gemini_key and GEMINI_AVAILABLE
        self.chunk_size = chunk_size
        self.debug = debug
        
        # Initialiser le processeur Gemini si demand√©
        self.gemini = None
        if self.use_gemini:
            try:
                self.gemini = GeminiProcessor(api_key=gemini_key, chunk_size=chunk_size)
                print(f"‚úÖ Mode IA activ√©: classification avec Gemini (chunks de {chunk_size} lignes)")
            except Exception as e:
                print(f"‚ùå Erreur initialisation Gemini: {e}")
                self.use_gemini = False
    
    def get_or_create_client(self, client_name: str) -> int:
        """R√©cup√®re ou cr√©e un client dans l'API"""
        if not client_name:
            raise ValueError("Nom de client requis")
        
        # 1. Essayer de trouver le client existant
        try:
            response = requests.get(f"{self.base_url}/api/v1/clients")
            response.raise_for_status()
            
            clients = response.json()
            for client in clients:
                if client.get('nom_client', '').upper() == client_name.upper():
                    print(f"‚úÖ Client existant trouv√©: {client_name} (ID: {client['id_client']})")
                    return client['id_client']
        
        except Exception as e:
            print(f"Erreur lors de la recherche de clients: {e}")
        
        # 2. Cr√©er le client s'il n'existe pas
        try:            
            # Utiliser 'nom_client' comme attendu par le sch√©ma ClientCreate
            client_payload = {
                'nom_client': client_name,
            }
            
            response = requests.post(f"{self.base_url}/api/v1/clients", json=client_payload)
            response.raise_for_status()
            
            client_id = response.json()['id_client']
            print(f"‚úÖ Nouveau client cr√©√©: {client_name} (ID: {client_id})")
            return client_id
            
        except Exception as e:
            print(f"‚ùå Erreur cr√©ation client {client_name}: {e}")
            raise
            
    def get_or_create_dpgf(self, client_id: int, nom_projet: str, file_path: str) -> int:
        """R√©cup√®re ou cr√©e un DPGF pour le client"""
        fichier_source = Path(file_path).name
        
        # 1. Chercher DPGF existant UNIQUEMENT par fichier source exact
        try:
            response = requests.get(f"{self.base_url}/api/v1/dpgf", params={'id_client': client_id})
            response.raise_for_status()
            
            dpgfs = response.json()
            # Recherche UNIQUEMENT par fichier source pour √©viter les confusions
            for dpgf in dpgfs:
                if dpgf.get('fichier_source') == fichier_source:
                    print(f"‚úÖ DPGF existant trouv√© (fichier source identique): {dpgf['nom_projet']} (ID: {dpgf['id_dpgf']})")
                    return dpgf['id_dpgf']
                    
            print(f"üÜï Aucun DPGF existant trouv√© pour le fichier {fichier_source}. Cr√©ation d'un nouveau DPGF.")
        
        except Exception as e:
            print(f"Erreur lors de la recherche de DPGF: {e}")
        
        # 2. Cr√©er nouveau DPGF (toujours cr√©er un nouveau pour chaque fichier unique)
        try:
            # S'assurer que le nom du projet est unique en ajoutant le nom du fichier
            if fichier_source not in nom_projet:
                nom_projet_unique = f"{nom_projet} - {fichier_source}"
            else:
                nom_projet_unique = nom_projet
                
            # Adapter le payload au sch√©ma DPGFCreate attendu
            dpgf_payload = {
                'id_client': client_id,
                'nom_projet': nom_projet_unique,
                'date_dpgf': date.today().isoformat(),
                'statut_offre': 'en_cours',
                'fichier_source': fichier_source
            }
            
            response = requests.post(f"{self.base_url}/api/v1/dpgf", json=dpgf_payload)
            response.raise_for_status()
            
            dpgf_id = response.json()['id_dpgf']
            print(f"‚úÖ Nouveau DPGF cr√©√©: {nom_projet} (ID: {dpgf_id})")
            return dpgf_id
            
        except Exception as e:
            print(f"‚ùå Erreur cr√©ation DPGF {nom_projet}: {e}")
            raise
    
    def get_or_create_lot(self, dpgf_id: int, numero_lot: str, nom_lot: str) -> int:
        """R√©cup√®re ou cr√©e un lot dans l'API"""
        # 1. V√©rifier si le lot existe d√©j√†
        try:
            response = requests.get(f"{self.base_url}/api/v1/lots", params={'id_dpgf': dpgf_id})
            response.raise_for_status()
            
            lots = response.json()
            for lot in lots:
                if lot.get('numero_lot') == numero_lot:
                    print(f"üîÑ Lot existant r√©utilis√©: {numero_lot} - {lot.get('nom_lot')}")
                    self.stats.lots_reused += 1
                    return lot['id_lot']
        
        except Exception as e:
            print(f"Erreur lors de la recherche de lots: {e}")
        
        # 2. Cr√©er le lot s'il n'existe pas
        try:
            lot_payload = {
                'id_dpgf': dpgf_id,
                'numero_lot': numero_lot,
                'nom_lot': nom_lot
            }
            
            response = requests.post(f"{self.base_url}/api/v1/lots", json=lot_payload)
            response.raise_for_status()
            
            lot_id = response.json()['id_lot']
            print(f"‚úÖ Nouveau lot cr√©√©: {numero_lot} - {nom_lot} (ID: {lot_id})")
            self.stats.lots_created += 1
            return lot_id
            
        except Exception as e:
            print(f"‚ùå Erreur cr√©ation lot {numero_lot}: {e}")
            raise
    
    def create_section(self, lot_id: int, section_data: Dict) -> int:
        """Cr√©e une section unique ou la r√©cup√®re si elle existe d√©j√†"""
        numero = section_data.get('numero_section', '')
        niveau_hierarchique = section_data.get('niveau_hierarchique', 1)
        
        # 1. V√©rifier si une section avec ce num√©ro existe d√©j√† dans ce lot
        try:
            response = requests.get(f"{self.base_url}/api/v1/sections", params={'lot_id': lot_id})
            response.raise_for_status()
            
            sections = response.json()
            for section in sections:
                if section.get('numero_section') == numero:
                    print(f"üîÑ Section existante r√©utilis√©e: {numero} - {section.get('titre_section')}")
                    self.stats.sections_reused += 1
                    return section['id_section']
        except Exception as e:
            print(f"Erreur lors de la recherche de section existante: {e}")
        
        # 2. Cr√©er la section si elle n'existe pas
        payload = {
            'id_lot': lot_id,
            'section_parent_id': None,
            'numero_section': numero,
            'titre_section': section_data.get('titre_section', ''),
            'niveau_hierarchique': niveau_hierarchique
        }
        
        try:
            response = requests.post(f"{self.base_url}/api/v1/sections", json=payload)
            response.raise_for_status()
            section_id = response.json()['id_section']
            print(f"‚ûï Nouvelle section cr√©√©e: {numero} - {section_data.get('titre_section')}")
            self.stats.sections_created += 1
            return section_id
        except requests.exceptions.HTTPError as e:
            if e.response.status_code == 500:
                error_details = e.response.text
                print(f"Erreur 500 d√©taill√©e pour la section: {error_details}")
            self.stats.errors += 1
            raise
    
    def create_element(self, section_id: int, element_data: Dict):
        """Cr√©e un √©l√©ment d'ouvrage"""
        # Afficher les valeurs en mode debug pour diagnostiquer les probl√®mes
        if self.debug:
            print(f"DEBUG - Donn√©es √©l√©ment: {element_data}")
        
        # Nettoyage des donn√©es avec gestion des valeurs nulles
        def safe_float(value, default=0.0):
            if value is None:
                return default
            try:
                return float(value)
            except (ValueError, TypeError):
                return default
        
        # Nettoyer et afficher les valeurs num√©riques pour debug
        quantite = safe_float(element_data.get('quantite'))
        prix_unitaire = safe_float(element_data.get('prix_unitaire_ht'))
        prix_total = safe_float(element_data.get('prix_total_ht'))
        
        if self.debug:
            print(f"DEBUG - Valeurs num√©riques converties: quantit√©={quantite}, PU={prix_unitaire}, PT={prix_total}")
        
        cleaned_data = {
            'id_section': section_id,
            'designation_exacte': element_data.get('designation_exacte', 'Description manquante'),
            'unite': str(element_data.get('unite', ''))[:10],
            'quantite': quantite,
            'prix_unitaire_ht': prix_unitaire,
            'prix_total_ht': prix_total,
            'offre_acceptee': False
        }
        
        try:
            response = requests.post(f"{self.base_url}/api/v1/element_ouvrages", json=cleaned_data)
            response.raise_for_status()
            self.stats.elements_created += 1
            return response.json()
        except requests.exceptions.HTTPError as e:
            self.stats.errors += 1
            print(f"‚ùå Erreur cr√©ation √©l√©ment: {e}")
            if e.response.status_code == 500:
                error_details = e.response.text
                print(f"Erreur 500 d√©taill√©e: {error_details}")
            raise    
            
    def classify_with_gemini(self, description: str) -> str:
        """
        Classifie une description d'√©l√©ment d'ouvrage en utilisant l'API Google Gemini.
        
        Args:
            description: Description de l'√©l√©ment √† classifier
        
        Returns:
            R√©sultat de la classification (texte)
        """
        if not GEMINI_AVAILABLE:
            raise RuntimeError("API Google Gemini non disponible")
        
        try:
            # Appel √† l'API Gemini pour la classification en utilisant GenerativeModel
            model = genai.GenerativeModel('gemini-1.5-flash')
            prompt = f"Classifie l'√©l√©ment de construction suivant dans une cat√©gorie appropri√©e (max 3 mots): {description}"
            
            response = model.generate_content(prompt, 
                generation_config=genai.GenerationConfig(
                    temperature=0.2,
                    max_output_tokens=60,
                    top_p=0.95,
                    top_k=40
                )
            )
            
            # Extraire et retourner le r√©sultat
            if response and hasattr(response, 'text'):
                result = response.text.strip()
                print(f"üîç Classification Gemini: {result}")
                return result
            
            return "Classification inconnue"
        
        except Exception as e:
            print(f"‚ùå Erreur classification Gemini: {e}")
            return "Erreur classification"
            
    def import_file(self, file_path: str, dpgf_id: Optional[int] = None, lot_num: Optional[str] = None):
        """Import complet d'un fichier DPGF Excel"""
        print(f"\nüìÅ Import DPGF: {file_path}")
        print(f"Mode traitement: {'Gemini AI' if self.use_gemini else 'Analyse classique'}")
        print(f"Mode debug: {'Activ√©' if self.debug else 'D√©sactiv√©'}")
        
        try:
            parser = ExcelParser(file_path)
            client_detector = ClientDetector()
            
            # 1. D√©tecter le client si n√©cessaire
            client_name = client_detector.detect_client(file_path)
            if not client_name:
                client_name = "Client par d√©faut"
                print(f"‚ö†Ô∏è Utilisation d'un nom de client par d√©faut: {client_name}")
            
            client_id = self.get_or_create_client(client_name)
            
            # 2. Obtenir ou cr√©er le DPGF
            if not dpgf_id:
                project_name = parser.detect_project_name(client_name)
                dpgf_id = self.get_or_create_dpgf(client_id, project_name, file_path)
            
            # 3. D√©tecter ou utiliser le lot sp√©cifi√©
            if lot_num:
                # Utiliser un nom g√©n√©rique pour le lot sp√©cifi√©
                lot_id = self.get_or_create_lot(dpgf_id, lot_num, f"Lot {lot_num}")
            else:
                # Rechercher dans le fichier
                lots = parser.find_lot_headers()
                if not lots:
                    raise ValueError("Aucun lot trouv√© dans le fichier et aucun num√©ro de lot sp√©cifi√©")
                
                # Utiliser le premier lot trouv√©
                lot_num, lot_name = lots[0]
                lot_id = self.get_or_create_lot(dpgf_id, lot_num, lot_name)
            
            # 4. Extraire les sections et √©l√©ments
            if self.use_gemini:
                # Utiliser Gemini pour la classification avanc√©e
                items = []
                for chunk_num, df_chunk in enumerate(self.gemini.read_excel_chunks(file_path)):
                    print(f"\nTraitement chunk {chunk_num + 1} (lignes {chunk_num*self.chunk_size}-{chunk_num*self.chunk_size + len(df_chunk)})")
                    classified_rows = self.gemini.classify_chunk(df_chunk, chunk_num*self.chunk_size)
                    items.extend(classified_rows)
                    
                # Mettre √† jour les statistiques depuis Gemini
                self.stats.cache_hits = self.gemini.stats.cache_hits
                self.stats.gemini_calls = self.gemini.stats.gemini_calls
            else:
                # Utiliser l'analyse classique avec d√©tection automatique des colonnes
                header_row = parser.find_header_row()
                
                if header_row is None:
                    print("‚ö†Ô∏è Impossible de trouver les en-t√™tes du tableau DPGF")
                
                # D√©tecter les colonnes
                col_indices = parser.detect_column_indices(header_row)
                
                # En mode debug, on affiche les premi√®res lignes du fichier pour aider au diagnostic
                if self.debug:
                    print("\nDEBUG - Aper√ßu des 15 premi√®res lignes:")
                    for i in range(min(15, len(parser.df))):
                        row_values = [str(val) if pd.notna(val) else "" for val in parser.df.iloc[i].values]
                        print(f"Ligne {i}: {row_values}")
                    
                    if header_row is not None:
                        print(f"\nDEBUG - Ligne d'en-t√™te (ligne {header_row}):")
                        header_values = [str(val) if pd.notna(val) else "" for val in parser.df.iloc[header_row].values]
                        print(f"Valeurs: {header_values}")
                
                # Utiliser la nouvelle m√©thode de d√©tection des sections et √©l√©ments
                items = parser.detect_sections_and_elements(header_row)
            
            # Filtrer les items ignor√©s
            items = [item for item in items if item.get('type') != 'ignore']
            print(f"üîç {len(items)} items d√©tect√©s ({sum(1 for i in items if i.get('type') == 'section')} sections, {sum(1 for i in items if i.get('type') == 'element')} √©l√©ments)")
            
            # 5. Traiter les items
            current_section_id = None
            for item in tqdm(items, desc="Import"):
                self.stats.total_rows += 1
                
                if item['type'] == 'section':
                    try:
                        current_section_id = self.create_section(lot_id, item['data'])
                    except Exception as e:
                        print(f"‚ùå Erreur section ligne {item['row']}: {e}")
                        self.stats.errors += 1
                
                elif item['type'] == 'element' and current_section_id:
                    try:
                        # Classification avanc√©e avec Gemini
                        if GEMINI_AVAILABLE:
                            description = item['data'].get('designation_exacte', '')
                            classification = self.classify_with_gemini(description)
                            item['data']['classification'] = classification
                        
                        self.create_element(current_section_id, item['data'])
                    except Exception as e:
                        print(f"‚ùå Erreur √©l√©ment ligne {item['row']}: {e}")
                        self.stats.errors += 1
            
            # 6. Afficher les statistiques
            print(f"\n‚úÖ Import termin√©:")
            print(f"   - Lots cr√©√©s: {self.stats.lots_created}, r√©utilis√©s: {self.stats.lots_reused}")
            print(f"   - Sections cr√©√©es: {self.stats.sections_created}, r√©utilis√©es: {self.stats.sections_reused}")
            print(f"   - √âl√©ments cr√©√©s: {self.stats.elements_created}")
            print(f"   - Erreurs: {self.stats.errors}")
            
            if self.use_gemini:
                print(f"   - Appels Gemini: {self.stats.gemini_calls}")
                print(f"   - Cache hits: {self.stats.cache_hits}")
            
            return dpgf_id
            
        except Exception as e:
            print(f"‚ùå Erreur critique: {e}")
            import traceback
            traceback.print_exc()
            return None


def main():
    """Point d'entr√©e du script"""
    parser = argparse.ArgumentParser(description="Import complet d'un fichier DPGF")
    parser.add_argument("--file", required=True, help="Chemin du fichier Excel DPGF")
    parser.add_argument("--base-url", default="http://127.0.0.1:8000", help="URL de l'API")
    parser.add_argument("--dpgf-id", type=int, help="ID du DPGF existant (optionnel)")
    parser.add_argument("--lot-num", help="Num√©ro du lot (optionnel)")
    parser.add_argument("--gemini-key", help="Cl√© API Google Gemini pour la classification avanc√©e")
    parser.add_argument("--chunk-size", type=int, default=20, help="Taille des chunks pour l'analyse Gemini (d√©faut: 20)")
    parser.add_argument("--no-gemini", action="store_true", help="D√©sactiver l'utilisation de Gemini m√™me si la cl√© est fournie")
    parser.add_argument("--debug", action="store_true", help="Activer le mode debug pour plus d'informations")
    
    args = parser.parse_args()
    
    # D√©terminer si on utilise Gemini
    use_gemini = args.gemini_key is not None and not args.no_gemini
    
    importer = DPGFImporter(
        base_url=args.base_url,
        gemini_key=args.gemini_key,
        use_gemini=use_gemini,
        chunk_size=args.chunk_size,
        debug=args.debug
    )
    
    importer.import_file(
        file_path=args.file,
        dpgf_id=args.dpgf_id,
        lot_num=args.lot_num
    )


if __name__ == "__main__":
    main()
