"""
Script d'import DPGF complet am√©lior√©:
- D√©tection automatique du client
- Import de lots
- Import des sections et sous-sections
- Import des √©l√©ments d'ouvrage
- D√©tection dynamique des colonnes de prix et quantit√©s
- Gestion intelligente des erreurs et des doublons
- Classification avanc√©e avec l'API Google Gemini (optionnelle)
- Support des fichiers SharePoint avec d√©tection sp√©cialis√©e
"""

import argparse
import sys
import json
import os
import re
import hashlib
import pickle
import csv
from typing import Optional, Dict, List, Tuple, Generator
from datetime import date
from pathlib import Path
import pandas as pd
import requests
from tqdm import tqdm

# Import du module de logging am√©lior√©
try:
    from scripts.enhanced_logging import get_import_logger, ImportLogger
except ImportError:
    # Essayer import direct si dans le m√™me r√©pertoire
    try:
        from enhanced_logging import get_import_logger, ImportLogger
    except ImportError:
        # Fallback vers logging standard
        import logging
        
        class ImportLogger:
            def __init__(self, file_path):
                self.logger = logging.getLogger(f"import_{Path(file_path).stem}")
                self.logger.setLevel(logging.INFO)
                if not self.logger.handlers:
                    handler = logging.StreamHandler()
                    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
                    handler.setFormatter(formatter)
                    self.logger.addHandler(handler)
            
            def info(self, msg): self.logger.info(msg)
            def warning(self, msg): self.logger.warning(msg)
            def error(self, msg): self.logger.error(msg)
            def debug(self, msg): self.logger.debug(msg)
            def log_file_start(self, *args): self.info("Import d√©marr√©")
            def log_file_success(self, *args): self.info("Import termin√© avec succ√®s")
            def log_file_error(self, *args, **kwargs): self.error(f"Import √©chou√©: {kwargs}")
            def log_lot_detected(self, *args): self.info("Lot d√©tect√©")
            def log_section_detected(self, *args): self.info("Section d√©tect√©e")
            def log_element_detected(self, *args): self.info("√âl√©ment d√©tect√©")
            def log_element_without_section(self, *args): self.warning("√âl√©ment sans section")
            def close(self): pass
        
        def get_import_logger(file_path):
            return ImportLogger(file_path)

# Import du module d'aide pour les fichiers SharePoint
try:
    from scripts.sharepoint_import_helper import SharePointExcelHelper, is_sharepoint_file
    SHAREPOINT_HELPER_AVAILABLE = True
except ImportError:
    SHAREPOINT_HELPER_AVAILABLE = False
    print("‚ö†Ô∏è Module sharepoint_import_helper non disponible. Le support optimis√© pour SharePoint ne sera pas utilis√©.")

# Import conditionnel de l'API Gemini
try:
    import google.generativeai as genai
    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False
    print("‚ö†Ô∏è Module google.generativeai non disponible. L'analyse avanc√©e par IA ne sera pas utilis√©e.")

# Configuration de l'encodage pour √©viter les erreurs avec les caract√®res sp√©ciaux
if sys.platform.startswith('win'):
    import codecs
    sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer, 'strict')
    sys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer, 'strict')


class ImportStats:
    """Statistiques d'import"""
    def __init__(self):
        self.total_rows = 0
        self.sections_created = 0
        self.elements_created = 0
        self.sections_reused = 0
        self.lots_created = 0
        self.lots_reused = 0
        self.errors = 0
        self.gemini_calls = 0
        self.cache_hits = 0
        self.gemini_fallback_used = False  # Nouveau flag pour le fallback
        self.gemini_failure_reason = None  # Raison de l'√©chec de Gemini


class ColumnMapping:
    """Gestionnaire de mapping des colonnes avec persistance"""
    
    def __init__(self, mappings_file: str = "mappings.pkl"):
        self.mappings_file = mappings_file
        self.mappings = self._load_mappings()
    
    def _load_mappings(self) -> Dict:
        """Charge les mappings sauvegard√©s"""
        if os.path.exists(self.mappings_file):
            try:
                with open(self.mappings_file, 'rb') as f:
                    return pickle.load(f)
            except Exception as e:
                print(f"‚ö†Ô∏è Erreur chargement mappings: {e}")
        return {}
    
    def _save_mappings(self):
        """Sauvegarde les mappings sur disque"""
        try:
            with open(self.mappings_file, 'wb') as f:
                pickle.dump(self.mappings, f)
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur sauvegarde mappings: {e}")
    
    def _get_file_signature(self, headers: List[str], filename: str = None) -> str:
        """G√©n√®re une signature unique pour un type de fichier"""
        # Utiliser les headers pour cr√©er une signature
        headers_str = '|'.join([str(h).strip().lower() for h in headers if h])
        signature = hashlib.md5(headers_str.encode()).hexdigest()[:8]
        
        # Ajouter des infos du nom de fichier si disponible
        if filename:
            file_pattern = re.sub(r'\d+', 'X', filename.lower())  # Remplacer chiffres par X
            return f"{signature}_{file_pattern}"
        
        return signature
    
    def get_mapping(self, headers: List[str], filename: str = None) -> Optional[Dict[str, int]]:
        """R√©cup√®re un mapping existant"""
        signature = self._get_file_signature(headers, filename)
        return self.mappings.get(signature)
    
    def save_mapping(self, headers: List[str], mapping: Dict[str, int], filename: str = None):
        """Sauvegarde un nouveau mapping"""
        signature = self._get_file_signature(headers, filename)
        self.mappings[signature] = mapping
        self._save_mappings()
        print(f"‚úÖ Mapping sauvegard√© pour le type de fichier: {signature}")
    
    def interactive_mapping(self, headers: List[str]) -> Dict[str, Optional[int]]:
        """Interface interactive pour cr√©er un mapping manuel"""
        print("\n" + "="*60)
        print("üîß CONFIGURATION MANUELLE DU MAPPING DES COLONNES")
        print("="*60)
        print("En-t√™tes d√©tect√©s dans le fichier:")
        for i, header in enumerate(headers):
            print(f"  {i}: {header}")
        
        print("\nVeuillez indiquer l'indice de colonne pour chaque type de donn√©e:")
        print("(Tapez 'skip' ou laissez vide si la colonne n'existe pas)")
        
        mapping = {}
        column_types = [
            ('designation', 'D√©signation/Description'),
            ('unite', 'Unit√©'),
            ('quantite', 'Quantit√©'),
            ('prix_unitaire', 'Prix unitaire'),
            ('prix_total', 'Prix total')
        ]
        
        for col_key, col_description in column_types:
            while True:
                try:
                    response = input(f"\n{col_description}: ").strip()
                    if response.lower() in ['skip', '']:
                        mapping[col_key] = None
                        break
                    
                    col_index = int(response)
                    if 0 <= col_index < len(headers):
                        mapping[col_key] = col_index
                        print(f"‚úì {col_description} -> Colonne {col_index}: {headers[col_index]}")
                        break
                    else:
                        print(f"‚ùå Indice invalide. Doit √™tre entre 0 et {len(headers)-1}")
                        
                except ValueError:
                    print("‚ùå Veuillez entrer un nombre ou 'skip'")
        
        print("\n" + "="*60)
        print("Mapping configur√©:")
        for col_key, col_index in mapping.items():
            if col_index is not None:
                print(f"  {col_key}: Colonne {col_index} ({headers[col_index]})")
            else:
                print(f"  {col_key}: Non mapp√©")
        print("="*60)
        
        # Demander confirmation
        while True:
            confirm = input("\nConfirmer ce mapping? (o/n): ")

            if confirm.lower() in ['o', 'oui', 'y', 'yes']:
                return mapping
            elif confirm.lower() in ['n', 'non', 'no']:
                print("Mapping annul√©, recommencer...")
                return self.interactive_mapping(headers)
            else:
                print("R√©pondez par 'o' ou 'n'")


class ErrorReporter:
    """Gestionnaire de rapport d'erreurs CSV"""
    
    def __init__(self, error_file: str = "import_errors.csv"):
        self.error_file = error_file
        self.errors = []
        self._init_csv()
    
    def _init_csv(self):
        """Initialise le fichier CSV avec les en-t√™tes"""
        if not os.path.exists(self.error_file):
            with open(self.error_file, 'w', newline='', encoding='utf-8') as f:
                writer = csv.writer(f)
                writer.writerow(['timestamp', 'filename', 'line_number', 'error_type', 'error_message', 'raw_data'])
    
    def add_error(self, filename: str, line_number: int, error_type: str, error_message: str, raw_data: str = ""):
        """Ajoute une erreur au rapport"""
        from datetime import datetime
        error = {
            'timestamp': datetime.now().isoformat(),
            'filename': filename,
            'line_number': line_number,
            'error_type': error_type,
            'error_message': error_message,
            'raw_data': raw_data
        }
        self.errors.append(error)
    
    def save_report(self):
        """Sauvegarde toutes les erreurs dans le fichier CSV"""
        if not self.errors:
            return
        
        with open(self.error_file, 'a', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            for error in self.errors:
                writer.writerow([
                    error['timestamp'],
                    error['filename'],
                    error['line_number'],
                    error['error_type'],
                    error['error_message'],
                    error['raw_data']
                ])
        
        print(f"üìù {len(self.errors)} erreur(s) sauvegard√©e(s) dans {self.error_file}")
        self.errors.clear()


class ClientDetector:
    """D√©tecteur automatique du nom du client"""
    
    def __init__(self):
        # Patterns pour extraire le client du nom de fichier
        self.filename_patterns = [
            r'DPGF[_\-\s]*([A-Z][A-Za-z\s&\'\.]+?)[_\-\s]*Lot',
            r'([A-Z][A-Za-z\s&\'\.]+?)[\-_\s]*DPGF',
            r'Client[_\-\s]*([A-Z][A-Za-z\s&\'\.]+)',
            r'([A-Z]{2,}[\s&][A-Z\s\'\.]+)',  # Acronymes + mots
            r'^((?:[A-Z][a-zA-Z\'\.]+\s*)+)',  # S√©quence de mots capitalis√©s au d√©but
            r'[\\/]([A-Z][A-Za-z\s&\'\.]+?)[\\/][^\\\/]+\.xlsx$', # Client dans le chemin du dossier
            r'(?:projet|chantier)[_\-\s]+([A-Z][A-Za-z\s&\'\.]+)',  # Pattern apr√®s "projet" ou "chantier"
            r'_([A-Z][a-z]{2,}(?:[A-Z][a-z]+)+)_', # Nom en camelCase entour√© de underscores
        ]
        
        # Patterns pour d√©tecter un client dans le contenu
        self.content_patterns = [
            r'(?:client|ma√Ætre d\'ouvrage|maitre d\'ouvrage|donneur d\'ordre)[^\w\n]{1,5}([A-Z][A-Za-z\s&\'\.]{2,})',
            r'(?:pour|destin√© √†|r√©alis√© pour)[^\w\n]{1,5}([A-Z][A-ZaZ\s&\'\.]{2,})',
            r'(?:soci√©t√©|entreprise|groupe)[^\w\n]{1,5}([A-Z][A-ZaZ\s&\'\.]{2,})',
            r'^([A-Z][A-z]+(?:[\s\-][A-Z][A-z]+){1,3})\s*$', # Ligne avec uniquement un nom capitalis√©
            r'Projet\s*(?:pour|de|avec)?\s*(?:la|le)?\s*([A-Z][A-Za-z\s&\'\.]{2,})',
            r'Chantier\s*(?:de|pour)?\s*([A-Z][A-ZaZ\s&\'\.]{2,})',
            r'(?:SA|SAS|SARL|GROUP|HABITAT)\s+([A-Z][A-ZaZ\s&\'\.]{2,})',
            r'([A-Z][A-ZaZ\s&\'\.]{2,})\s+(?:SA|SAS|SARL|GROUP|HABITAT)'
        ]
        
        # Mots-cl√©s √† ignorer dans la d√©tection
        self.ignore_words = {'LOT', 'DPGF', 'NOVEMBRE', 'DECEMBRE', 'JANVIER', 'FEVRIER', 'MARS', 'AVRIL', 'MAI', 'JUIN', 
                           'JUILLET', 'AOUT', 'SEPTEMBRE', 'OCTOBRE', 'DCE', 'CONSTRUCTION', 'TRAVAUX', 'BATIMENT',
                           'APPEL', 'OFFRE', 'MARCHE', 'MAITRISE', 'OEUVRE', 'PROJET', 'CHANTIER', 'RENOVATION',
                           'REHABILITATION', 'DEVIS', 'ESTIMATION', 'PRIX', 'DOCUMENT', 'BORDEREAU', 'QUANTITATIF',
                           'REFERENCE', 'DESCRIPTION', 'CODE', 'UNITE', 'TOTAL', 'EUROS', 'EUR', 'HT', 'TTC'}
    
    def detect_from_filename(self, file_path: str) -> Optional[str]:
        """D√©tecte le client depuis le nom de fichier"""
        filename = Path(file_path).stem
        print(f"Analyse du nom de fichier: {filename}")
        
        for pattern in self.filename_patterns:
            match = re.search(pattern, filename, re.IGNORECASE)
            if match:
                client_name = match.group(1).strip()
                # Nettoyer et valider
                client_name = self._clean_client_name(client_name)
                if client_name and len(client_name) > 3:
                    print(f"Client d√©tect√© dans le nom de fichier: {client_name}")
                    return client_name
        
        return None
        
    def detect_from_excel_header(self, file_path: str) -> Optional[str]:
        """D√©tecte le client dans les 15 premi√®res lignes du fichier Excel"""
        try:
            # D√©tection automatique du moteur Excel selon l'extension
            engine = detect_excel_engine(file_path)
            # Lire seulement les premi√®res lignes (augment√© √† 15 pour une meilleure couverture)
            df = pd.read_excel(file_path, engine=engine, nrows=15, header=None)
            
            print("Analyse des premi√®res lignes du fichier...")
            
            # 1. D'abord chercher des mots-cl√©s sp√©cifiques comme "Client:", "Ma√Ætre d'ouvrage:"
            for row_idx in range(min(15, len(df))):
                row_text = " ".join([str(val).strip() for val in df.iloc[row_idx].values if pd.notna(val)])
                
                for pattern in self.content_patterns:
                    match = re.search(pattern, row_text, re.IGNORECASE)
                    if match:
                        client_name = match.group(1).strip()
                        client_name = self._clean_client_name(client_name)
                        if client_name and len(client_name) > 2:
                            print(f"Client d√©tect√© avec pattern sp√©cifique (ligne {row_idx}): {client_name}")
                            return client_name
            
            # 2. Chercher dans toutes les cellules des premi√®res lignes
            for row_idx in range(min(15, len(df))):
                for col_idx in range(min(8, len(df.columns))):  # Augment√© √† 8 colonnes
                    cell_value = df.iloc[row_idx, col_idx]
                    
                    if pd.notna(cell_value):
                        cell_text = str(cell_value).strip()
                        
                        # Chercher des patterns de nom de client
                        client = self._extract_client_from_text(cell_text)
                        if client:
                            print(f"Client d√©tect√© dans la cellule [{row_idx},{col_idx}]: {client}")
                            return client
            
            # 3. Recherche de texte libre avec des noms d'entreprises connus
            known_companies = [
                "CDC HABITAT", "VINCI", "BOUYGUES", "EIFFAGE", "AXA", "BNP", "ICADE", "NEXITY", 
                "KAUFMAN", "COGEDIM", "PITCH", "PICHET", "AMETIS", "ADIM", "SOGEPROM", "MARIGNAN",
                "DEMATHIEU BARD", "ALTAREA", "CREDIT AGRICOLE", "SOCIETE GENERALE", "CARREFOUR", 
                "LECLERC", "AUCHAN", "LEROY MERLIN", "CASTORAMA", "LIDL", "ALDI", "COLAS"
            ]
            
            for row_idx in range(min(15, len(df))):
                row_text = " ".join([str(val).strip() for val in df.iloc[row_idx].values if pd.notna(val)])
                for company in known_companies:
                    if company in row_text.upper():
                        print(f"Entreprise connue d√©tect√©e (ligne {row_idx}): {company}")
                        return company
            
            return None
        except Exception as e:
            print(f"Erreur lors de l'analyse de l'en-t√™te: {e}")
            return None
            
    def _extract_client_from_text(self, text: str) -> Optional[str]:
        """Extrait un nom de client depuis un texte"""
        # Patterns pour identifier un client
        client_patterns = [
            r'^([A-Z]{2,}(?:\s+[A-Z&\'\.]+)*)\s*$',  # Acronymes en majuscules
            r'([A-Z][a-z]+(?:\s+[A-Z][a-z\'\.]+)*)\s*(?:HABITAT|GROUP|COMPANY|SA|SAS|SARL|SCI|IMMOBILIER)',
            r'((?:[A-Z]{2,}\s*)+)(?:HABITAT|GROUP|IMMOBILIER)',  # CDC HABITAT, BNP GROUP, etc.
            r'(?:^|\s)([A-Z][a-zA-Z\'\.]+(?:\s+[A-Z][a-zA-Z\'\.]+){1,3})(?:\s|$)',  # Mots capitalis√©s (2-4 mots)
            r'(?:^|\s)([A-Z]{2,}(?:\s*[A-Z]{2,}){0,2})(?:\s|$)',  # Acronymes 2-3 lettres
            r'(?:soci√©t√©|entreprise|groupe|client|constructeur)\s+([A-Z][a-zA-Z\'\.]+(?:\s+[A-Z][a-zA-Z\'\.]+){0,3})',
            r'[Pp]our\s+(?:le\s+compte\s+de\s+)?([A-Z][a-zA-Z\'\.]+(?:\s+[A-Z][a-zA-Z\'\.]+){0,3})',
            r'[Aa]dresse\s*:?\s*(?:[^,]+,\s*)?([A-Z][a-zA-Z\'\.]+(?:\s+[A-Z][a-zA-Z\'\.]+){1,3})',
            r'(?:^|\n)\s*([A-Z][a-zA-Z\'\.]+(?:\s+[A-Z][a-zA-Z\'\.]+){1,2})\s*(?:$|\n)', # Nom isol√© sur une ligne
        ]
        
        text = text.strip()
        for pattern in client_patterns:
            match = re.search(pattern, text)
            if match:
                client_name = match.group(1).strip()
                client_name = self._clean_client_name(client_name)
                
                # Valider que c'est un vrai nom de client
                if (len(client_name) >= 3 and 
                    not any(word.upper() in self.ignore_words for word in client_name.split()) and
                    any(c.isalpha() for c in client_name) and
                    len([w for w in client_name.split() if len(w) > 1]) > 0):  # Au moins un mot de plus d'une lettre
                    return client
        
        # D√©tecter les noms d'entreprises connus m√™me sans pattern
        known_companies = [
            "CDC HABITAT", "VINCI", "BOUYGUES", "EIFFAGE", "AXA", "BNP", "ICADE", "NEXITY", 
            "KAUFMAN", "COGEDIM", "PITCH", "PICHET", "AMETIS", "ADIM", "SOGEPROM", "MARIGNAN",
            "DEMATHIEU BARD", "ALTAREA", "CREDIT AGRICOLE", "SOCIETE GENERALE", "CARREFOUR", 
            "LECLERC", "AUCHAN", "LEROY MERLIN", "CASTORAMA", "LIDL", "ALDI", "COLAS"
        ]
        for company in known_companies:
            if company in text.upper():
                return company
                
        return None
    
    def _clean_client_name(self, name: str) -> str:
        """Nettoie un nom de client"""
        # Supprimer caract√®res ind√©sirables
        name = re.sub(r'[_\-\.]+', ' ', name)
        name = re.sub(r'\s+', ' ', name)
        name = name.strip()
        
        # Supprimer mots parasites
        words = name.split()
        cleaned_words = [w for w in words if w.upper() not in self.ignore_words]
        
        return ' '.join(cleaned_words)
        
    def detect_client(self, file_path: str, include_file_suffix: bool = True) -> Optional[str]:
        """
        D√©tection compl√®te du client (nom de fichier + contenu)
        
        Args:
            file_path: Chemin du fichier Excel
            include_file_suffix: Si True, ajoute un suffixe du nom de fichier au client pour le rendre unique
        """
        print(f"üîç D√©tection automatique du client pour: {file_path}")
        filename = Path(file_path).stem
        
        client_candidates = []
        
        # 1. Essayer depuis le contenu du fichier (priorit√© au contenu car plus fiable)
        client_from_content = self.detect_from_excel_header(file_path)
        if client_from_content:
            client_candidates.append(("contenu", client_from_content))
        
        # 2. Essayer depuis le nom de fichier
        client_from_filename = self.detect_from_filename(file_path)
        if client_from_filename:
            client_candidates.append(("nom_fichier", client_from_filename))
        
        # 3. Essayer avec des mots uniques du nom de fichier
        unique_words = []
        if len(filename) > 5:
            words = re.findall(r'[A-Za-z]{4,}', filename)
            for word in words:
                if word.upper() not in self.ignore_words and word not in ["DPGF", "Lot", "LOT"]:
                    if len(word) >= 4:
                        unique_words.append(word)
        
        # S√©lection du meilleur candidat
        if client_candidates:
            # Priorit√© au client d√©tect√© dans le contenu
            source, client = next((s, c) for s, c in client_candidates if s == "contenu") \
                            if any(s == "contenu" for s, _ in client_candidates) \
                            else client_candidates[0]
            
            # G√©n√©rer un identifiant unique bas√© sur le nom de fichier
            if include_file_suffix:
                # Extraire un identifiant significatif du nom de fichier
                # Pr√©f√©rer un num√©ro de lot s'il existe
                lot_match = re.search(r'Lot\s*(\d+)', filename, re.IGNORECASE)
                if lot_match:
                    file_id = f"Lot{lot_match.group(1)}"
                else:
                    # Sinon utiliser un mot unique ou les premiers caract√®res
                    if unique_words:
                        file_id = unique_words[0][:8]
                    else:
                        file_id = filename.split()[0][:8]  # Premiers caract√®res du premier mot
                
                # Ajouter l'identifiant uniquement s'il n'est pas d√©j√† dans le nom du client
                if file_id.upper() not in client.upper():
                    client = f"{client} ({file_id})"
            
            print(f"‚úì Client d√©tect√© (source: {source}): {client}")
            return client
        
        # 4. Dernier recours: utiliser une partie du nom de fichier
        if unique_words:
            client = unique_words[0].capitalize()
            print(f"‚ö†Ô∏è Utilisation d'un mot unique du nom de fichier comme client: {client}")
            return client
        
        print("‚ö†Ô∏è Aucun client d√©tect√© automatiquement")
        return None


def detect_excel_engine(file_path: str) -> str:
    """D√©tecte automatiquement le bon moteur Excel selon l'extension du fichier"""
    file_extension = Path(file_path).suffix.lower()
    
    if file_extension == '.xls':
        # Fichier Excel ancien, utiliser xlrd
        try:
            import xlrd
            return 'xlrd'
        except ImportError:
            print("‚ö†Ô∏è Module xlrd non disponible pour les fichiers .xls")
            # Essayer openpyxl quand m√™me (peut √©chouer)
            return 'openpyxl'
    elif file_extension in ['.xlsx', '.xlsm']:
        # Fichier Excel moderne, utiliser openpyxl
        return 'openpyxl'
    else:
        # Extension non reconnue, essayer openpyxl par d√©faut
        return 'openpyxl'


class GeminiCache:
    """Cache intelligent pour les r√©ponses Gemini"""
    
    def __init__(self, cache_dir: str = "cache"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        self.cache_file = self.cache_dir / "gemini_patterns.pkl"
        self.patterns = self._load_cache()
    
    def _load_cache(self) -> Dict:
        """Charge le cache depuis le disque"""
        if self.cache_file.exists():
            try:
                with open(self.cache_file, 'rb') as f:
                    return pickle.load(f)
            except:
                pass
        return {}
    
    def _save_cache(self):
        """Sauvegarde le cache sur disque"""
        with open(self.cache_file, 'wb') as f:
            pickle.dump(self.patterns, f)
    
    def _get_pattern_hash(self, rows: List[str]) -> str:
        """G√©n√®re un hash pour un pattern de lignes"""
        # Normaliser les lignes (enlever espaces, casse)
        normalized = []
        for row in rows:
            normalized.append(''.join(row.lower().split()))
        
        pattern = '|'.join(normalized)
        return hashlib.md5(pattern.encode()).hexdigest()
    
    def get(self, rows: List[str]) -> Optional[List[Dict]]:
        """R√©cup√®re une classification depuis le cache"""
        pattern_hash = self._get_pattern_hash(rows)
        return self.patterns.get(pattern_hash)
    
    def set(self, rows: List[str], classification: List[Dict]):
        """Met en cache une classification"""
        pattern_hash = self._get_pattern_hash(rows)
        self.patterns[pattern_hash] = classification
        self._save_cache()


class ExcelParser:
    """Analyse les fichiers Excel DPGF avec d√©tection de colonnes am√©lior√©e
    et support sp√©cifique pour les formats SharePoint"""
    
    def __init__(self, file_path: str, column_mapper: ColumnMapping = None, error_reporter: ErrorReporter = None, 
                 dry_run: bool = False, gemini_processor: 'GeminiProcessor' = None):
        self.file_path = file_path
        self.column_mapper = column_mapper or ColumnMapping()
        self.error_reporter = error_reporter or ErrorReporter()
        self.dry_run = dry_run
        self.gemini_processor = gemini_processor
        
        # Initialiser le logger d'import am√©lior√©
        self.logger = get_import_logger(file_path)
        self.logger.info(f"Initialisation de l'analyse pour {Path(file_path).name}")
        
        # Initialiser tous les attributs de colonnes et confiance
        self.col_designation = None
        self.col_unite = None
        self.col_quantite = None
        self.col_prix_unitaire = None
        self.col_prix_total = None
        self.mapping_confidence = 'unknown'  # 'high', 'medium', 'low', 'manual', 'sharepoint'
        self.headers_detected = False  # Ajouter cette ligne manquante
        self.is_sharepoint = False  # Initialiser par d√©faut √† False
        
        # V√©rifier si c'est un fichier SharePoint
        if SHAREPOINT_HELPER_AVAILABLE:
            try:
                self.is_sharepoint = is_sharepoint_file(file_path)
                if self.is_sharepoint:
                    self.logger.info("Format SharePoint d√©tect√© - Utilisation du helper sp√©cialis√©")
                    print("üîÑ Format SharePoint d√©tect√© - Utilisation du helper sp√©cialis√©")
                    self.sharepoint_helper = SharePointExcelHelper(file_path)
                    self.sharepoint_helper.select_best_sheet()
                    self.df = self.sharepoint_helper.load_selected_sheet()
                    self.mapping_confidence = 'sharepoint'  # Indiquer que c'est un fichier SharePoint
                    return
            except Exception as e:
                self.logger.error(f"Erreur lors de l'analyse SharePoint: {e}")
                print(f"‚ùå Erreur lors de l'analyse SharePoint: {e}")
                self.is_sharepoint = False
        
        # Si ce n'est pas un fichier SharePoint ou si l'analyse √©choue, utiliser la m√©thode standard
        self.df = self._read_best_sheet(file_path)
    
    def _read_best_sheet(self, file_path: str) -> pd.DataFrame:
        """Lit la meilleure feuille du fichier Excel (celle qui contient des donn√©es DPGF)"""
        try:
            # D√©tection automatique du moteur Excel selon l'extension
            engine = detect_excel_engine(file_path)
            xl_file = pd.ExcelFile(file_path, engine=engine)
            
            if len(xl_file.sheet_names) == 1:
                # Un seule feuille, l'utiliser directement
                return pd.read_excel(file_path, engine=engine, header=None)
            
            print(f"üîç Fichier multi-feuilles d√©tect√© ({len(xl_file.sheet_names)} feuilles)")
            
            best_sheet = None
            best_score = 0
            
            for sheet_name in xl_file.sheet_names:
                try:
                    # √âviter les pages de garde et feuilles vides
                    if any(skip_word in sheet_name.lower() for skip_word in ['garde', 'page', 'cover', 'sommaire']):
                        continue
                    
                    df_sheet = pd.read_excel(file_path, sheet_name=sheet_name, engine=engine, header=None)
                    
                    if df_sheet.shape[0] == 0 or df_sheet.shape[1] == 0:
                        continue  # Feuille vide
                    
                    # Ajouter le nom de la feuille comme attribut pour le scoring
                    df_sheet.name = sheet_name
                    
                    # Scorer la feuille selon son contenu DPGF
                    score = self._score_sheet_content(df_sheet)
                    
                    print(f"   Feuille '{sheet_name}': {df_sheet.shape[0]}√ó{df_sheet.shape[1]}, score: {score}")
                    
                    if score > best_score:
                        best_score = score
                        best_sheet = (sheet_name, df_sheet)
                        
                except Exception as e:
                    print(f"   ‚ö†Ô∏è Erreur lecture feuille '{sheet_name}': {e}")
                    continue
            
            if best_sheet:
                sheet_name, df = best_sheet
                print(f"‚úÖ Feuille s√©lectionn√©e: '{sheet_name}' (score: {best_score})")
                return df
            else:
                print("‚ö†Ô∏è Aucune feuille valide trouv√©e, utilisation de la premi√®re")
                return pd.read_excel(file_path, engine=engine, header=None)
                
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur lors de la d√©tection multi-feuilles: {e}")
            return pd.read_excel(file_path, engine=engine, header=None)
    
    def _score_sheet_content(self, df: pd.DataFrame) -> int:
        """Score le contenu d'une feuille pour d√©terminer si elle contient des donn√©es DPGF"""
        score = 0
        
        # === V√âRIFICATIONS PRIORITAIRES ===
        # Bonus important si la feuille contient plus de 20 lignes (donn√©es substantielles)
        if df.shape[0] > 20:
            score += 10
        elif df.shape[0] > 10:
            score += 5
        
        # Bonus pour le nombre de colonnes appropri√© pour un DPGF (4-15 colonnes typiques)
        if 4 <= df.shape[1] <= 15:
            score += 5
        elif df.shape[1] > 15:
            score -= 2  # P√©nalit√© pour trop de colonnes
        
        # === ANALYSE DU CONTENU ===
        # Chercher des indices de contenu DPGF dans les premi√®res lignes
        search_rows = min(20, df.shape[0])
        
        for i in range(search_rows):
            row = df.iloc[i]
            row_text = ' '.join([str(val).lower() for val in row if pd.notna(val)])
            
            # Mots-cl√©s DPGF tr√®s sp√©cifiques
            dpgf_keywords = [
                'designation', 'd√©signation', 'quantit√©', 'quantite', 'prix unitaire', 'prix total',
                'montant', 'unitaire', 'p.u.', 'pu', 'unit√©', 'unite'
            ]
            for keyword in dpgf_keywords:
                if keyword in row_text:
                    score += 8  # Score √©lev√© pour les mots-cl√©s DPGF
            
            # Patterns de num√©rotation d'articles (X.X.X, A.1.2)
            for val in row:
                if pd.notna(val):
                    val_str = str(val).strip()
                    if re.match(r'^\d+(\.\d+)*$', val_str) or re.match(r'^[A-Z]\d+(\.\d+)*$', val_str):
                        score += 3
            
            # Unit√©s typiques BTP
            unit_keywords = ['ens', 'u', 'ml', 'm2', 'm¬≤', 'm3', 'm¬≥', 'kg', 'h', 'j', 'forfait', 'ft']
            for keyword in unit_keywords:
                if keyword in row_text:
                    score += 2
            
            # Termes techniques BTP
            btp_keywords = [
                'fourniture', 'pose', 'installation', 'montage', 'ma√ßonnerie', 'maconnerie',
                'charpente', 'couverture', 'menuiserie', 'plomberie', '√©lectricit√©', 'electricite'
            ]
            for keyword in btp_keywords:
                if keyword in row_text:
                    score += 3
        
        # === D√âTECTION DE VALEURS NUM√âRIQUES ===
        # Compter les colonnes avec beaucoup de valeurs num√©riques (prix, quantit√©s)
        numeric_columns = 0
        for col in range(min(10, df.shape[1])):
            numeric_count = 0
            for row in range(min(20, df.shape[0])):
                try:
                    if pd.notna(df.iloc[row, col]):
                        val = str(df.iloc[row, col]).replace(',', '.')
                        float(val)
                        numeric_count += 1
                except (ValueError, TypeError):
                    pass
            
            if numeric_count > 5:  # Plus de 5 valeurs num√©riques dans la colonne
                numeric_columns += 1
        
        score += numeric_columns * 3  # Bonus pour les colonnes num√©riques
        
        # === P√âNALIT√âS POUR FAUSSES FEUILLES ===
        sheet_names_penalties = ['info', 'infos', 'garde', 'page', 'cover', 'sommaire', 'recap']
        sheet_name = getattr(df, 'name', '').lower() if hasattr(df, 'name') else ''
        
        # P√©nalit√© si c'est probablement une feuille d'information
        if any(penalty_name in sheet_name for penalty_name in sheet_names_penalties):
            score -= 15
        
        # P√©nalit√© si tr√®s peu de lignes (moins de 10)
        if df.shape[0] < 10:
            score -= 10
        
        # Bonus pour les noms de feuilles √©vocateurs de lots
        if re.search(r'lot\s*\d+', sheet_name, re.IGNORECASE):
            score += 15
        
        return max(0, score)  # Score minimum de 0
    
    def find_lot_headers(self) -> List[Tuple[str, str]]:
        """
        Recherche les intitul√©s de lot avec priorit√© au nom de fichier.
        Ordre de priorit√©:
        1. Extraction depuis le nom de fichier (le plus fiable)
        2. D√©tection avec Gemini (si disponible)
        3. M√©thode classique - contenu du fichier
        
        Returns:
            Liste de tuples (numero_lot, nom_lot)
        """
        lots = []
        
        self.logger.info("==== D√âTECTION DE LOT ====")
        self.logger.info("M√©thode 1: Extraction depuis le nom du fichier")
        
        # Priorit√© 1: Essayer d'extraire depuis le nom de fichier (plus fiable)
        filename_lot = self.extract_lot_from_filename()
        if filename_lot:
            self.logger.log_lot_detection("filename", True, filename_lot)
            print(f"‚úÖ Lot d√©tect√© depuis le nom de fichier: {filename_lot[0]} - {filename_lot[1]}")
            return [filename_lot]
        else:
            self.logger.log_lot_detection("filename", False, error="Aucun pattern de lot trouv√© dans le nom du fichier")
        
        # Priorit√© 2: Essayer avec Gemini si disponible
        if self.gemini_processor:
            self.logger.info("M√©thode 2: D√©tection avec l'IA Gemini")
            try:
                filename = Path(self.file_path).name
                gemini_lot = self.gemini_processor.detect_lot_info(self.file_path, filename)
                if gemini_lot:
                    self.logger.log_lot_detection("gemini", True, gemini_lot)
                    print(f"‚úÖ Lot d√©tect√© par Gemini: {gemini_lot[0]} - {gemini_lot[1]}")
                    return [gemini_lot]
                else:
                    self.logger.log_lot_detection("gemini", False, error="Gemini n'a pas pu identifier un lot")
            except Exception as e:
                error_msg = f"Erreur Gemini pour d√©tection lot: {e}"
                self.logger.log_lot_detection("gemini", False, error=error_msg)
                print(f"‚ö†Ô∏è {error_msg}, fallback sur m√©thode classique")
        else:
            self.logger.info("M√©thode 2: D√©tection avec l'IA Gemini [NON DISPONIBLE]")
        
        # Priorit√© 3: M√©thode classique - analyser le contenu du fichier
        self.logger.info("M√©thode 3: Analyse classique du contenu")
        pattern = re.compile(r'lot\s+([^\s‚Äì-]+)\s*[‚Äì-]\s*(.+)', re.IGNORECASE)
        
        # Parcourir les 15 premi√®res lignes
        self.logger.debug(f"Recherche dans les {min(15, len(self.df))} premi√®res lignes du fichier")
        for i in range(min(15, len(self.df))):
            for col in range(len(self.df.columns)):
                if col < len(self.df.columns):  # V√©rification de s√©curit√©
                    cell_value = self.df.iloc[i, col]
                    if pd.notna(cell_value):
                        cell_str = str(cell_value).strip()
                        match = pattern.search(cell_str)
                        if match:
                            numero_lot = match.group(1).strip()
                            nom_lot = match.group(2).strip()
                            lot_info = (numero_lot, nom_lot)
                            self.logger.log_lot_detection("content", True, lot_info, 
                                                         pattern=r'lot\s+([^\s‚Äì-]+)\s*[‚Äì-]\s*(.+)',
                                                         error=f"Trouv√© dans la cellule [{i},{col}]: '{cell_str}'")
                            print(f"‚úÖ Lot d√©tect√© dans le contenu: {numero_lot} - {nom_lot}")
                            lots.append(lot_info)
        
        if lots:
            return lots
        
        self.logger.warning("√âCHEC DE D√âTECTION - Aucun lot trouv√© avec les m√©thodes disponibles")
        print("‚ö†Ô∏è Aucun lot d√©tect√© avec aucune m√©thode")
        return lots
    
    def extract_lot_from_filename(self) -> Optional[Tuple[str, str]]:
        """
        Extrait le num√©ro et nom du lot depuis le nom du fichier.
        Version ultra-renforc√©e pour tous les formats, m√™me exotiques.
        
        Returns:
            Tuple (numero_lot, nom_lot) ou None si non trouv√©
        """
        filename = Path(self.file_path).stem
        
        self.logger.debug(f"Analyse du nom de fichier: {filename}")
        
        # Patterns renforc√©s pour d√©tecter un lot dans le nom de fichier (ordre de priorit√©)
        patterns = [
            # === PATTERNS STANDARDS ===
            # LOT 06 - DPGF - METALLERIE (tr√®s sp√©cifique)
            r'lot\s*(\d{1,2})\s*-\s*(?:dpgf|devis|bpu|dqe)\s*-\s*([\w\s\-&¬∞\'\.]+)',
            
            # DPGF-Lot 06 M√©tallerie (avec tiret)
            r'dpgf\s*[-_]?\s*lot\s*(\d{1,2})\s+([\w\s\-&¬∞\'\.]+)',
            
            # LOT 06 - METALLERIE (avec tiret et nom)
            r'lot\s*(\d{1,2})\s*-\s*([\w\s\-&¬∞\'\.]+)',
            
            # === PATTERNS COMPLEXES ===
            # 802 DPGF Lot 2 - Curage (num√©ro au d√©but + lot)
            r'^\d+\s+dpgf\s+lot\s*(\d{1,2})\s*-\s*([\w\s\-&¬∞\'\.]+)',
            
            # DPGF Lot 6 - M√©tallerie
            r'dpgf\s+lot\s*(\d{1,2})\s*-\s*([\w\s\-&¬∞\'\.]+)',
            
            # Lot06_M√©tallerie ou Lot 06 M√©tallerie
            r'lot\s*(\d{1,2})[_\-\s]+([\w\s\-&¬∞\'\.]+)',
            
            # === PATTERNS SHAREPOINT ET ENTREPRISES ===
            # 25S012 - DPGF -Lot4 (pattern sp√©cial SharePoint)
            r'-\s*dpgf\s*-?\s*lot\s*(\d{1,2})\s*-?\s*([\w\s\-&¬∞\'\.]*)',
            
            # [Entreprise] - Lot 03 - Nom du lot
            r'[\[\(][\w\s]+[\]\)]\s*-\s*lot\s*(\d{1,2})\s*-\s*([\w\s\-&¬∞\'\.]+)',
            
            # === PATTERNS AVEC PR√âFIXES ===
            # DCE_Lot_06_Metallerie
            r'(?:dce|bce|appel|marche|projet)[-_\s]*lot[-_\s]*(\d{1,2})[-_\s]+([\w\s\-&¬∞\'\.]+)',
            
            # Chantier_Nom_Lot06_Description
            r'(?:chantier|projet|travaux)[-_\s]*[\w\s]*[-_\s]*lot[-_\s]*(\d{1,2})[-_\s]+([\w\s\-&¬∞\'\.]+)',
            
            # === PATTERNS AVEC CODES CLIENTS ===
            # CDC_HABITAT_LOT_6_METALLERIE
            r'(?:cdc|bnp|axa|vinci|bouygues)[-_\s]*(?:habitat|group|immobilier)?[-_\s]*lot[-_\s]*(\d{1,2})[-_\s]+([\w\s\-&¬∞\'\.]+)',
            
            # === PATTERNS ALTERNATIFS ===
            # LOT6 - Description (coll√©)
            r'lot(\d{1,2})\s*-\s*([\w\s\-&¬∞\'\.]+)',
            
            # Lot_6_Description (avec underscores)
            r'lot[-_](\d{1,2})[-_]([\w\s\-&¬∞\'\.]+)',
            
            # 06_METALLERIE_LOT (invers√©)
            r'(\d{1,2})[-_\s]*([\w\s\-&¬∞\'\.]+)[-_\s]*lot',
            
            # === PATTERNS MINIMALISTES ===
            # Lot6 (juste num√©ro, sans description)
            r'lot\s*(\d{1,2})(?!\d)(?:[^\w\d]|$)',  # √âviter lot123
            
            # L06, L6 (format abr√©g√©)
            r'\bL(\d{1,2})\b',
            
            # 6-METALLERIE (sans "lot")
            r'^(\d{1,2})\s*-\s*([\w\s\-&¬∞\'\.]{5,})',
            
            # === PATTERNS DANS LE CHEMIN ===
            # Chercher aussi dans le chemin du fichier
            r'[\\/]lot[-_\s]*(\d{1,2})[-_\s]*([\w\s\-&¬∞\'\.]*)',
        ]
        
        for idx, pattern in enumerate(patterns):
            match = re.search(pattern, filename, re.IGNORECASE)
            if match:
                try:
                    numero_lot = match.group(1).strip()
                    
                    # Si on a un deuxi√®me groupe de capture, c'est le nom du lot
                    if len(match.groups()) > 1 and match.group(2):
                        nom_lot = self._clean_lot_name(match.group(2).strip())
                        
                        # Validation du nom de lot
                        if len(nom_lot) < 3:
                            nom_lot = self._generate_fallback_lot_name(numero_lot, filename)
                    else:
                        nom_lot = self._generate_fallback_lot_name(numero_lot, filename)
                    
                    # Validation du num√©ro de lot
                    try:
                        lot_num = int(numero_lot)
                        if not (1 <= lot_num <= 99):
                            continue  # Num√©ro de lot invalide
                    except ValueError:
                        continue
                    
                    self.logger.debug(f"Pattern #{idx+1} correspondant: '{pattern}' dans '{filename}'")
                    print(f"‚úì Lot d√©tect√© depuis le nom du fichier: {numero_lot} - {nom_lot}")
                    return (numero_lot, nom_lot)
                except Exception as e:
                    self.logger.debug(f"Erreur lors de l'extraction avec le pattern #{idx+1}: {e}")
        
        # === D√âTECTION AVANC√âE PAR MOTS-CL√âS ===
        self.logger.debug("Aucun pattern standard trouv√©, essai de d√©tection avanc√©e")
        
        # Chercher des mots-cl√©s sp√©cialis√©s du BTP pour inf√©rer le type de lot
        specialty_keywords = {
            'gros_oeuvre': ['gros', 'oeuvre', 'b√©ton', 'beton', 'ma√ßonnerie', 'maconnerie', 'structure'],
            'charpente': ['charpente', 'bois', 'ossature'],
            'couverture': ['couverture', 'toiture', 'zinc', 'tuile', 'ardoise'],
            'menuiserie': ['menuiserie', 'fen√™tre', 'fenetre', 'porte', 'volet'],
            'serrurerie': ['serrurerie', 'm√©tallerie', 'metallerie', 'acier', 'fer'],
            'plomberie': ['plomberie', 'sanitaire', 'eau', '√©vacuation', 'evacuation'],
            'electricite': ['√©lectricit√©', 'electricite', '√©clairage', 'eclairage', 'courant'],
            'peinture': ['peinture', 'rev√™tement', 'revetement', 'finition'],
            'isolation': ['isolation', 'thermique', 'phonique'],
            'carrelage': ['carrelage', 'fa√Øence', 'faience', 'sol'],
            'cloisons': ['cloison', 'doublage', 'pl√¢tre', 'platre'],
            'vrd': ['vrd', 'voirie', 'r√©seau', 'reseau', 'assainissement'],
            'espaces_verts': ['espaces', 'verts', 'paysager', 'jardinage', 'plantation']
        }
        
        keywords = ['lot', 'dpgf', 'bpu', 'dqe', 'devis', 'bordereau']
        if any(keyword in filename.lower() for keyword in keywords):
            # Chercher un num√©ro dans le contexte
            digit_matches = re.finditer(r'(\d{1,2})', filename)
            for match in digit_matches:
                numero = match.group(1)
                try:
                    if 1 <= int(numero) <= 99:
                        # Identifier le type de lot par les mots-cl√©s
                        filename_lower = filename.lower()
                        lot_type = "Travaux"  # Type par d√©faut
                        
                        for specialty, keywords_list in specialty_keywords.items():
                            if any(kw in filename_lower for kw in keywords_list):
                                lot_type = specialty.replace('_', ' ').title()
                                break
                        
                        nom_lot = f"{lot_type} - Lot {numero}"
                        
                        self.logger.debug(f"Lot inf√©r√© par mots-cl√©s: {numero} - {nom_lot}")
                        print(f"‚úì Lot inf√©r√© depuis le nom du fichier: {numero} - {nom_lot}")
                        return (numero, nom_lot)
                except ValueError:
                    continue
        
        # === ANALYSE DU CHEMIN DU FICHIER ===
        full_path = str(self.file_path)
        path_match = re.search(r'[\\/]lot[-_\s]*(\d{1,2})[-_\s]*([\w\s\-&¬∞\'\.]*)', full_path, re.IGNORECASE)
        if path_match:
            numero_lot = path_match.group(1)
            nom_lot = self._clean_lot_name(path_match.group(2)) if path_match.group(2) else f"Lot {numero_lot}"
            
            self.logger.debug(f"Lot d√©tect√© dans le chemin: {numero_lot} - {nom_lot}")
            print(f"‚úì Lot d√©tect√© depuis le chemin du fichier: {numero_lot} - {nom_lot}")
            return (numero_lot, nom_lot)
        
        self.logger.debug("√âchec de la d√©tection de lot dans le nom du fichier")
        return None
    
    def _clean_lot_name(self, name: str) -> str:
        """Nettoie et normalise un nom de lot"""
        if not name:
            return ""
        
        # Supprimer caract√®res ind√©sirables
        cleaned = re.sub(r'[_\-\.]+', ' ', name)
        cleaned = re.sub(r'\s+', ' ', cleaned)
        cleaned = cleaned.strip()
        
        # Supprimer les extensions de fichier r√©siduelles
        cleaned = re.sub(r'\.(xlsx?|pdf|docx?)$', '', cleaned, flags=re.IGNORECASE)
        
        # Supprimer les mots parasites
        parasites = ['dpgf', 'bpu', 'dqe', 'devis', 'bordereau', 'quantitatif', 'prix', 'lot']
        words = cleaned.split()
        cleaned_words = [w for w in words if w.lower() not in parasites]
        
        result = ' '.join(cleaned_words).strip()
        
        # Capitaliser proprement
        if result:
            result = result.title()
        
        return result
    
    def _generate_fallback_lot_name(self, numero_lot: str, filename: str) -> str:
        """G√©n√®re un nom de lot par d√©faut bas√© sur le contexte"""
        # Essayer d'extraire des mots significatifs du nom de fichier
        words = re.findall(r'[A-Za-z]{3,}', filename)
        meaningful_words = []
        
        ignore_words = {
            'dpgf', 'bpu', 'dqe', 'devis', 'bordereau', 'quantitatif', 'prix', 'lot',
            'document', 'fichier', 'excel', 'pdf', 'word', 'nouveau', 'final',
            'version', 'copie', 'backup', 'temp', 'draft', 'brouillon'
        }
        
        for word in words:
            if word.lower() not in ignore_words and len(word) > 3:
                meaningful_words.append(word.title())
                if len(meaningful_words) >= 3:  # Limiter √† 3 mots
                    break
        
        if meaningful_words:
            return f"Lot {numero_lot} - {' '.join(meaningful_words)}"
        else:
            return f"Lot {numero_lot} - Travaux"
        
    def detect_project_name(self, client_name: str = None) -> str:
        """
        Extrait le nom de projet avec une strat√©gie intelligente pour garantir l'unicit√©.
        
        Args:
            client_name: Nom du client pour l'inclure dans le nom du projet
            
        Returns:
            Nom du projet (str)
        """
        potential_names = []
        filename = Path(self.file_path).stem
        
        # 1. Chercher dans les premi√®res cellules
        for i in range(min(5, len(self.df))):
            for col in range(min(3, len(self.df.columns))):
                cell_value = self.df.iloc[i, col]
                if pd.notna(cell_value):
                    value = str(cell_value).strip()
                    if len(value) > 5 and not any(w in value.upper() for w in ['DPGF', 'QUANTITATIF', 'BORDEREAU']):
                        potential_names.append(value)
        
        # 2. Extraire des infos pertinentes du nom de fichier
        file_info = []
        
        # Extraire le num√©ro et nom de lot
        lot_match = re.search(r'Lot\s*(\d+)[^\w]*([\w\s\-]+?)(?:\.xlsx|$)', filename, re.IGNORECASE)
        if lot_match:
            lot_num = lot_match.group(1)
            lot_name = lot_match.group(2).strip()
            if lot_name:
                file_info.append(f"Lot {lot_num} - {lot_name}")
            else:
                file_info.append(f"Lot {lot_num}")
        
        # 3. Construire un nom de projet distinctif
        project_parts = []
        
        # Inclure le nom du client s'il est fourni
        if client_name and len(client_name) > 2:
            project_parts.append(client_name)
        
        # Ajouter la meilleure info de contenu 
        if potential_names:
            best_name = max(potential_names, key=len)[:50]  # Limiter la longueur
            if best_name and not any(best_name.lower() in part.lower() for part in project_parts):
                project_parts.append(best_name)
        
        # Ajouter l'info du fichier
        if file_info:
            best_file_info = file_info[0]
            if not any(best_file_info.lower() in part.lower() for part in project_parts):
                project_parts.append(best_file_info)
        
        # Si on n'a pas assez d'infos, utiliser le nom de fichier
        if not project_parts or len(' - '.join(project_parts)) < 10:
            return filename
            
        # Construire le nom final
        return ' - '.join(project_parts)
    
    def find_header_row(self) -> Optional[int]:
        """
        Trouve la ligne d'en-t√™te contenant D√©signation/Quantit√©/Prix unitaire/Prix total.
        Version am√©lior√©e avec plus de patterns et de robustesse.
        
        Returns:
            Index de la ligne d'en-t√™te ou None si non trouv√©e
        """
        # Si c'est un fichier SharePoint, utiliser la d√©tection sp√©cialis√©e
        if self.is_sharepoint and self.sharepoint_helper:
            self.logger.info("Utilisation de la d√©tection d'en-t√™te SharePoint")
            header_row = self.sharepoint_helper.find_header_row_sharepoint()
            if header_row is not None:
                self.logger.info(f"En-t√™te SharePoint trouv√© √† la ligne {header_row+1}")
            else:
                self.logger.warning("Aucun en-t√™te SharePoint trouv√©")
            return header_row
            
        # Patterns am√©lior√©s pour reconna√Ætre les en-t√™tes (fran√ßais et variations)
        header_patterns = {
            'designation': [
                r'd√©signation', r'designation', r'libell√©', r'libelle', r'description', r'prestation', 
                r'article', r'd√©tail', r'detail', r'ouvrage', r'intitul√©', r'intitule', r'nature',
                r'objet', r'travaux', r'ouvrages?', r'prestations?', r'descriptions?', r'libelles?',
                r'n¬∞.*art.*', r'ref.*art.*', r'code.*art.*', r'art\.?.*d√©s.*', r'd√©s.*art.*'
            ],
            'unite': [
                r'unit√©', r'unite', r'u\.?$', r'un\.?$', r'un$', r'unit√© de mesure', r'mesure', 
                r'unit', r'^u$', r'unit√©s', r'mesures', r'type.*unit.*', r'unit.*mes.*'
            ],
            'quantite': [
                r'quantit√©', r'quantite', r'qt√©\.?', r'qt\.?', r'quant\.?', r'qte', r'nombre', 
                r'nb\.?', r'q\.?$', r'qt√©s?', r'quantit√©s', r'nbres?', r'nombres'
            ],
            'prix_unitaire': [
                r'prix\s*(?:unitaire|unit\.?)(?:\s*h\.?t\.?)?', r'p\.u\.(?:\s*h\.?t\.?)?', 
                r'pu(?:\s*h\.?t\.?)?$', r'prix$', r'pu\s*ht$', r'prix\s*ht$',
                r'co√ªt.*unit.*', r'tarif.*unit.*', r'‚Ç¨.*unit.*', r'euro.*unit.*',
                r'prix.*‚Ç¨', r'tarif', r'co√ªt', r'montant.*unit.*'
            ],
            'prix_total': [
                r'prix\s*(?:total|tot\.?)(?:\s*h\.?t\.?)?', r'montant(?:\s*h\.?t\.?)?', 
                r'p\.t\.(?:\s*h\.?t\.?)?', r'pt(?:\s*h\.?t\.?)?', r'total(?:\s*h\.?t\.?)?',
                r'sous.*total', r'co√ªt.*total', r'‚Ç¨.*total', r'somme', r'montants?'
            ]
        }
        best_row = None
        best_score = 0
        
        # Parcourir les 30 premi√®res lignes pour chercher les en-t√™tes
        for i in range(min(30, len(self.df))):
            row_values = [str(val).strip().lower() if pd.notna(val) else "" for val in self.df.iloc[i].values]
            row_text = " ".join(row_values)
            
            # Compter le nombre de patterns correspondants dans cette ligne
            score = 0
            found_patterns = {k: False for k in header_patterns.keys()}
            
            for col_name, patterns in header_patterns.items():
                # Chercher chaque pattern dans toute la ligne d'abord
                for pattern in patterns:
                    if re.search(pattern, row_text, re.IGNORECASE):
                        found_patterns[col_name] = True
                        score += 1
                        break
                
                # Si le pattern n'est pas trouv√© dans la ligne enti√®re, chercher dans chaque cellule
                if not found_patterns[col_name]:
                    for col_idx, cell_text in enumerate(row_values):
                        for pattern in patterns:
                            if re.search(f"^{pattern}$", cell_text, re.IGNORECASE):
                                found_patterns[col_name] = True
                                score += 1
                                break
                        if found_patterns[col_name]:
                            break
            
            # Si on a trouv√© au moins 2 des 5 en-t√™tes attendus, c'est probablement la bonne ligne
            if score >= 2:
                if score > best_score:
                    best_score = score
                    best_row = i
            
            # Si on a trouv√© tous les en-t√™tes, on arr√™te la recherche
            if score >= 4:  # 4/5 colonnes trouv√©es = excellent score
                print(f"‚úì Ligne d'en-t√™te trouv√©e (ligne {i+1}): score excellent ({score}/5)")
                return i
        
        if best_row is not None:
            print(f"‚úì Ligne d'en-t√™te trouv√©e (ligne {best_row+1}): score {best_score}/5")
        else:
            print("‚ö†Ô∏è Aucune ligne d'en-t√™te trouv√©e, l'analyse pourrait √™tre moins pr√©cise")
            
        return best_row
    
    def detect_column_indices(self, header_row_idx: Optional[int]) -> Dict[str, Optional[int]]:
        """
        D√©termine l'indice des colonnes importantes en se basant sur l'en-t√™te
        Int√®gre le mapping manuel interactif et persistant
        
        Args:
            header_row_idx: Indice de la ligne d'en-t√™te
            
        Returns:
            Dictionnaire avec les indices des colonnes
        """
        # Si c'est un fichier SharePoint, utiliser la d√©tection sp√©cialis√©e
        if self.is_sharepoint and self.sharepoint_helper:
            self.logger.info("Utilisation de la d√©tection de colonnes SharePoint")
            column_indices = self.sharepoint_helper.detect_column_indices_sharepoint(header_row_idx)
            self._store_column_indices(column_indices)
            self.mapping_confidence = 'sharepoint'
            print(f"‚úÖ Mapping SharePoint appliqu√©: {column_indices}")
            return column_indices
            
        # R√©cup√©rer les headers pour le mapping
        if header_row_idx is not None:
            headers = [str(val).strip() if pd.notna(val) else f"Colonne_{i}" 
                      for i, val in enumerate(self.df.iloc[header_row_idx].values)]
        else:
            # G√©n√©rer des headers par d√©faut
            headers = [f"Colonne_{i}" for i in range(len(self.df.columns))]
        
        # 1. Essayer de r√©cup√©rer un mapping existant
        filename = Path(self.file_path).stem
        existing_mapping = self.column_mapper.get_mapping(headers, filename)
        
        if existing_mapping:
            print("‚úÖ Mapping existant trouv√© et appliqu√©")
            self.mapping_confidence = 'manual'
            self._store_column_indices(existing_mapping)
            return existing_mapping
        
        # 2. Essayer la d√©tection automatique
        column_indices = self._detect_columns_automatically(header_row_idx, headers)
        
        # 3. √âvaluer la confiance du mapping automatique
        confidence_score = self._evaluate_mapping_confidence(column_indices, headers)
        
        if confidence_score >= 4:  # Mapping tr√®s confiant
            self.mapping_confidence = 'high'
            print(f"‚úÖ Mapping automatique avec haute confiance (score: {confidence_score}/5)")
        elif confidence_score >= 2:  # Mapping mod√©r√©ment confiant
            self.mapping_confidence = 'medium'
            print(f"‚ö†Ô∏è Mapping automatique avec confiance moyenne (score: {confidence_score}/5)")
            
            # Demander confirmation en mode interactif
            if not self.dry_run:
                print("\nVoulez-vous:")
                print("1. Utiliser ce mapping automatique")
                print("2. Configurer manuellement")
                choice = input("Votre choix (1/2): ").strip()
                
                if choice == '2':
                    column_indices = self.column_mapper.interactive_mapping(headers)
                    self.column_mapper.save_mapping(headers, column_indices, filename)
                    self.mapping_confidence = 'manual'
        else:  # Mapping peu confiant
            self.mapping_confidence = 'low'
            print(f"‚ùå Mapping automatique peu fiable (score: {confidence_score}/5)")
            
            if not self.dry_run:
                print("üîß Configuration manuelle recommand√©e...")
                column_indices = self.column_mapper.interactive_mapping(headers)
                self.column_mapper.save_mapping(headers, column_indices, filename)
                self.mapping_confidence = 'manual'
            else:
                print("‚ö†Ô∏è Mode dry-run: mapping automatique utilis√© malgr√© la faible confiance")
        
        self._store_column_indices(column_indices)
        return column_indices
    
    def _store_column_indices(self, column_indices: Dict[str, Optional[int]]):
        """Stocke les indices des colonnes dans l'instance"""
        self.col_designation = column_indices['designation']
        self.col_unite = column_indices['unite']
        self.col_quantite = column_indices['quantite']
        self.col_prix_unitaire = column_indices['prix_unitaire']
        self.col_prix_total = column_indices['prix_total']
        self.headers_detected = True
    
    def _evaluate_mapping_confidence(self, column_indices: Dict[str, Optional[int]], headers: List[str]) -> int:
        """√âvalue la confiance du mapping automatique"""
        score = 0
        
        # Compter les colonnes essentielles d√©tect√©es
        essential_cols = ['designation', 'unite', 'prix_unitaire']
        for col in essential_cols:
            if column_indices.get(col) is not None:
                score += 1
        
        # Bonus si on a quantit√© ou prix total
        if column_indices.get('quantite') is not None or column_indices.get('prix_total') is not None:
            score += 1
        
        # V√©rifier que les headers semblent coh√©rents
        if column_indices.get('designation') is not None and column_indices['designation'] < len(headers):
            header = headers[column_indices['designation']].lower()
            if any(word in header for word in ['designation', 'description', 'libelle', 'article']):
                score += 1
        
        return score
    
    def _detect_columns_automatically(self, header_row_idx: Optional[int], headers: List[str]) -> Dict[str, Optional[int]]:
        """D√©tection automatique des colonnes (code original)"""
        # Initialiser les indices √† None
        column_indices = {
            'designation': None,
            'unite': None,
            'quantite': None,
            'prix_unitaire': None,
            'prix_total': None
        }
        
        # Si on n'a pas d'en-t√™te, on essaie une approche par d√©faut
        if header_row_idx is None:
            print("‚ö†Ô∏è Utilisation des indices de colonne par d√©faut")
            column_indices['designation'] = 0
            
            # Pour les autres colonnes, on examine quelques lignes pour trouver des nombres
            num_rows = min(20, len(self.df))
            num_cols = min(10, len(self.df.columns))
            
            # Compter combien de valeurs num√©riques on a dans chaque colonne
            num_counts = {col: 0 for col in range(1, num_cols)}
            for row in range(5, num_rows):  # Commencer apr√®s les potentiels en-t√™tes
                for col in range(1, num_cols):
                    if pd.notna(self.df.iloc[row, col]):
                        try:
                            # Tester si la valeur est num√©rique (entier ou d√©cimal)
                            val_str = str(self.df.iloc[row, col]).replace(',', '.')
                            float(val_str)
                            num_counts[col] += 1
                        except ValueError:
                            pass
            
            # Les colonnes avec le plus de valeurs num√©riques sont probablement quantit√©/prix
            numeric_cols = sorted([(col, count) for col, count in num_counts.items() if count > 3], 
                                  key=lambda x: x[1], reverse=True)
            
            if len(numeric_cols) >= 3:
                # Supposer un ordre typique: unit√©, quantit√©, prix unitaire, prix total
                column_indices['unite'] = 1  # Souvent juste avant les nombres
                column_indices['quantite'] = numeric_cols[0][0]
                column_indices['prix_unitaire'] = numeric_cols[1][0]
                column_indices['prix_total'] = numeric_cols[2][0]
            elif len(numeric_cols) == 2:
                # Au minimum, on a besoin de quantit√© et prix
                column_indices['quantite'] = numeric_cols[0][0]
                column_indices['prix_unitaire'] = numeric_cols[1][0]
            
            print(f"D√©tection colonnes par d√©faut: {column_indices}")
            return column_indices
        
        # Si on a un en-t√™te, on cherche les correspondances avec des patterns connus
        header_row = [str(val).strip().lower() if pd.notna(val) else "" for val in self.df.iloc[header_row_idx].values]
        
        # Patterns pour chaque type de colonne
        patterns = {
            'designation': [r'd√©signation', r'designation', r'libell√©', r'libelle', r'description', r'prestation', r'article', r'd√©tail', r'detail', r'ouvrage', r'intitul√©', r'intitule', r'nature'],
            'unite': [r'unit√©', r'unite', r'u\.?$', r'un\.?$', r'un$', r'unit√© de mesure', r'mesure', r'^u$'],
            'quantite': [r'quantit√©', r'quantite', r'qt√©\.?', r'qt\.?', r'quant\.?', r'qte'],
            'prix_unitaire': [r'prix\s*(?:unitaire|unit\.?)(?:\s*h\.?t\.?)?', r'p\.u\.(?:\s*h\.?t\.?)?', r'pu(?:\s*h\.?t\.?)?', r'pu\s*ht$', r'prix\s*ht$'],
            'prix_total': [r'prix\s*(?:total|tot\.?)(?:\s*h\.?t\.?)?', r'montant(?:\s*h\.?t\.?)?', r'p\.t\.(?:\s*h\.?t\.?)?', r'pt(?:\s*h\.?t\.?)?', r'total(?:\s*h\.?t\.?)?']
        }
        
        # Chercher chaque pattern dans les cellules de la ligne d'en-t√™te
        for col_name, col_patterns in patterns.items():
            for col_idx, cell_text in enumerate(header_row):
                cell_text = cell_text.lower()
                for pattern in col_patterns:
                    if re.search(pattern, cell_text, re.IGNORECASE):
                        column_indices[col_name] = col_idx
                        print(f"Colonne '{col_name}' d√©tect√©e: indice {col_idx}, valeur: '{header_row[col_idx]}'")
                        break
                if column_indices[col_name] is not None:
                    break
        
        # Pour les colonnes non d√©tect√©es, essayer une d√©tection par position logique
        # Si la d√©signation n'est pas trouv√©e, chercher la colonne la plus large avec du texte
        if column_indices['designation'] is None:
            # Chercher la colonne avec le plus de contenu textuel dans les lignes suivantes
            max_text_col = 0
            max_text_score = 0
            
            for col_idx in range(min(5, len(header_row))):  # Examiner les 5 premi√®res colonnes
                text_score = 0
                for row_idx in range(header_row_idx + 1, min(header_row_idx + 10, len(self.df))):
                    if col_idx < len(self.df.iloc[row_idx]) and pd.notna(self.df.iloc[row_idx, col_idx]):
                        cell_value = str(self.df.iloc[row_idx, col_idx])
                        if len(cell_value) > 10:  # D√©signations sont g√©n√©ralement longues
                            text_score += len(cell_value)
                
                if text_score > max_text_score:
                    max_text_score = text_score
                    max_text_col = col_idx
            
            column_indices['designation'] = max_text_col
            print(f"‚ö†Ô∏è Colonne 'designation' non d√©tect√©e, suppos√©e √™tre √† l'indice {max_text_col} (analyse du contenu)")
        
        # V√©rifications et inf√©rences des colonnes non d√©tect√©es
        # Si on a trouv√© prix unitaire et quantit√© mais pas prix total, on cherche apr√®s prix unitaire
        if column_indices['prix_unitaire'] is not None and column_indices['quantite'] is not None and column_indices['prix_total'] is None:
            if column_indices['prix_unitaire'] + 1 < len(header_row):
                column_indices['prix_total'] = column_indices['prix_unitaire'] + 1
                print(f"‚ö†Ô∏è Colonne 'prix_total' non d√©tect√©e, suppos√©e √™tre √† l'indice {column_indices['prix_total']}")
        
        # Si on a trouv√© prix total et quantit√© mais pas prix unitaire, on cherche avant prix total
        if column_indices['prix_total'] is not None and column_indices['quantite'] is not None and column_indices['prix_unitaire'] is None:
            if column_indices['prix_total'] - 1 >= 0:
                column_indices['prix_unitaire'] = column_indices['prix_total'] - 1
                print(f"‚ö†Ô∏è Colonne 'prix_unitaire' non d√©tect√©e, suppos√©e √™tre √† l'indice {column_indices['prix_unitaire']}")
        
        # Si on a trouv√© prix unitaire mais pas quantit√©, v√©rifier si ce n'est pas un BPU (bordereau de prix unitaires)
        # Dans un BPU, il n'y a souvent pas de colonne quantit√© car ce sont des prix de r√©f√©rence
        if column_indices['prix_unitaire'] is not None and column_indices['quantite'] is None:
            # Si on a 'designation', 'unite' et 'prix_unitaire' mais pas 'quantite', c'est probablement un BPU
            if column_indices['designation'] is not None and column_indices['unite'] is not None:
                print(f"‚ÑπÔ∏è Format BPU d√©tect√© (pas de colonne quantit√©), ce qui est normal")
                # Ne pas assigner d'indice de colonne pour quantit√© dans ce cas
            else:
                # Sinon, essayer d'inf√©rer la position de la quantit√©
                if column_indices['prix_unitaire'] - 1 >= 0:
                    column_indices['quantite'] = column_indices['prix_unitaire'] - 1
                    print(f"‚ö†Ô∏è Colonne 'quantite' non d√©tect√©e, suppos√©e √™tre √† l'indice {column_indices['quantite']}")
        
        # Si l'unit√© n'est pas trouv√©e, on peut supposer qu'elle est entre d√©signation et quantit√©
        if column_indices['unite'] is None and column_indices['designation'] is not None and column_indices['quantite'] is not None:
            if column_indices['quantite'] > column_indices['designation'] + 1:
                column_indices['unite'] = column_indices['quantite'] - 1
                print(f"‚ö†Ô∏è Colonne 'unite' non d√©tect√©e, suppos√©e √™tre √† l'indice {column_indices['unite']}")
        
        # Afficher les r√©sultats de la d√©tection
        print(f"‚úì Indices des colonnes d√©tect√©s: {column_indices}")
        
        return column_indices
    
    def safe_convert_to_float(self, value) -> float:
        """
        Convertit une valeur en float de fa√ßon s√©curis√©e, en g√©rant les formats europ√©ens.
        
        Args:
            value: Valeur √† convertir (str, int, float, etc.)
            
        Returns:
            Valeur convertie en float, ou 0.0 si erreur
        """
        if pd.isna(value):
            return 0.0
        
        if isinstance(value, (int, float)):
            return float(value)
        
        try:
            # Nettoyer la valeur
            val_str = str(value).strip()
            
            # Supprimer les symboles mon√©taires et autres caract√®res
            val_str = re.sub(r'[‚Ç¨$¬£\s]', '', val_str)
            
            # Remplacer les virgules par des points (format europ√©en)
            if ',' in val_str and '.' not in val_str:
                val_str = val_str.replace(',', '.')
            
            # Traiter les cas comme "1.234,56" (format europ√©en) -> "1234.56"
            if '.' in val_str and ',' in val_str:
                if val_str.find('.') < val_str.find(','):
                    val_str = val_str.replace('.', '')
                    val_str = val_str.replace(',', '.')
            
            # Convertir en float
            return float(val_str)
        except (ValueError, TypeError):
            # Si la conversion √©choue, on retourne 0
            print(f"‚ö†Ô∏è Impossible de convertir en nombre: '{value}'")
            return 0.0
    
    def detect_sections_and_elements(self, header_row: Optional[int] = None) -> List[Dict]:
        """
        Version consid√©rablement am√©lior√©e de la d√©tection des sections et √©l√©ments d'ouvrage.
        Utilise des algorithmes adaptatifs et des heuristiques avanc√©es pour tous les formats de DPGF.
        
        Args:
            header_row: Index de la ligne d'en-t√™te
            
        Returns:
            Liste de dictionnaires avec 'type' ('section' ou 'element') et 'data'
        """
        results = []
        
        self.logger.info("==== D√âTECTION AVANC√âE DES SECTIONS ET √âL√âMENTS ====")
        
        # Trouver la ligne d'en-t√™te si non sp√©cifi√©e
        if header_row is None:
            header_row = self.find_header_row()
            self.logger.info(f"Recherche automatique de l'en-t√™te: {'trouv√© √† la ligne ' + str(header_row+1) if header_row is not None else 'non trouv√©'}")
        else:
            self.logger.info(f"Utilisation de l'en-t√™te sp√©cifi√©: ligne {header_row+1}")
        
        # D√©tecter les indices des colonnes
        if not self.headers_detected:
            self.detect_column_indices(header_row)
        
        # Si on n'a pas pu d√©tecter les colonnes essentielles, on utilise des valeurs par d√©faut
        if self.col_designation is None:
            self.col_designation = 0
            self.logger.warning("Colonne de d√©signation non d√©tect√©e, utilisation de la premi√®re colonne par d√©faut")
        
        self.logger.info(f"Colonnes utilis√©es: d√©signation={self.col_designation}, unit√©={self.col_unite}, "
              f"quantit√©={self.col_quantite}, prix unitaire={self.col_prix_unitaire}, prix total={self.col_prix_total}")
        
        print(f"Colonnes utilis√©es: d√©signation={self.col_designation}, unit√©={self.col_unite}, "
              f"quantit√©={self.col_quantite}, prix unitaire={self.col_prix_unitaire}, prix total={self.col_prix_total}")
        
        # Patterns ultra-renforc√©s pour la d√©tection des sections
        section_patterns = {
            # === PATTERNS STANDARDS ===
            # Sections num√©rot√©es standard (1.2.3 Titre)
            'numbered_standard': re.compile(r'^(\d+(?:\.\d+)*)\s+(.+)'),
            
            # Sections num√©rot√©es avec tirets ou points (1.2.3- Titre, 1.2.3. Titre)
            'numbered_punctuated': re.compile(r'^(\d+(?:\.\d+)*)[.-]\s*(.+)'),
            
            # === PATTERNS HI√âRARCHIQUES ===
            # Num√©rotation hi√©rarchique complexe (A.1.2.3, 01.02.03.04)
            'hierarchical_complex': re.compile(r'^([A-Z]?\d{1,2}(?:\.\d{1,2}){1,4})\s+(.+)'),
            
            # Num√©rotation avec pr√©fixes (A1, B2, C3)
            'letter_number': re.compile(r'^([A-Z]\d{1,3})\s+(.+)'),
            
            # === PATTERNS SP√âCIALIS√âS BTP ===
            # Sections avec num√©ros de lots (LOT 06.01, LOT 6.1)
            'lot_subsection': re.compile(r'^(LOT\s+\d{1,2}(?:\.\d+)*)\s+(.+)', re.IGNORECASE),
            
            # Articles de devis (ART. 123, Art 456)
            'article_numbered': re.compile(r'^(ART\.?\s*\d+)\s+(.+)', re.IGNORECASE),
            
            # === PATTERNS DE TITRES ===
            # Titres en majuscules (ESCALIERS METALLIQUES)
            'uppercase_title': re.compile(r'^([A-Z][A-Z\s\d\.\-\_\&\']{4,})$'),
            
            # Titres soulign√©s ou encadr√©s
            'underlined_title': re.compile(r'^([=\-_]{3,})\s*([A-Z].{3,})\s*[=\-_]{3,}$'),
            
            # Titres avec num√©rotation romaine (I. Titre, IV - Titre)
            'roman_numeral': re.compile(r'^([IVX]{1,5})[.\-\s]\s*(.+)'),
            
            # Lettres majuscules (A. Titre, B - Titre)
            'letter_numeral': re.compile(r'^([A-H])[.\-\s]\s*(.+)'),
            
            # === PATTERNS AVEC PR√âFIXES ===
            # Sections avec pr√©fixes (CHAPITRE 1, LOT 06, PARTIE A)
            'prefixed_section': re.compile(r'^(CHAPITRE|LOT|PARTIE|SECTION|SOUS-SECTION|TITRE)\s+([A-Z0-9]+)[\s\:]*(.*)'),
            
            # Sections avec pr√©fixes techniques (POSTE, OUVRAGE, PRESTATION)
            'technical_prefix': re.compile(r'^(POSTE|OUVRAGE|PRESTATION|TRAVAUX|FOURNITURE)\s+([A-Z0-9\.]+)[\s\:]*(.*)'),
            
            # === PATTERNS DE TOTAUX ===
            # Totaux et sous-totaux (SOUS-TOTAL, TOTAL GENERAL)
            'total_section': re.compile(r'^(SOUS[\-\s]*TOTAL|TOTAL|MONTANT\s+TOTAL|R√âCAPITULATIF|RECAPITULATIF)[\s\:]*(.*)'),
            
            # === PATTERNS SHAREPOINT SP√âCIAUX ===
            # Sections SharePoint sp√©ciales (5.1, 5.1.1)
            'sharepoint_numbered': re.compile(r'^(\d+\.\d+(?:\.\d+)*)\s*(.*)'),
            
            # Num√©rotation SharePoint avec tirets
            'sharepoint_dashed': re.compile(r'^(\d+\-\d+(?:\-\d+)*)\s+(.+)'),
            
            # === PATTERNS EXOTIQUES ===
            # Sections avec parenth√®ses (1) Titre, (A) Titre
            'parentheses_numbered': re.compile(r'^\(([A-Z0-9]+)\)\s+(.+)'),
            
            # Sections avec crochets [1] Titre, [A] Titre
            'brackets_numbered': re.compile(r'^\[([A-Z0-9]+)\]\s+(.+)'),
            
            # Sections avec tirets initiaux (- Titre de section)
            'dash_section': re.compile(r'^\s*[-‚Ä¢]\s+([A-Z].{5,})$'),
            
            # Sections avec puces (‚Ä¢ Titre, ‚ó¶ Titre)
            'bullet_section': re.compile(r'^\s*[‚Ä¢‚ó¶‚ñ™‚ñ´]\s+([A-Z].{5,})$'),
            
            # === PATTERNS DE NUM√âROTATION ALTERNATIVE ===
            # Num√©rotation d√©cimale fran√ßaise (1,2,3 au lieu de 1.2.3)
            'decimal_french': re.compile(r'^(\d+(?:,\d+)*)\s+(.+)'),
            
            # Num√©rotation avec suffixes (1er, 2√®me, 3√®me)
            'ordinal_french': re.compile(r'^(\d+(?:er|√®me|nd|rd|th))\s+(.+)'),
            
            # === PATTERNS CONTEXTUELS ===
            # Phases de travaux (PHASE 1, √âTAPE A)
            'phase_step': re.compile(r'^(PHASE|√âTAPE|ETAPE|STADE)\s+([A-Z0-9]+)\s*[\:\-]?\s*(.*)'),
            
            # Zones de travaux (ZONE A, SECTEUR 1)
            'zone_sector': re.compile(r'^(ZONE|SECTEUR|P√âRIM√àTRE|PERIMETRE)\s+([A-Z0-9]+)\s*[\:\-]?\s*(.*)'),
            
            # === PATTERNS MULTI-FORMATS ===
            # Format mixte alphanum√©rique (A1.2, B3.4)
            'mixed_alphanumeric': re.compile(r'^([A-Z]\d+(?:\.\d+)*)\s+(.+)'),
            
            # Codes articles complexes (ABC123, XYZ456)
            'complex_codes': re.compile(r'^([A-Z]{2,4}\d{2,4})\s+(.+)'),
            
            # === PATTERNS DE CONTINUIT√â ===
            # Num√©rotation continue avec slash (1/10, 2/10)
            'fraction_numbered': re.compile(r'^(\d+/\d+)\s+(.+)'),
            
            # Num√©rotation avec version (V1.2, REV.3)
            'version_numbered': re.compile(r'^(V\d+(?:\.\d+)*|REV\.?\d+)\s+(.+)'),
        }
        
        self.logger.info(f"Patterns de d√©tection utilis√©s: {len(section_patterns)} patterns avanc√©s")
        self.logger.debug(f"Patterns: {', '.join(section_patterns.keys())}")
        current_section = None
        
        # Si header_row est None (pas trouv√©), commencer depuis le d√©but
        start_row = header_row + 1 if header_row is not None else 0
        self.logger.info(f"D√©but de l'analyse √† partir de la ligne {start_row+1}")
        
        sections_count = 0
        elements_count = 0
        default_sections_created = 0
        
        # Variables pour l'analyse contextuelle
        last_section_level = 0
        section_hierarchy = {}
        potential_elements_without_section = []
        
        for i in range(start_row, len(self.df)):
            row = self.df.iloc[i]
            
            # Ignorer les lignes vides
            if all(pd.isna(val) for val in row.values):
                continue
            
            # V√©rifier si c'est une section (texte en d√©but de ligne)
            if pd.notna(row.iloc[self.col_designation]):
                cell_text = str(row.iloc[self.col_designation]).strip()
                section_detected = False
                
                # Essayer tous les patterns de section dans l'ordre de priorit√©
                for pattern_name, pattern in section_patterns.items():
                    match = pattern.match(cell_text)
                    if match:
                        section_data = self._extract_section_from_match(match, pattern_name, cell_text)
                        
                        if section_data:
                            # Calculer le niveau hi√©rarchique
                            niveau = self._calculate_hierarchical_level(section_data['numero_section'], pattern_name, last_section_level)
                            section_data['niveau_hierarchique'] = niveau
                            
                            # Mettre √† jour la hi√©rarchie
                            section_hierarchy[niveau] = section_data['numero_section']
                            # Nettoyer les niveaux inf√©rieurs
                            keys_to_remove = [k for k in section_hierarchy.keys() if k > niveau]
                            for k in keys_to_remove:
                                del section_hierarchy[k]
                            
                            current_section = section_data
                            last_section_level = niveau
                            
                            self.logger.log_section_detection(True, i, current_section, pattern_name, cell_text)
                            
                            results.append({
                                'type': 'section',
                                'data': current_section,
                                'row': i
                            })
                            sections_count += 1
                            section_detected = True
                            break
                
                if section_detected:
                    continue
                else:
                    self.logger.log_section_detection(False, i, None, "aucun", cell_text)
                
                # Si ce n'est pas une section, analyser si c'est un √©l√©ment
                element_analysis = self._analyze_potential_element(row, cell_text, i)
                
                if element_analysis['is_element']:
                    # Si on n'a pas encore de section, cr√©er une section par d√©faut ou utiliser les √©l√©ments en attente
                    if current_section is None:
                        if not potential_elements_without_section:
                            # Cr√©er une section par d√©faut
                            current_section = {
                                'numero_section': '1',
                                'titre_section': '√âl√©ments du bordereau',
                                'niveau_hierarchique': 1
                            }
                            results.append({
                                'type': 'section',
                                'data': current_section,
                                'row': i
                            })
                            self.logger.log_section_creation('1', '√âl√©ments du bordereau', 1, True)
                            default_sections_created += 1
                            sections_count += 1
                        else:
                            # Traiter les √©l√©ments en attente
                            for pending_element in potential_elements_without_section:
                                results.append(pending_element)
                                elements_count += 1
                            potential_elements_without_section.clear()
                    
                    # Cr√©er l'√©l√©ment avec les donn√©es extraites
                    element_data = self._create_element_data(row, element_analysis, cell_text)
                    
                    self.logger.log_element_detection(i, element_data['designation_exacte'], 
                                                      element_analysis['has_price_data'], 
                                                      element_analysis['has_unit_data'])
                    
                    element_entry = {
                        'type': 'element',
                        'data': element_data,
                        'row': i
                    }
                    
                    if current_section is not None:
                        results.append(element_entry)
                        elements_count += 1
                    else:
                        potential_elements_without_section.append(element_entry)
                    
                    continue
        
        # Traiter les √©l√©ments en attente √† la fin
        if potential_elements_without_section:
            # Cr√©er une derni√®re section par d√©faut si n√©cessaire
            if current_section is None:
                current_section = {
                    'numero_section': '1',
                    'titre_section': '√âl√©ments restants',
                    'niveau_hierarchique': 1
                }
                results.append({
                    'type': 'section',
                    'data': current_section,
                    'row': len(self.df)
                })
                sections_count += 1
                default_sections_created += 1
            
            for pending_element in potential_elements_without_section:
                results.append(pending_element)
                elements_count += 1
        
        self.logger.info(f"D√©tection termin√©e: {sections_count} sections ({default_sections_created} par d√©faut), {elements_count} √©l√©ments")
        print(f"Total √©l√©ments/sections d√©tect√©s: {len(results)} ({sections_count} sections, {elements_count} √©l√©ments)")
        return results

    def _extract_section_from_match(self, match, pattern_name: str, original_text: str) -> Optional[Dict]:
        """Extrait les donn√©es de section selon le pattern correspondant avec gestion √©tendue"""
        try:
            if pattern_name == 'uppercase_title':
                # Titre en majuscules
                titre_section = match.group(1).strip()
                # G√©n√©rer un num√©ro unique mais pr√©visible
                section_hash = abs(hash(titre_section)) % 10000
                numero_section = f"S{section_hash:04d}"
                
            elif pattern_name in ['numbered_standard', 'numbered_punctuated', 'sharepoint_numbered', 'sharepoint_dashed']:
                # Sections num√©rot√©es standards
                numero_section = match.group(1).strip()
                titre_section = match.group(2).strip() if len(match.groups()) > 1 and match.group(2) else f"Section {numero_section}"
                
            elif pattern_name in ['roman_numeral', 'letter_numeral']:
                # Num√©rotation romaine ou lettre
                numero_section = match.group(1).strip()
                titre_section = match.group(2).strip()
                
            elif pattern_name in ['prefixed_section', 'technical_prefix']:
                # Sections avec pr√©fixes (CHAPITRE, LOT, POSTE, etc.)
                prefix = match.group(1).strip()
                number = match.group(2).strip()
                title = match.group(3).strip() if len(match.groups()) > 2 and match.group(3) else ""
                numero_section = f"{prefix} {number}"
                titre_section = title if title else numero_section
                
            elif pattern_name == 'total_section':
                # Totaux et sous-totaux
                numero_section = match.group(1).strip()
                titre_section = match.group(2).strip() if len(match.groups()) > 1 and match.group(2) else numero_section
                
            elif pattern_name in ['hierarchical_complex', 'mixed_alphanumeric', 'complex_codes']:
                # Articles et codes complexes
                numero_section = match.group(1).strip()
                titre_section = match.group(2).strip()
                
            elif pattern_name in ['parentheses_numbered', 'brackets_numbered']:
                # Sections avec parenth√®ses ou crochets
                numero_section = match.group(1).strip()
                titre_section = match.group(2).strip()
                
            elif pattern_name in ['dash_section', 'bullet_section']:
                # Sections avec tirets ou puces
                titre_section = match.group(1).strip()
                numero_section = f"SEC_{abs(hash(titre_section)) % 1000:03d}"
                
            elif pattern_name in ['letter_number', 'lot_subsection', 'article_numbered']:
                # Patterns avec lettres + num√©ros
                numero_section = match.group(1).strip()
                titre_section = match.group(2).strip() if len(match.groups()) > 1 and match.group(2) else f"Section {numero_section}"
                
            elif pattern_name in ['decimal_french', 'ordinal_french', 'fraction_numbered', 'version_numbered']:
                # Num√©rotations alternatives
                numero_section = match.group(1).strip()
                titre_section = match.group(2).strip()
                
            elif pattern_name in ['phase_step', 'zone_sector']:
                # Phases et zones
                prefix = match.group(1).strip()
                identifier = match.group(2).strip()
                description = match.group(3).strip() if len(match.groups()) > 2 and match.group(3) else ""
                numero_section = f"{prefix} {identifier}"
                titre_section = description if description else numero_section
                
            elif pattern_name == 'underlined_title':
                # Titres soulign√©s
                titre_section = match.group(2).strip()
                numero_section = f"TITLE_{abs(hash(titre_section)) % 1000:03d}"
                
            else:
                # Pattern non reconnu, essayer une extraction g√©n√©rique
                if len(match.groups()) >= 2:
                    numero_section = match.group(1).strip()
                    titre_section = match.group(2).strip()
                else:
                    titre_section = match.group(1).strip()
                    numero_section = f"GEN_{abs(hash(titre_section)) % 1000:03d}"
            
            # Nettoyage et validation
            if not titre_section:
                titre_section = f"Section {numero_section}"
            
            # S'assurer que le num√©ro de section ne d√©passe pas 50 caract√®res (contrainte DB)
            if len(numero_section) > 50:
                numero_section = numero_section[:47] + "..."
            
            # S'assurer que le titre ne d√©passe pas 255 caract√®res
            if len(titre_section) > 255:
                titre_section = titre_section[:252] + "..."
            
            return {
                'numero_section': numero_section,
                'titre_section': titre_section
            }
            
        except Exception as e:
            self.logger.error(f"Erreur lors de l'extraction de section avec pattern '{pattern_name}': {e}")
            # Fallback en cas d'erreur
            safe_title = re.sub(r'[^\w\s\-]', '', original_text)[:100]
            return {
                'numero_section': f"ERR_{abs(hash(safe_title)) % 1000:03d}",
                'titre_section': safe_title if safe_title else "Section non identifi√©e"
            }

    def _calculate_hierarchical_level(self, numero_section: str, pattern_name: str, last_level: int) -> int:
        """Calcule le niveau hi√©rarchique d'une section avec gestion √©tendue des patterns"""
        if pattern_name in ['numbered_standard', 'numbered_punctuated', 'sharepoint_numbered', 
                          'hierarchical_complex', 'mixed_alphanumeric']:
            # Pour les sections num√©rot√©es, compter les points/s√©parateurs
            level = numero_section.count('.') + numero_section.count(',') + 1
            
            # Cas sp√©ciaux pour les num√©rotations complexes
            if pattern_name == 'hierarchical_complex':
                # A.1.2.3 -> niveau 4, mais A1 -> niveau 1
                if re.match(r'^[A-Z]\d+$', numero_section):
                    level = 1
                else:
                    level = numero_section.count('.') + 1
            
            return level
            
        elif pattern_name in ['uppercase_title', 'underlined_title']:
            # Les titres en majuscules sont g√©n√©ralement de niveau 1 (titres principaux)
            return 1
            
        elif pattern_name in ['roman_numeral']:
            # Num√©rotation romaine = niveau 1 g√©n√©ralement (chapitres principaux)
            return 1
            
        elif pattern_name in ['letter_numeral', 'letter_number']:
            # Lettres = niveau 2 g√©n√©ralement (sous-sections)
            return 2
            
        elif pattern_name in ['prefixed_section', 'technical_prefix']:
            # Les sections pr√©fix√©es sont souvent de niveau 1 (sections principales)
            return 1
            
        elif pattern_name == 'total_section':
            # Les totaux sont au m√™me niveau que la section pr√©c√©dente ou niveau 2
            return max(2, last_level)
            
        elif pattern_name in ['lot_subsection', 'article_numbered']:
            # Sous-sections de lot = niveau 2 ou 3
            if re.search(r'\d+\.\d+', numero_section):
                return 3  # LOT 06.01 = niveau 3
            else:
                return 2  # LOT 06 = niveau 2
                
        elif pattern_name in ['parentheses_numbered', 'brackets_numbered']:
            # Sections avec parenth√®ses/crochets = niveau 2 ou 3
            if numero_section.isdigit():
                return int(numero_section) if int(numero_section) <= 5 else 2
            else:
                return 2
                
        elif pattern_name in ['dash_section', 'bullet_section']:
            # Sections avec tirets/puces = niveau 2 g√©n√©ralement
            return 2
            
        elif pattern_name in ['decimal_french', 'ordinal_french']:
            # Num√©rotation d√©cimale fran√ßaise = compter les virgules
            return numero_section.count(',') + 1
            
        elif pattern_name in ['fraction_numbered']:
            # Num√©rotation fractionnelle (1/10) = niveau bas√© sur le premier nombre
            first_num = numero_section.split('/')[0]
            try:
                return min(int(first_num), 5)  # Max niveau 5
            except ValueError:
                return 2
                
        elif pattern_name in ['version_numbered']:
            # Num√©rotation de version = niveau 1 (versions principales)
            return 1
            
        elif pattern_name in ['phase_step', 'zone_sector']:
            # Phases et zones = niveau 1 (divisions principales)
            return 1
            
        elif pattern_name in ['sharepoint_dashed']:
            # Num√©rotation SharePoint avec tirets = compter les tirets
            return numero_section.count('-') + 1
            
        elif pattern_name in ['complex_codes']:
            # Codes complexes = niveau 2 g√©n√©ralement
            return 2
            
        else:
            # Pattern non reconnu, utiliser le niveau pr√©c√©dent ou 1 par d√©faut
            return max(1, last_level)

    def _analyze_potential_element(self, row, cell_text: str, row_index: int) -> Dict:
        """
        Analyse ultra-renforc√©e pour d√©terminer si une ligne est un √©l√©ment d'ouvrage.
        Capable de g√©rer tous les formats DPGF, m√™me exotiques, avec gestion multi-lignes.
        """
        analysis = {
            'is_element': False,
            'has_price_data': False,
            'has_unit_data': False,
            'has_designation_data': False,
            'has_article_number': False,
            'has_technical_indicators': False,
            'has_quantity_data': False,
            'is_multiline_element': False,
            'confidence_score': 0,
            'element_type': 'standard'  # 'standard', 'forfait', 'variable', 'bpu'
        }
        
        # === 1. ANALYSE DE LA D√âSIGNATION ===
        if len(cell_text) > 2:
            analysis['has_designation_data'] = True
            analysis['confidence_score'] += 1
            
            # Indicateurs techniques avanc√©s dans la d√©signation
            technical_indicators = [
                # Fourniture et pose
                'fourniture', 'pose', 'f et p', 'f&p', 'fp', 'fourni et pos√©', 'fourni pos√©',
                'installation', 'montage', 'mise en place', 'mise en ≈ìuvre', 'mise en oeuvre',
                
                # Actions sp√©cifiques du BTP
                'd√©molition', 'demolition', 'd√©pose', 'depose', 'd√©coupe', 'decoupe', 'per√ßage', 'percage',
                'calfeutrement', '√©tanch√©it√©', 'etancheite', 'isolation', 'raccordement',
                'scellement', 'fixation', 'assemblage', 'soudure', 'vissage', 'clouage',
                
                # Types d'ouvrages
                'ma√ßonnerie', 'maconnerie', 'b√©ton', 'beton', 'ferraillage', 'coffrage', 'banche',
                'charpente', 'couverture', 'zinguerie', 'bardage', 'fa√ßade', 'facade',
                'cloison', 'doublage', 'plafond', 'sol', 'carrelage', 'fa√Øence', 'faience',
                'peinture', 'enduit', 'cr√©pi', 'crepi', 'papier peint', 'tapisserie',
                'menuiserie', 'serrurerie', 'm√©tallerie', 'metallerie', 'aluminium', 'pvc',
                'plomberie', 'sanitaire', 'chauffage', 'ventilation', 'climatisation', 'vmc',
                '√©lectricit√©', 'electricite', '√©clairage', 'eclairage', 'tableau √©lectrique',
                'r√©seau', 'reseau', 'c√¢blage', 'cablage', 'gaine', 'conduit',
                
                # Mat√©riaux sp√©cifiques
                'acier', 'inox', 'galvanis√©', 'galvanise', 'laiton', 'cuivre', 'plomb',
                'pierre', 'marbre', 'granit', 'calcaire', 'gr√®s', 'gres', 'ardoise',
                'tuile', 'zinc', 'plomb', 'membrane', 'bitume', 'epdm',
                'laine de verre', 'laine de roche', 'polystyr√®ne', 'polystyrene',
                'plaque de pl√¢tre', 'ba13', 'fermacell', 'osb', 'contreplaqu√©', 'contreplaque',
                
                # √âquipements et accessoires
                'robinetterie', 'appareil', '√©quipement', 'equipement', 'accessoire',
                'poign√©e', 'poignee', 'serrure', 'cylindre', 'g√¢che', 'gache',
                'charni√®re', 'charniere', 'paumelle', 'pivot', 'rail', 'guide',
                
                # Finitions
                'finition', 'parement', 'habillage', 'protection', 'traitement',
                'lasure', 'vernis', 'teinture', 'impr√©gnation', 'impregnation'
            ]
            
            if any(indicator in cell_text.lower() for indicator in technical_indicators):
                analysis['has_technical_indicators'] = True
                analysis['confidence_score'] += 2
                
            # D√©tection des forfaits et prestations globales
            forfait_indicators = [
                'forfait', 'ft', 'global', 'ensemble', 'prestation',
                'intervention', 'd√©placement', 'deplacement', 'minimum',
                'heure', 'jour', 'semaine', 'mois', 'p√©riode', 'periode'
            ]
            
            if any(indicator in cell_text.lower() for indicator in forfait_indicators):
                analysis['element_type'] = 'forfait'
                analysis['confidence_score'] += 1
                
            # D√©tection des prix variables/provisoires
            variable_indicators = [
                'variable', 'provisoire', '√©ventuel', 'eventuel', 'optionnel',
                'selon', 'suivant', 'conform√©ment', 'conformement',
                '√† d√©finir', 'a definir', '√† pr√©ciser', 'a preciser'
            ]
            
            if any(indicator in cell_text.lower() for indicator in variable_indicators):
                analysis['element_type'] = 'variable'
                analysis['confidence_score'] += 1
        
        # === 2. ANALYSE DES NUM√âROS D'ARTICLES (RENFORC√âE) ===
        if cell_text:
            words = cell_text.split()
            if words:
                first_word = words[0].strip()
                
                # Patterns d'articles √©tendus
                article_patterns = [
                    r'^[A-Z]\d+(?:\.\d+)*$',           # A1.2.3
                    r'^\d+(?:\.\d+){1,4}$',            # 1.2.3.4
                    r'^[A-Z]{1,3}\.\d+(?:\.\d+)*$',    # ABC.1.2
                    r'^\d{2,4}[A-Z]?$',                # 1234, 123A
                    r'^[A-Z]\d{2,4}$',                 # A123
                    r'^\d+[A-Z]\d+$',                  # 1A2
                    r'^Art\.\s*\d+',                   # Art. 123
                    r'^\d+\s*-',                       # 123 -
                    r'^\d+\)',                         # 123)
                    r'^\w+\.\w+\.\w+',                 # ABC.DEF.123
                ]
                
                for pattern in article_patterns:
                    if re.match(pattern, first_word, re.IGNORECASE):
                        analysis['has_article_number'] = True
                        analysis['confidence_score'] += 2
                        break
                
                # V√©rifier si le premier mot ressemble √† un code article m√™me sans pattern strict
                if (len(first_word) >= 3 and 
                    any(c.isdigit() for c in first_word) and 
                    len(cell_text) > len(first_word) + 5):
                    analysis['has_article_number'] = True
                    analysis['confidence_score'] += 1
        
        # === 3. ANALYSE DES UNIT√âS (ULTRA-RENFORC√âE) ===
        if self.col_unite is not None and self.col_unite < len(row) and pd.notna(row.iloc[self.col_unite]):
            unit_text = str(row.iloc[self.col_unite]).strip().lower()
            if unit_text and unit_text not in ['', '0', 'nan', '-']:
                analysis['has_unit_data'] = True
                analysis['confidence_score'] += 1
                
                # Unit√©s √©tendues et variantes
                standard_units = [
                    # Surfaces
                    'm2', 'm¬≤', 'M2', 'M¬≤', 'm√®tres carr√©s', 'metres carres', 'mc', 'm.c.',
                    'dm2', 'dm¬≤', 'cm2', 'cm¬≤', 'ha', 'hectare',
                    
                    # Longueurs
                    'ml', 'm.l.', 'm√®tre lin√©aire', 'metre lineaire', 'm√®tres lin√©aires',
                    'm', 'm√®tre', 'metre', 'mm', 'millim√®tre', 'millimetre',
                    'cm', 'centim√®tre', 'centimetre', 'km', 'kilom√®tre', 'kilometre',
                    
                    # Volumes
                    'm3', 'm¬≥', 'M3', 'M¬≥', 'm√®tres cubes', 'metres cubes',
                    'dm3', 'dm¬≥', 'cm3', 'cm¬≥', 'litre', 'l', 'L',
                    
                    # Poids
                    'kg', 'kilogramme', 'kilo', 'g', 'gramme', 't', 'tonne',
                    
                    # Unit√©s de comptage
                    'u', 'un', 'unit√©', 'unite', 'pi√®ce', 'piece', 'pce', 'pc',
                    'ens', 'ensemble', 'jeu', 'lot', 's√©rie', 'serie',
                    'paire', 'pr', 'kit', 'bo√Æte', 'boite', 'sachet', 'sac',
                    
                    # Temps
                    'h', 'heure', 'j', 'jour', 'journ√©e', 'journee',
                    'semaine', 'mois', 'ann√©e', 'annee',
                    
                    # Forfaits
                    'forfait', 'ft', 'f', 'global', 'gb', 'intervention',
                    
                    # Sp√©cialis√©es BTP
                    'point', 'pt', 'passage', 'rang', 'couche',
                    'application', 'appl', 'traitement', 'trmt'
                ]
                
                if any(unit in unit_text for unit in standard_units):
                    analysis['confidence_score'] += 2
                elif len(unit_text) <= 5 and unit_text.isalpha():
                    # Unit√© courte alphab√©tique probable
                    analysis['confidence_score'] += 1
        
        # === 4. ANALYSE DES DONN√âES NUM√âRIQUES (AM√âLIOR√âE) ===
        numeric_cols_with_data = 0
        total_numeric_value = 0
        
        # V√©rifier la quantit√©
        if self.col_quantite is not None and self.col_quantite < len(row) and pd.notna(row.iloc[self.col_quantite]):
            try:
                val = self.safe_convert_to_float(row.iloc[self.col_quantite])
                if val > 0:
                    analysis['has_quantity_data'] = True
                    numeric_cols_with_data += 1
                    total_numeric_value += val
                    analysis['confidence_score'] += 1
                    
                    # Bonus pour quantit√©s coh√©rentes
                    if 0.01 <= val <= 10000:  # Plage raisonnable
                        analysis['confidence_score'] += 1
            except:
                pass
        
        # V√©rifier le prix unitaire
        if self.col_prix_unitaire is not None and self.col_prix_unitaire < len(row) and pd.notna(row.iloc[self.col_prix_unitaire]):
            try:
                val = self.safe_convert_to_float(row.iloc[self.col_prix_unitaire])
                if val > 0:
                    analysis['has_price_data'] = True
                    numeric_cols_with_data += 1
                    total_numeric_value += val
                    analysis['confidence_score'] += 2
                    
                    # Bonus pour prix coh√©rents
                    if 0.01 <= val <= 100000:  # Plage raisonnable pour un prix unitaire
                        analysis['confidence_score'] += 1
            except:
                pass
        
        # V√©rifier le prix total
        if self.col_prix_total is not None and self.col_prix_total < len(row) and pd.notna(row.iloc[self.col_prix_total]):
            try:
                val = self.safe_convert_to_float(row.iloc[self.col_prix_total])
                if val > 0:
                    analysis['has_price_data'] = True
                    numeric_cols_with_data += 1
                    total_numeric_value += val
                    analysis['confidence_score'] += 2
                    
                    # Bonus pour prix totaux coh√©rents
                    if 1 <= val <= 1000000:  # Plage raisonnable pour un prix total
                        analysis['confidence_score'] += 1
            except:
                pass
        
        # === 5. D√âTECTION MULTI-LIGNES ===
        # V√©rifier si l'√©l√©ment continue sur la ligne suivante
        if row_index + 1 < len(self.df):
            next_row = self.df.iloc[row_index + 1]
            if pd.notna(next_row.iloc[self.col_designation]):
                next_text = str(next_row.iloc[self.col_designation]).strip()
                
                # Indicateurs de continuation
                continuation_patterns = [
                    r'^-',                    # Commence par tiret
                    r'^\.',                   # Commence par point
                    r'^[a-z]',               # Commence par minuscule
                    r'^et\s',                # Commence par "et"
                    r'^ou\s',                # Commence par "ou"
                    r'^\(',                  # Commence par parenth√®se
                    r'^avec\s',              # Commence par "avec"
                    r'^comprenant\s',        # Commence par "comprenant"
                    r'^y\s*compris\s',       # Y compris
                ]
                
                if any(re.match(pattern, next_text, re.IGNORECASE) for pattern in continuation_patterns):
                    analysis['is_multiline_element'] = True
                    analysis['confidence_score'] += 1
        
        # === 6. INDICATEURS CONTEXTUELS SP√âCIALIS√âS ===
        context_indicators = [
            # Termes techniques BTP
            'selon dtu', 'selon nf', 'selon caue', 'conforme √†', 'conforme a',
            'r√®gles de l\'art', 'regles de l\'art', 'prescriptions', 'cahier des charges',
            
            # Localisation des travaux
            'en fa√ßade', 'en facade', 'en toiture', 'en combles', 'en sous-sol',
            '√† l\'√©tage', 'a l\'etage', 'au rez-de-chauss√©e', 'au rdc',
            'en ext√©rieur', 'en exterieur', 'en int√©rieur', 'en interieur',
            
            # Conditions de mise en ≈ìuvre
            'sur chantier', 'en atelier', 'en usine', '√† pied d\'≈ìuvre', 'a pied d\'oeuvre',
            'transport compris', 'livraison comprise', '√©vacuation comprise',
            
            # Prestations associ√©es
            'nettoyage compris', 'protection comprise', '√©tiquetage compris',
            'garantie comprise', 'maintenance comprise', 'entretien compris'
        ]
        
        if any(indicator in cell_text.lower() for indicator in context_indicators):
            analysis['confidence_score'] += 1
        
        # === 7. D√âTECTION DE FAUX POSITIFS ===
        false_positive_indicators = [
            # Titres et sections
            'chapitre', 'partie', 'section', 'sous-total', 'total g√©n√©ral', 'total general',
            'montant total', 'r√©capitulatif', 'recapitulatif', 'sommaire',
            
            # En-t√™tes et descriptions
            'd√©signation', 'designation', 'quantit√©', 'quantite', 'prix unitaire',
            'prix total', 'montant', 'r√©f√©rence', 'reference',
            
            # Informations g√©n√©rales
            'page', 'feuille', 'annexe', 'note', 'remarque', 'observation',
            'conditions g√©n√©rales', 'conditions generales', 'modalit√©s', 'modalites'
        ]
        
        if any(indicator in cell_text.lower() for indicator in false_positive_indicators):
            if len(cell_text) < 50:  # Si c'est court, c'est probablement un faux positif
                analysis['confidence_score'] -= 2
        
        # === 8. AJUSTEMENTS SELON LE TYPE DE DOCUMENT ===
        if analysis['element_type'] == 'forfait':
            # Pour les forfaits, moins d'exigences sur les donn√©es num√©riques
            min_score_required = 2
        elif analysis['element_type'] == 'variable':
            # Pour les √©l√©ments variables, accepter sans prix
            min_score_required = 3
        else:
            # Pour les √©l√©ments standards
            min_score_required = 3
        
        # === 9. D√âCISION FINALE MULTI-CRIT√àRES ===
        essential_criteria = (
            analysis['has_designation_data'] and
            len(cell_text) >= 5 and  # D√©signation minimum viable
            (
                analysis['has_price_data'] or 
                analysis['has_unit_data'] or 
                analysis['has_article_number'] or
                analysis['has_technical_indicators'] or
                numeric_cols_with_data >= 1
            )
        )
        
        # Crit√®res de qualit√© suppl√©mentaires
        quality_criteria = (
            analysis['confidence_score'] >= min_score_required and
            not (analysis['confidence_score'] < 0)  # Pas de score n√©gatif
        )
        
        analysis['is_element'] = essential_criteria and quality_criteria
        
        # Logging d√©taill√© pour debug
        if analysis['is_element']:
            self.logger.debug(f"√âl√©ment d√©tect√© ligne {row_index+1}: score={analysis['confidence_score']}, "
                            f"type={analysis['element_type']}, multi-lignes={analysis['is_multiline_element']}")
        elif essential_criteria:
            self.logger.debug(f"√âl√©ment potentiel rejet√© ligne {row_index+1}: score insuffisant ({analysis['confidence_score']}/{min_score_required})")
        
        return analysis

    def _create_element_data(self, row, analysis: Dict, designation: str) -> Dict:
        """
        Cr√©e les donn√©es d'un √©l√©ment d'ouvrage avec gestion avanc√©e des diff√©rents types
        et agr√©gation multi-lignes
        """
        # === GESTION MULTI-LIGNES ===
        full_designation = designation
        
        # Si c'est un √©l√©ment multi-lignes, agr√©ger les lignes suivantes
        if analysis.get('is_multiline_element', False):
            current_row_idx = self.df.index[self.df.iloc[:, self.col_designation] == designation].tolist()
            if current_row_idx:
                row_idx = current_row_idx[0]
                # Chercher les lignes de continuation
                for next_idx in range(row_idx + 1, min(row_idx + 5, len(self.df))):  # Max 5 lignes
                    if pd.notna(self.df.iloc[next_idx, self.col_designation]):
                        next_text = str(self.df.iloc[next_idx, self.col_designation]).strip()
                        
                        # V√©rifier si c'est une continuation
                        continuation_patterns = [
                            r'^-', r'^\.', r'^[a-z]', r'^et\s', r'^ou\s', r'^\(',
                            r'^avec\s', r'^comprenant\s', r'^y\s*compris\s'
                        ]
                        
                        if any(re.match(pattern, next_text, re.IGNORECASE) for pattern in continuation_patterns):
                            full_designation += " " + next_text
                            self.logger.debug(f"Agr√©gation multi-lignes: {next_text}")
                        else:
                            break
                    else:
                        break
        
        # === R√âCUP√âRATION DES DONN√âES DE BASE ===
        # Unit√© avec normalisation
        unite = ""
        if self.col_unite is not None and self.col_unite < len(row) and pd.notna(row.iloc[self.col_unite]):
            unite_raw = str(row.iloc[self.col_unite]).strip()
            unite = self._normalize_unit(unite_raw)
        
        # Quantit√© avec validation
        quantite = 0.0
        if self.col_quantite is not None and self.col_quantite < len(row) and pd.notna(row.iloc[self.col_quantite]):
            quantite = self.safe_convert_to_float(row.iloc[self.col_quantite])
            # Validation de coh√©rence
            if quantite < 0:
                self.logger.warning(f"Quantit√© n√©gative d√©tect√©e: {quantite}, conversion en valeur absolue")
                quantite = abs(quantite)
        
        # Prix unitaire avec validation
        prix_unitaire = 0.0
        if self.col_prix_unitaire is not None and self.col_prix_unitaire < len(row) and pd.notna(row.iloc[self.col_prix_unitaire]):
            prix_unitaire = self.safe_convert_to_float(row.iloc[self.col_prix_unitaire])
            if prix_unitaire < 0:
                self.logger.warning(f"Prix unitaire n√©gatif d√©tect√©: {prix_unitaire}, conversion en valeur absolue")
                prix_unitaire = abs(prix_unitaire)
        
        # Prix total avec validation et calcul intelligent
        prix_total = 0.0
        if self.col_prix_total is not None and self.col_prix_total < len(row) and pd.notna(row.iloc[self.col_prix_total]):
            prix_total = self.safe_convert_to_float(row.iloc[self.col_prix_total])
            if prix_total < 0:
                self.logger.warning(f"Prix total n√©gatif d√©tect√©: {prix_total}, conversion en valeur absolue")
                prix_total = abs(prix_total)
        
        # === CALCULS INTELLIGENTS ET COH√âRENCE ===
        # Si prix total manque mais on a quantit√© et prix unitaire
        if prix_total == 0 and quantite > 0 and prix_unitaire > 0:
            prix_total = quantite * prix_unitaire
            self.logger.debug(f"Prix total calcul√©: {quantite} √ó {prix_unitaire} = {prix_total}")
        
        # Si prix unitaire manque mais on a quantit√© et prix total
        elif prix_unitaire == 0 and quantite > 0 and prix_total > 0:
            prix_unitaire = prix_total / quantite
            self.logger.debug(f"Prix unitaire calcul√©: {prix_total} √∑ {quantite} = {prix_unitaire}")
        
        # Si quantit√© manque mais on a prix unitaire et prix total
        elif quantite == 0 and prix_unitaire > 0 and prix_total > 0:
            quantite = prix_total / prix_unitaire
            self.logger.debug(f"Quantit√© calcul√©e: {prix_total} √∑ {prix_unitaire} = {quantite}")
        
        # === GESTION DES CAS SP√âCIAUX ===
        element_type = analysis.get('element_type', 'standard')
        
        if element_type == 'forfait':
            # Pour les forfaits, la quantit√© est souvent 1
            if quantite == 0:
                quantite = 1.0
                if prix_total > 0:
                    prix_unitaire = prix_total
                self.logger.debug(f"Forfait d√©tect√©, quantit√© ajust√©e √† 1")
        
        elif element_type == 'variable':
            # Pour les √©l√©ments variables, marquer dans la d√©signation
            if 'variable' not in full_designation.lower():
                full_designation += " (Prix variable)"
        
        # === VALIDATION FINALE ===
        # V√©rifications de coh√©rence
        if quantite > 0 and prix_unitaire > 0:
            calculated_total = quantite * prix_unitaire
            if prix_total > 0 and abs(calculated_total - prix_total) > 0.02:  # Tol√©rance de 2 centimes
                self.logger.warning(f"Incoh√©rence d√©tect√©e: {quantite} √ó {prix_unitaire} = {calculated_total} ‚â† {prix_total}")
                # Prioriser le prix total s'il est coh√©rent
                if prix_total > 0:
                    prix_unitaire = prix_total / quantite
        
        # === EXTRACTION DU NUM√âRO D'ARTICLE ===
        numero_article = ""
        words = full_designation.split()
        if words and analysis.get('has_article_number', False):
            first_word = words[0].strip()
            article_patterns = [
                r'^[A-Z]\d+(?:\.\d+)*$',
                r'^\d+(?:\.\d+){1,4}$',
                r'^[A-Z]{1,3}\.\d+(?:\.\d+)*$',
                r'^\d{2,4}[A-Z]?$',
                r'^[A-Z]\d{2,4}$'
            ]
            
            for pattern in article_patterns:
                if re.match(pattern, first_word, re.IGNORECASE):
                    numero_article = first_word
                    # Retirer le num√©ro de la d√©signation pour √©viter la duplication
                    full_designation = " ".join(words[1:]).strip()
                    break
        
        return {
            'designation_exacte': full_designation[:500],  # Limiter √† 500 caract√®res
            'numero_article': numero_article[:20] if numero_article else "",
            'unite': unite[:10] if unite else "",
            'quantite': round(quantite, 4),  # Pr√©cision √† 4 d√©cimales
            'prix_unitaire_ht': round(prix_unitaire, 4),
            'prix_total_ht': round(prix_total, 2),  # Prix en centimes
            'type_element': element_type,
            'multilignes': analysis.get('is_multiline_element', False)
        }
    
    def _normalize_unit(self, unit_raw: str) -> str:
        """Normalise les unit√©s pour uniformiser les donn√©es"""
        unit_normalized = unit_raw.lower().strip()
        
        # Dictionnaire de normalisation des unit√©s
        unit_mapping = {
            # Surfaces
            'm¬≤': 'm2', 'M¬≤': 'm2', 'M2': 'm2', 'm√®tres carr√©s': 'm2', 'metres carres': 'm2',
            'dm¬≤': 'dm2', 'cm¬≤': 'cm2', 'hectare': 'ha',
            
            # Longueurs
            'm.l.': 'ml', 'm√®tre lin√©aire': 'ml', 'metre lineaire': 'ml', 'm√®tres lin√©aires': 'ml',
            'm√®tre': 'm', 'metre': 'm', 'millim√®tre': 'mm', 'millimetre': 'mm',
            'centim√®tre': 'cm', 'centimetre': 'cm', 'kilom√®tre': 'km', 'kilometre': 'km',
            
            # Volumes
            'm¬≥': 'm3', 'M¬≥': 'm3', 'M3': 'm3', 'm√®tres cubes': 'm3', 'metres cubes': 'm3',
            'dm¬≥': 'dm3', 'cm¬≥': 'cm3', 'litre': 'L',
            
            # Poids
            'kilogramme': 'kg', 'kilo': 'kg', 'gramme': 'g', 'tonne': 't',
            
            # Unit√©s de comptage
            'unit√©': 'u', 'unite': 'u', 'pi√®ce': 'u', 'piece': 'u', 'pce': 'u', 'pc': 'u',
            'ensemble': 'ens', 'paire': 'pr', 'bo√Æte': 'boite', 'boite': 'boite',
            
            # Temps
            'heure': 'h', 'jour': 'j', 'journ√©e': 'j', 'journee': 'j',
            
            # Forfaits
            'forfait': 'ft', 'global': 'gb'
        }
        
        # Chercher une correspondance exacte
        if unit_normalized in unit_mapping:
            return unit_mapping[unit_normalized]
        
        # Chercher une correspondance partielle
        for original, normalized in unit_mapping.items():
            if original in unit_normalized:
                return normalized
        
        # Si pas de correspondance, retourner l'unit√© nettoy√©e
        return unit_raw[:10]  # Limiter √† 10 caract√®res


class GeminiProcessor:
    """Traitement des donn√©es avec l'API Gemini"""
    
    def __init__(self, api_key: str, chunk_size: int = 20):
        if not GEMINI_AVAILABLE:
            raise ImportError("Le module google.generativeai n'est pas disponible")
        
        self.api_key = api_key
        self.chunk_size = chunk_size
        self.cache = GeminiCache()
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel('gemini-1.5-flash')
        self.stats = ImportStats()
        
        # Flags pour le fallback automatique
        self.gemini_failed = False
        self.consecutive_failures = 0
        self.max_failures_before_fallback = 3
    
    def read_excel_chunks(self, file_path: str) -> Generator[pd.DataFrame, None, None]:
        """Lit un fichier Excel par chunks pour √©conomiser la m√©moire"""
        print(f"Lecture du fichier par chunks de {self.chunk_size} lignes...")
        
        # Lire par chunks
        skip_rows = 0
        # D√©tection automatique du moteur Excel selon l'extension
        engine = detect_excel_engine(file_path)
        
        while True:
            try:
                chunk = pd.read_excel(
                    file_path, 
                    engine=engine,
                    skiprows=range(1, skip_rows + 1) if skip_rows > 0 else None,
                    nrows=self.chunk_size
                )
                
                if chunk.empty:
                    break
                
                yield chunk
                skip_rows += self.chunk_size
                
            except Exception as e:
                print(f"Erreur lecture chunk √† partir de la ligne {skip_rows}: {e}")
                break
    
    def classify_chunk(self, df_chunk: pd.DataFrame, chunk_offset: int = 0) -> List[Dict]:
        """Classifie un chunk avec Gemini + cache + fallback intelligent"""
        
        # Si Gemini a √©chou√© trop de fois, ne pas essayer
        if self.gemini_failed:
            print(f"‚ö†Ô∏è Gemini en mode fallback (trop d'√©checs) - chunk ignor√©")
            return []
        
        # Pr√©parer les donn√©es du chunk
        chunk_rows = []
        for i, row in df_chunk.iterrows():
            row_values = [str(val) if pd.notna(val) else "" for val in row.values]
            if any(val.strip() for val in row_values):
                chunk_rows.append(f"Ligne {chunk_offset + i}: {row_values}")
        
        if not chunk_rows:
            return []
        
        # V√©rifier le cache
        cached_result = self.cache.get(chunk_rows)
        if cached_result:
            self.stats.cache_hits += 1
            print(f"Cache hit pour chunk de {len(chunk_rows)} lignes")
            # Ajuster les num√©ros de ligne pour le chunk actuel
            for item in cached_result:
                item['row'] += chunk_offset
            return cached_result
        
        # Appel Gemini si pas en cache
        self.stats.gemini_calls += 1
        result = self._call_gemini_api(chunk_rows, chunk_offset)
        
        # V√©rifier si Gemini a retourn√© un r√©sultat valide
        if result is None:  # Erreur d'API
            self.consecutive_failures += 1
            print(f"‚ö†Ô∏è √âchec Gemini #{self.consecutive_failures} pour ce chunk")
            
            if self.consecutive_failures >= self.max_failures_before_fallback:
                self.gemini_failed = True
                print(f"‚ùå Gemini d√©sactiv√© apr√®s {self.consecutive_failures} √©checs cons√©cutifs (quota/erreur API)")
            
            return []
        elif result == []:  # R√©sultat vide mais pas d'erreur
            return []
        else:  # Succ√®s
            self.consecutive_failures = 0  # Reset du compteur d'√©checs
            
            # Mettre en cache (avec les num√©ros de ligne relatifs)
            cache_result = []
            for item in result:
                cache_item = item.copy()
                cache_item['row'] -= chunk_offset  
                cache_result.append(cache_item)
            self.cache.set(chunk_rows, cache_result)
            
            return result
    
    def detect_lot_info(self, file_path: str, filename: str) -> Optional[Tuple[str, str]]:
        """
        Utilise Gemini pour d√©tecter les informations de lot depuis le fichier Excel
        
        Args:
            file_path: Chemin vers le fichier Excel
            filename: Nom du fichier (pour contexte)
            
        Returns:
            Tuple (numero_lot, nom_lot) ou None si non d√©tect√©
        """
        try:
            print(f"üß† D√©tection du lot avec Gemini depuis {filename}")
            
            # Lire les premi√®res lignes du fichier pour l'analyse
            df = pd.read_excel(file_path, nrows=20)  # Les 20 premi√®res lignes suffisent
            
            # Convertir les donn√©es en texte pour Gemini
            content_lines = []
            content_lines.append(f"NOM DU FICHIER: {filename}")
            content_lines.append("")
            
            # Ajouter le contenu des premi√®res cellules
            for i in range(min(15, len(df))):
                row_data = []
                for j in range(min(10, len(df.columns))):  # Premi√®res 10 colonnes
                    cell_value = df.iloc[i, j]
                    if pd.notna(cell_value):
                        row_data.append(str(cell_value).strip())
                    else:
                        row_data.append("")
                
                if any(cell for cell in row_data):  # Ne pas inclure les lignes vides
                    content_lines.append(f"Ligne {i}: " + " | ".join(row_data))
            
            content_text = "\n".join(content_lines)
            
            # Prompt pour Gemini
            prompt = f"""
Analyse ce document DPGF/BPU/DQE et identifie les informations de lot.

CONTENU DU DOCUMENT:
{content_text}

T√ÇCHE:
1. Identifie le num√©ro de lot (g√©n√©ralement entre 1 et 99)
2. Identifie le nom/description du lot

EXEMPLES DE FORMATS POSSIBLES:
- "LOT 06 - M√âTALLERIE SERRURERIE"
- "Lot 4 - Charpente & Ossature bois"
- "DPGF Lot 10 - Platrerie"
- Ou simplement dans le nom de fichier

R√âPONSE REQUISE:
Si tu identifies un lot, r√©ponds EXACTEMENT au format:
LOT_FOUND:num√©ro|description

Si aucun lot n'est identifi√© clairement, r√©ponds:
NO_LOT_FOUND

Exemples de r√©ponses valides:
LOT_FOUND:06|M√âTALLERIE SERRURERIE
LOT_FOUND:4|Charpente & Ossature bois
NO_LOT_FOUND
"""

            # Appel √† Gemini
            response = self.model.generate_content(prompt)
            response_text = response.text.strip()
            
            print(f"   R√©ponse Gemini: {response_text}")
            
            # Parser la r√©ponse
            if response_text.startswith("LOT_FOUND:"):
                lot_info = response_text.replace("LOT_FOUND:", "").strip()
                if "|" in lot_info:
                    parts = lot_info.split("|", 1)
                    numero_lot = parts[0].strip()
                    nom_lot = parts[1].strip()
                    
                    # Validation du num√©ro de lot
                    try:
                        int_lot = int(numero_lot)
                        if 1 <= int_lot <= 99:
                            print(f"‚úÖ Lot d√©tect√© par Gemini: {numero_lot} - {nom_lot}")
                            return (numero_lot, nom_lot)
                    except ValueError:
                        pass
            
            print("‚ö†Ô∏è Gemini n'a pas pu identifier un lot valide")
            return None
            
        except Exception as e:
            print(f"‚ùå Erreur lors de la d√©tection du lot avec Gemini: {e}")
            return None
    
    def _call_gemini_api(self, chunk_rows: List[str], chunk_offset: int) -> List[Dict]:
        """Appel direct √† l'API Gemini avec d√©tection d'erreurs am√©lior√©e"""
        data_text = "\n".join(chunk_rows)
        
        prompt = f"""
        Analyse ce chunk de fichier Excel DPGF ligne par ligne.
        
        Classifie chaque ligne comme :
        - "section" : Titre de section (ex: "2.9 FERRURES", "LOT 06")
        - "element" : √âl√©ment d'ouvrage avec prix/quantit√© 
        - "ignore" : Ligne vide, en-t√™te, non pertinente
        
        Pour les SECTIONS, extrais :
        - numero_section, titre_section, niveau_hierarchique
        
        Pour les ELEMENTS, extrais :
        - designation_exacte (OBLIGATOIRE - m√™me si vide mettre "Description manquante")
        - unite, quantite, prix_unitaire_ht, prix_total_ht
        
        Donn√©es :
        {data_text}
        
        R√©ponds en JSON : [{{"row": N, "type": "section|element|ignore", "data": {{...}}}}]
        """
        
        try:
            response = self.model.generate_content(prompt)
            response_text = response.text.strip()
            
            # Nettoyer le JSON
            if response_text.startswith('```json'):
                response_text = response_text.split('```json')[1].split('```')[0]
            elif response_text.startswith('```'):
                response_text = response_text.split('```')[1]
            
            result = json.loads(response_text.strip())
            print(f"Gemini a classifi√© {len(result)} lignes du chunk")
            return result
            
        except Exception as e:
            error_msg = str(e).lower()
            
            # D√©tecter les erreurs sp√©cifiques d'API
            if any(keyword in error_msg for keyword in ['429', 'quota', 'rate limit', 'too many requests']):
                print(f"‚ùå Quota Gemini d√©pass√©: {e}")
            elif any(keyword in error_msg for keyword in ['401', 'unauthorized', 'api key']):
                print(f"‚ùå Erreur d'authentification Gemini: {e}")
            elif any(keyword in error_msg for keyword in ['500', 'internal', 'server']):
                print(f"‚ùå Erreur serveur Gemini: {e}")
            else:
                print(f"‚ùå Erreur Gemini pour chunk: {e}")
            
            # Retourner None pour indiquer une erreur d'API (diff√©rent de [])
            return None


class DPGFImporter:
    """Importeur complet de DPGF avec d√©tection intelligente des colonnes"""
    
    def __init__(self, base_url: str = "http://127.0.0.1:8000", gemini_key: Optional[str] = None, 
                 use_gemini: bool = False, chunk_size: int = 20, debug: bool = False, dry_run: bool = False):
        self.base_url = base_url
        self.stats = ImportStats()
        self.gemini_key = gemini_key
        self.use_gemini = use_gemini and gemini_key and GEMINI_AVAILABLE
        self.chunk_size = chunk_size
        self.debug = debug
        self.dry_run = dry_run
        
        # Initialiser les nouveaux composants
        self.column_mapper = ColumnMapping()
        self.error_reporter = ErrorReporter()
        
        # Initialiser le processeur Gemini si demand√©
        self.gemini = None
        if self.use_gemini:
            try:
                self.gemini = GeminiProcessor(api_key=gemini_key, chunk_size=chunk_size)
                print(f"‚úÖ Mode IA activ√©: classification avec Gemini (chunks de {chunk_size} lignes)")
            except Exception as e:
                print(f"‚ùå Erreur initialisation Gemini: {e}")
                self.use_gemini = False
    
    def get_or_create_client(self, client_name: str) -> int:
        """R√©cup√®re ou cr√©e un client dans l'API"""
        if not client_name:
            raise ValueError("Nom de client requis")
        
        # 1. Essayer de trouver le client existant
        try:
            response = requests.get(f"{self.base_url}/api/v1/clients")
            response.raise_for_status()
            
            clients = response.json()
            for client in clients:
                if client.get('nom_client', '').upper() == client_name.upper():
                    print(f"‚úÖ Client existant trouv√©: {client_name} (ID: {client['id_client']})")
                    return client['id_client']
        
        except Exception as e:
            print(f"Erreur lors de la recherche de clients: {e}")
        
        # 2. Cr√©er le client s'il n'existe pas
        try:            
            # Utiliser 'nom_client' comme attendu par le sch√©ma ClientCreate
            client_payload = {
                'nom_client': client_name,
            }
            
            response = requests.post(f"{self.base_url}/api/v1/clients", json=client_payload)
            response.raise_for_status()
            
            client_id = response.json()['id_client']
            print(f"‚úÖ Nouveau client cr√©√©: {client_name} (ID: {client_id})")
            return client_id
            
        except Exception as e:
            print(f"‚ùå Erreur cr√©ation client {client_name}: {e}")
           
            raise
            
    def get_or_create_dpgf(self, client_id: int, nom_projet: str, file_path: str) -> int:
        """R√©cup√®re ou cr√©e un DPGF pour le client"""
        fichier_source = Path(file_path).name
        
        # 1. Chercher DPGF existant UNIQUEMENT par fichier source exact
        try:
            response = requests.get(f"{self.base_url}/api/v1/dpgf", params={'id_client': client_id})
            response.raise_for_status()
            
            dpgfs = response.json()
            # Recherche UNIQUEMENT par fichier source pour √©viter les confusions
            for dpgf in dpgfs:
                if dpgf.get('fichier_source') == fichier_source:
                    print(f"‚úÖ DPGF existant trouv√© (fichier source identique): {dpgf['nom_projet']} (ID: {dpgf['id_dpgf']})")
                    return dpgf['id_dpgf']
                    
            print(f"üÜï Aucun DPGF existant trouv√© pour le fichier {fichier_source}. Cr√©ation d'un nouveau DPGF.")
        
        except Exception as e:
            print(f"Erreur lors de la recherche de DPGF: {e}")
        
        # 2. Cr√©er nouveau DPGF (toujours cr√©er un nouveau pour chaque fichier unique)
        try:
            # S'assurer que le nom du projet est unique en ajoutant le nom du fichier
            if fichier_source not in nom_projet:
                nom_projet_unique = f"{nom_projet} - {fichier_source}"
            else:
                nom_projet_unique = nom_projet
                
            # Adapter le payload au sch√©ma DPGFCreate attendu
            dpgf_payload = {
                'id_client': client_id,
                'nom_projet': nom_projet_unique,
                'date_dpgf': date.today().isoformat(),
                'statut_offre': 'en_cours',
                'fichier_source': fichier_source
            }
            
            response = requests.post(f"{self.base_url}/api/v1/dpgf", json=dpgf_payload)
            response.raise_for_status()
            
            dpgf_id = response.json()['id_dpgf']
            print(f"‚úÖ Nouveau DPGF cr√©√©: {nom_projet} (ID: {dpgf_id})")
            return dpgf_id
            
        except Exception as e:
            print(f"‚ùå Erreur cr√©ation DPGF {nom_projet}: {e}")
            raise
    
    def get_or_create_lot(self, dpgf_id: int, numero_lot: str, nom_lot: str) -> int:
        """R√©cup√®re ou cr√©e un lot dans l'API"""
        # 1. V√©rifier si le lot existe d√©j√†
        try:
            response = requests.get(f"{self.base_url}/api/v1/lots", params={'id_dpgf': dpgf_id})
            response.raise_for_status()
            
            lots = response.json()
            for lot in lots:
                if lot.get('numero_lot') == numero_lot:
                    print(f"üîÑ Lot existant r√©utilis√©: {numero_lot} - {lot.get('nom_lot')}")
                    self.stats.lots_reused += 1
                    return lot['id_lot']
        
        except Exception as e:
            print(f"Erreur lors de la recherche de lots: {e}")
        
        # 2. Cr√©er le lot s'il n'existe pas
        try:
            lot_payload = {
                'id_dpgf': dpgf_id,
                'numero_lot': numero_lot,
                'nom_lot': nom_lot
            }
            
            response = requests.post(f"{self.base_url}/api/v1/lots", json=lot_payload)
            response.raise_for_status()
            
            lot_id = response.json()['id_lot']
            print(f"‚úÖ Nouveau lot cr√©√©: {numero_lot} - {nom_lot} (ID: {lot_id})")
            self.stats.lots_created += 1
            return lot_id
            
        except Exception as e:
            print(f"‚ùå Erreur cr√©ation lot {numero_lot}: {e}")
            raise
    
    def create_section(self, lot_id: int, section_data: Dict) -> int:
        """Cr√©e une section unique ou la r√©cup√®re si elle existe d√©j√†"""
        numero = section_data.get('numero_section', '')
        niveau_hierarchique = section_data.get('niveau_hierarchique', 1)
        
        # Mode dry-run: ne fait que simuler
        if self.dry_run:
            print(f"[DRY-RUN] Section: {numero} - {section_data.get('titre_section', '')}")
            return -1  # ID fictif pour le dry-run
        
        # 1. V√©rifier si une section avec ce num√©ro existe d√©j√† dans ce lot
        try:
            response = requests.get(f"{self.base_url}/api/v1/sections", params={'lot_id': lot_id})
            response.raise_for_status()
            
            sections = response.json()
            for section in sections:
                if section.get('numero_section') == numero:
                    print(f"üîÑ Section existante r√©utilis√©e: {numero} - {section.get('titre_section')}")
                    self.stats.sections_reused += 1
                    return section['id_section']
        except Exception as e:
            print(f"Erreur lors de la recherche de section existante: {e}")
        
        # 2. Cr√©er la section si elle n'existe pas
        payload = {
            'id_lot': lot_id,
            'section_parent_id': None,
            'numero_section': numero,
            'titre_section': section_data.get('titre_section', ''),
            'niveau_hierarchique': niveau_hierarchique
        }
        
        try:
            response = requests.post(f"{self.base_url}/api/v1/sections", json=payload)
            response.raise_for_status()
            section_id = response.json()['id_section']
            print(f"‚ûï Nouvelle section cr√©√©e: {numero} - {section_data.get('titre_section')}")
            self.stats.sections_created += 1
            return section_id
        except requests.exceptions.HTTPError as e:
            if e.response.status_code == 500:
                error_details = e.response.text
                print(f"Erreur 500 d√©taill√©e pour la section: {error_details}")
            self.stats.errors += 1
            raise
    
    def create_element(self, section_id: int, element_data: Dict, row_number: int = 0):
        """Cr√©e un √©l√©ment d'ouvrage avec gestion d'erreur am√©lior√©e"""
        filename = Path(self.file_path).name if hasattr(self, 'file_path') else "inconnu"
        
        # Mode dry-run: ne fait que simuler
        if self.dry_run:
            print(f"[DRY-RUN] √âl√©ment: {element_data.get('designation_exacte', 'N/A')}")
            return {"id_element": -1}  # ID fictif
        
        # Afficher les valeurs en mode debug pour diagnostiquer les probl√®mes
        if self.debug:
            print(f"DEBUG - Donn√©es √©l√©ment: {element_data}")
        
        # Nettoyage des donn√©es avec gestion des valeurs nulles
        def safe_float(value, default=0.0):
            if value is None:
                return default
            try:
                return float(value)
            except (ValueError, TypeError):
                return default
        
        # Nettoyer et afficher les valeurs num√©riques pour debug
        quantite = safe_float(element_data.get('quantite'))
        prix_unitaire = safe_float(element_data.get('prix_unitaire_ht'))
        prix_total = safe_float(element_data.get('prix_total_ht'))
        
        if self.debug:
            print(f"DEBUG - Valeurs num√©riques converties: quantit√©={quantite}, PU={prix_unitaire}, PT={prix_total}")
        
        cleaned_data = {
            'id_section': section_id,
            'designation_exacte': element_data.get('designation_exacte', 'Description manquante'),
            'unite': str(element_data.get('unite', ''))[:10],
            'quantite': quantite,
            'prix_unitaire_ht': prix_unitaire,
            'prix_total_ht': prix_total,
            'offre_acceptee': False
        }
        
        try:
            response = requests.post(f"{self.base_url}/api/v1/element_ouvrages", json=cleaned_data)
            response.raise_for_status()
            self.stats.elements_created += 1
            return response.json()
        except requests.exceptions.HTTPError as e:
            self.stats.errors += 1
            error_msg = f"Erreur API HTTP {e.response.status_code}: {e.response.text if e.response else str(e)}"
            
            # Ajouter l'erreur au rapport
            self.error_reporter.add_error(
                filename=filename,
                line_number=row_number,
                error_type="API_ERROR",
                error_message=error_msg,
                raw_data=str(element_data)
            )
            
            print(f"‚ùå Erreur cr√©ation √©l√©ment ligne {row_number}: {error_msg}")
            if e.response and e.response.status_code == 500:
                error_details = e.response.text
                print(f"Erreur 500 d√©taill√©e: {error_details}")
            raise
        except Exception as e:
            self.stats.errors += 1
            error_msg = f"Erreur inattendue: {str(e)}"
            
            # Ajouter l'erreur au rapport
            self.error_reporter.add_error(
                filename=filename,
                line_number=row_number,
                error_type="PROCESSING_ERROR",
                error_message=error_msg,
                raw_data=str(element_data)
            )
            
            print(f"‚ùå Erreur inattendue ligne {row_number}: {error_msg}")
            raise    
            
    def classify_with_gemini(self, description: str) -> str:
        """
        Classifie une description d'√©l√©ment d'ouvrage en utilisant l'API Google Gemini.
        
        Args:
            description: Description de l'√©l√©ment √† classifier
        
        Returns:
            R√©sultat de la classification (texte)
        """
        if not GEMINI_AVAILABLE:
            raise RuntimeError("API Google Gemini non disponible")
        
        try:
            # Appel √† l'API Gemini pour la classification en utilisant GenerativeModel
            model = genai.GenerativeModel('gemini-1.5-flash')
            prompt = f"Classifie l'√©l√©ment de construction suivant dans une cat√©gorie appropri√©e (max 3 mots): {description}"
            
            response = model.generate_content(prompt, 
                generation_config=genai.GenerationConfig(
                    temperature=0.2,
                    max_output_tokens=60,
                    top_p=0.95,
                    top_k=40
                )
            )
            
            # Extraire et retourner le r√©sultat
            if response and hasattr(response, 'text'):
                result = response.text.strip()
                print(f"üîç Classification Gemini: {result}")
                return result
            
            return "Classification inconnue"
        
        except Exception as e:
            print(f"‚ùå Erreur classification Gemini: {e}")
            return "Erreur classification"
            
    def import_file(self, file_path: str, dpgf_id: Optional[int] = None, lot_num: Optional[str] = None, original_filename: Optional[str] = None):
        """Import complet d'un fichier DPGF Excel avec mapping interactif et rapport d'erreurs"""
        self.file_path = file_path  # Pour les rapports d'erreur
        filename = Path(file_path).name
        
        print(f"\nüìÅ Import DPGF: {file_path}")
        print(f"Mode traitement: {'Gemini AI' if self.use_gemini else 'Analyse classique'}")
        print(f"Mode debug: {'Activ√©' if self.debug else 'D√©sactiv√©'}")
        print(f"Mode dry-run: {'Activ√©' if self.dry_run else 'D√©sactiv√©'}")
        
        try:
            # Initialiser le parser avec les nouveaux composants
            parser = ExcelParser(file_path, self.column_mapper, self.error_reporter, self.dry_run, 
                               self.gemini if hasattr(self, 'gemini') else None)
            client_detector = ClientDetector()
            
            # 1. D√©tecter le client si n√©cessaire
            # Utiliser le nom original du fichier pour la d√©tection si fourni
            filename_for_detection = original_filename if original_filename else file_path
            client_name = client_detector.detect_client(filename_for_detection)
            if not client_name:
                client_name = "Client par d√©faut"
                print(f"‚ö†Ô∏è Utilisation d'un nom de client par d√©faut: {client_name}")
            
            if not self.dry_run:
                client_id = self.get_or_create_client(client_name)
            else:
                client_id = -1  # ID fictif pour dry-run
                print(f"[DRY-RUN] Client: {client_name}")
            
            # 2. Obtenir ou cr√©er le DPGF
            if not dpgf_id:
                project_name = parser.detect_project_name(client_name)
                if not self.dry_run:
                    dpgf_id = self.get_or_create_dpgf(client_id, project_name, file_path)
                else:
                    dpgf_id = -1  # ID fictif pour dry-run
                    print(f"[DRY-RUN] DPGF: {project_name}")
            
            # 3. D√©tecter ou utiliser le lot sp√©cifi√©
            if lot_num:
                # Utiliser un nom g√©n√©rique pour le lot sp√©cifi√©
                parser.logger.info(f"Utilisation du lot sp√©cifi√© en param√®tre: {lot_num}")
                if not self.dry_run:
                    lot_id = self.get_or_create_lot(dpgf_id, lot_num, f"Lot {lot_num}")
                    parser.logger.log_lot_creation(lot_num, f"Lot {lot_num}", "parameter")
                else:
                    lot_id = -1  # ID fictif pour dry-run
                    print(f"[DRY-RUN] Lot: {lot_num} - Lot {lot_num}")
            else:
                # Rechercher dans le fichier
                lots = parser.find_lot_headers()
                if not lots:
                    parser.logger.warning("AUCUN LOT TROUV√â - Cr√©ation d'un lot par d√©faut pour √©viter la perte de donn√©es")
                    if not self.dry_run:
                        # Cr√©er un lot par d√©faut au lieu d'√©chouer
                        default_lot_num = "00"
                        default_lot_name = f"Lot par d√©faut - {Path(file_path).stem}"
                        lot_id = self.get_or_create_lot(dpgf_id, default_lot_num, default_lot_name)
                        parser.logger.log_lot_creation(default_lot_num, default_lot_name, "fallback", True)
                        print(f"‚ö†Ô∏è Aucun lot trouv√©, cr√©ation d'un lot par d√©faut: {default_lot_num} - {default_lot_name}")
                        lot_num, lot_name = default_lot_num, default_lot_name
                    else:
                        print("[DRY-RUN] Aucun lot trouv√©, cr√©ation d'un lot par d√©faut")
                        lot_id = -1
                        lot_num, lot_name = "00", "Lot par d√©faut"
                else:
                    # Utiliser le premier lot trouv√©
                    lot_num, lot_name = lots[0]
                    parser.logger.info(f"Utilisation du lot d√©tect√©: {lot_num} - {lot_name}")
                    if not self.dry_run:
                        lot_id = self.get_or_create_lot(dpgf_id, lot_num, lot_name)
                        parser.logger.log_lot_creation(lot_num, lot_name, "detection")
                    else:
                        lot_id = -1
                        print(f"[DRY-RUN] Lot: {lot_num} - {lot_name}")
            
            # 4. Extraire les sections et √©l√©ments
            items = []
            gemini_fallback_triggered = False
            
            if self.use_gemini:
                # Utiliser Gemini pour la classification avanc√©e
                print(f"üß† Traitement avec Gemini (mode IA avanc√©)")
                
                chunks_processed = 0
                for chunk_num, df_chunk in enumerate(self.gemini.read_excel_chunks(file_path)):
                    print(f"\nTraitement chunk {chunk_num + 1} (lignes {chunk_num*self.chunk_size}-{chunk_num*self.chunk_size + len(df_chunk)})")
                    classified_rows = self.gemini.classify_chunk(df_chunk, chunk_num*self.chunk_size)
                    items.extend(classified_rows)
                    chunks_processed += 1
                    
                    # Si Gemini a √©chou√©, on va basculer vers la m√©thode classique
                    if self.gemini.gemini_failed:
                        print(f"üîÑ FALLBACK AUTOMATIQUE: Basculement vers la m√©thode classique")
                        gemini_fallback_triggered = True
                        break
                
                # Mettre √† jour les statistiques depuis Gemini
                self.stats.cache_hits = self.gemini.stats.cache_hits
                self.stats.gemini_calls = self.gemini.stats.gemini_calls
                
                # V√©rifier si Gemini n'a pas produit de r√©sultats du tout
                if not items and chunks_processed > 0:
                    print(f"‚ö†Ô∏è Gemini n'a produit aucun r√©sultat - activation du fallback")
                    gemini_fallback_triggered = True
            
            # Si Gemini a √©chou√© ou n'est pas utilis√©, utiliser la m√©thode classique
            if not self.use_gemini or gemini_fallback_triggered:
                if gemini_fallback_triggered:
                    print(f"üîß FALLBACK: Analyse classique apr√®s √©chec de Gemini")
                else:
                    print(f"üìã Analyse classique (d√©tection automatique des colonnes)")
                
                # Utiliser l'analyse classique avec d√©tection automatique des colonnes
                header_row = parser.find_header_row()
                
                if header_row is None:
                    print("‚ö†Ô∏è Impossible de trouver les en-t√™tes du tableau DPGF")
                
                # D√©tecter les colonnes
                col_indices = parser.detect_column_indices(header_row)
                
                # En mode debug, on affiche les premi√®res lignes du fichier pour aider au diagnostic
                if self.debug:
                    print("\nDEBUG - Aper√ßu des 15 premi√®res lignes:")
                    for i in range(min(15, len(parser.df))):
                        row_values = [str(val) if pd.notna(val) else "" for val in parser.df.iloc[i].values]
                        print(f"Ligne {i}: {row_values}")
                    
                    if header_row is not None:
                        print(f"\nDEBUG - Ligne d'en-t√™te (ligne {header_row}):")
                        header_values = [str(val) if pd.notna(val) else "" for val in parser.df.iloc[header_row].values]
                        print(f"Valeurs: {header_values}")
                
                # Utiliser la nouvelle m√©thode de d√©tection des sections et √©l√©ments
                classic_items = parser.detect_sections_and_elements(header_row)
                
                # Si c'est un fallback, remplacer les items de Gemini par ceux de la m√©thode classique
                if gemini_fallback_triggered:
                    items = classic_items
                    self.stats.gemini_fallback_used = True
                    if self.gemini.gemini_failed:
                        self.stats.gemini_failure_reason = f"Gemini a √©chou√© apr√®s {self.gemini.consecutive_failures} tentatives"
                    else:
                        self.stats.gemini_failure_reason = "Gemini n'a produit aucun r√©sultat"
                    print(f"‚úÖ Fallback termin√©: {len(items)} items d√©tect√©s par la m√©thode classique")
                else:
                    items = classic_items
                
                # Si on √©tait en mode fallback, remplacer les items Gemini par les items classiques
                if gemini_fallback_triggered:
                    items = classic_items
                    print(f"‚úÖ Fallback termin√©: {len(items)} items d√©tect√©s par la m√©thode classique")
                else:
                    items = classic_items
            
            # Debug: afficher tous les items avant filtrage
            if self.debug:
                print(f"\nDEBUG - Items d√©tect√©s avant filtrage ({len(items)}):")
                for i, item in enumerate(items[:10]):  # Limiter √† 10 pour √©viter trop de logs
                    print(f"  {i}: {item}")
                if len(items) > 10:
                    print(f"  ... et {len(items) - 10} autres")
            
            # Filtrer les items ignor√©s
            items_before_filter = len(items)
            items = [item for item in items if item.get('type') != 'ignore']
            items_after_filter = len(items)
            
            print(f"üîç {items_after_filter} items valides d√©tect√©s sur {items_before_filter} total ({sum(1 for i in items if i.get('type') == 'section')} sections, {sum(1 for i in items if i.get('type') == 'element')} √©l√©ments)")
            
            # Debug: afficher les items finaux
            if self.debug and items:
                print(f"\nDEBUG - Items finaux ({len(items)}):")
                for i, item in enumerate(items[:5]):
                    print(f"  {i}: Ligne {item.get('row', '?')}, Type: {item.get('type', '?')}, Data: {str(item.get('data', {}))[:100]}...")
                if len(items) > 5:
                    print(f"  ... et {len(items) - 5} autres")
            elif self.debug:
                print("\nDEBUG - Aucun item final d√©tect√© !")
            
            # 5. Traiter les items
            current_section_id = None
            for item in tqdm(items, desc="Import"):
                self.stats.total_rows += 1
                
                if item['type'] == 'section':
                    try:
                        current_section_id = self.create_section(lot_id, item['data'])
                    except Exception as e:
                        print(f"‚ùå Erreur section ligne {item['row']}: {e}")
                        self.stats.errors += 1
                
                elif item['type'] == 'element' and current_section_id:
                    try:
                        # Classification avanc√©e avec Gemini
                        if self.use_gemini:
                            description = item['data'].get('designation_exacte', '')
                            classification = self.classify_with_gemini(description)
                            item['data']['classification'] = classification
                        
                        self.create_element(current_section_id, item['data'], item['row'])
                    except Exception as e:
                        self.error_reporter.add_error(
                            filename=filename,
                            line_number=item['row'],
                            error_type="ELEMENT_PROCESSING_ERROR",
                            error_message=str(e),
                            raw_data=str(item['data'])
                        )
                        print(f"‚ùå Erreur √©l√©ment ligne {item['row']}: {e}")
                        self.stats.errors += 1
            
            # 6. Afficher les warnings de mapping si n√©cessaire
            if parser.mapping_confidence == 'low':
                print(f"\n‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è ATTENTION: MAPPING DE COLONNES PEU FIABLE ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è")
                print(f"Le mapping automatique des colonnes pour ce fichier est incertain.")
                print(f"V√©rifiez imp√©rativement le rapport d'erreurs: {self.error_reporter.error_file}")
                print(f"Consid√©rez utiliser le mapping manuel interactif √† la prochaine ex√©cution.")
            elif parser.mapping_confidence == 'medium':
                print(f"\n‚ö†Ô∏è Mapping automatique avec confiance moyenne.")
                print(f"V√©rifiez le rapport d'erreurs si n√©cessaire: {self.error_reporter.error_file}")
            
            # 7. Sauvegarder le rapport d'erreurs
            self.error_reporter.save_report()
            
            # 8. Afficher les statistiques
            print(f"\n‚úÖ Import termin√©:")
            if self.dry_run:
                print(f"   [DRY-RUN] Mode simulation - aucune donn√©e ins√©r√©e")
            
            # Afficher le mode utilis√©
            if self.use_gemini and not gemini_fallback_triggered:
                print(f"   üìä Mode: Gemini IA (appels: {self.stats.gemini_calls}, cache: {self.stats.cache_hits})")
            elif gemini_fallback_triggered:
                print(f"   üìä Mode: Fallback Automatique (Gemini ‚Üí Classique)")
                print(f"       Gemini: {self.stats.gemini_calls} appels avant √©chec")
            else:
                print(f"   üìä Mode: Analyse classique")
            
            print(f"   - Lots cr√©√©s: {self.stats.lots_created}, r√©utilis√©s: {self.stats.lots_reused}")
            print(f"   - Sections cr√©√©es: {self.stats.sections_created}, r√©utilis√©es: {self.stats.sections_reused}")
            print(f"   - √âl√©ments cr√©√©s: {self.stats.elements_created}")
            print(f"   - Erreurs: {self.stats.errors}")
            print(f"   - Confiance mapping: {parser.mapping_confidence}")
            
            if gemini_fallback_triggered:
                print(f"\nüí° INFO: Le fallback automatique a √©t√© activ√© car Gemini a rencontr√© des erreurs")
                print(f"    (quota d√©pass√©, erreur d'API, etc.). Les donn√©es ont √©t√© trait√©es")
                print(f"    avec la m√©thode classique pour garantir un r√©sultat complet.")
            
            if self.use_gemini:
                print(f"   - Appels Gemini: {self.stats.gemini_calls}")
                print(f"   - Cache hits: {self.stats.cache_hits}")
            
            return dpgf_id
            
        except Exception as e:
            # Ajouter l'erreur critique au rapport
            self.error_reporter.add_error(
                filename=filename,
                line_number=0,
                error_type="CRITICAL_ERROR",
                error_message=str(e),
                raw_data="Script failure"
            )
            self.error_reporter.save_report()
            
            print(f"‚ùå Erreur critique: {e}")
            import traceback
            traceback.print_exc()
            return None


def main():
    """Point d'entr√©e du script"""
    parser = argparse.ArgumentParser(description="Import complet d'un fichier DPGF avec mapping interactif")
    parser.add_argument("--file", required=True, help="Chemin du fichier Excel DPGF")
    parser.add_argument("--original-filename", help="Nom original du fichier (si diff√©rent du nom du fichier local)")
    parser.add_argument("--base-url", default="http://127.0.0.1:8000", help="URL de l'API")
    parser.add_argument("--dpgf-id", type=int, help="ID du DPGF existant (optionnel)")
    parser.add_argument("--lot-num", help="Num√©ro du lot (optionnel)")
    parser.add_argument("--gemini-key", help="Cl√© API Google Gemini pour la classification avanc√©e")
    parser.add_argument("--chunk-size", type=int, default=20, help="Taille des chunks pour l'analyse Gemini (d√©faut: 20)")
    parser.add_argument("--no-gemini", action="store_true", help="D√©sactiver l'utilisation de Gemini m√™me si la cl√© est fournie")
    parser.add_argument("--debug", action="store_true", help="Activer le mode debug pour plus d'informations")
    parser.add_argument("--dry-run", action="store_true", help="Mode simulation: analyse et preview sans insertion en base")
    parser.add_argument("--log-dir", default="logs", help="R√©pertoire pour les logs d√©taill√©s (par d√©faut: 'logs')")
    parser.add_argument("--verbose-logs", action="store_true", help="Activer la journalisation d√©taill√©e pour le diagnostic d'erreurs")
    
    args = parser.parse_args()
    
    # D√©terminer si on utilise Gemini
    use_gemini = args.gemini_key is not None and not args.no_gemini
    
    importer = DPGFImporter(
        base_url=args.base_url,
        gemini_key=args.gemini_key,
        use_gemini=use_gemini,
        chunk_size=args.chunk_size,
        debug=args.debug,
        dry_run=args.dry_run
    )
    
    # Utiliser le nom original pour la d√©tection si fourni
    file_path_for_detection = args.original_filename if args.original_filename else args.file
    
    importer.import_file(
        file_path=args.file,
        dpgf_id=args.dpgf_id,
        lot_num=args.lot_num,
        original_filename=file_path_for_detection
    )


if __name__ == "__main__":
    main()
