"""
Script d'analyse des performances simplifi√© pour identifier les goulots d'√©tranglement.
"""

import sys
import os
import time
import psutil
import pandas as pd
from pathlib import Path
import requests


def test_api_performance(base_url: str = "http://127.0.0.1:8000"):
    """Teste les performances de l'API"""
    print("üîç Test performances API...")
    
    results = {
        'base_url': base_url,
        'connectivity': False,
        'response_times': {},
        'errors': []
    }
    
    # Test de connectivit√©
    try:
        start = time.time()
        response = requests.get(f"{base_url}/docs", timeout=10)
        results['connectivity'] = response.status_code == 200
        results['response_times']['docs'] = time.time() - start
        print(f"   ‚úÖ Connectivit√© API: {results['response_times']['docs']:.3f}s")
    except Exception as e:
        results['errors'].append(f"Connectivit√©: {str(e)}")
        print(f"   ‚ùå Erreur connectivit√©: {e}")
    
    # Test des endpoints principaux
    endpoints = [
        '/api/v1/clients/',
        '/api/v1/dpgfs/',
        '/api/v1/lots/',
        '/api/v1/sections/',
        '/api/v1/elements/'
    ]
    
    for endpoint in endpoints:
        try:
            start = time.time()
            response = requests.get(f"{base_url}{endpoint}", timeout=10)
            response_time = time.time() - start
            results['response_times'][endpoint] = response_time
            
            status = "‚úÖ" if response_time < 1.0 else "‚ö†Ô∏è" if response_time < 5.0 else "‚ùå"
            print(f"   {status} {endpoint}: {response_time:.3f}s (HTTP {response.status_code})")
            
            if response.status_code not in [200, 404]:  # 404 peut √™tre normal si pas de donn√©es
                results['errors'].append(f"{endpoint}: HTTP {response.status_code}")
                
        except Exception as e:
            results['errors'].append(f"{endpoint}: {str(e)}")
            print(f"   ‚ùå {endpoint}: {e}")
    
    return results


def analyze_local_test_files():
    """Analyse les fichiers de test locaux pour identifier les probl√®mes de performance"""
    print("\\nüîç Analyse des fichiers de test locaux...")
    
    test_dir = Path("test_data")
    if not test_dir.exists():
        print("   ‚ö†Ô∏è R√©pertoire test_data non trouv√©")
        return []
    
    results = []
    excel_files = list(test_dir.glob("*.xlsx")) + list(test_dir.glob("*.xls"))
    
    for file_path in excel_files:
        print(f"   üìÅ Analyse: {file_path.name}")
        
        result = {
            'file_path': str(file_path),
            'file_name': file_path.name,
            'success': False,
            'error': None,
            'timing': {},
            'file_stats': {},
            'sheet_info': {}
        }
        
        try:
            start_time = time.time()
            
            # Stats du fichier
            file_size = file_path.stat().st_size
            result['file_stats'] = {
                'size_bytes': file_size,
                'size_mb': round(file_size / (1024 * 1024), 2)
            }
            
            # Analyse des feuilles Excel
            excel_file = pd.ExcelFile(file_path)
            sheet_names = excel_file.sheet_names
            result['sheet_info'] = {
                'sheet_count': len(sheet_names),
                'sheet_names': sheet_names,
                'sheets_data': {}
            }
            
            # Analyser chaque feuille
            for sheet_name in sheet_names:
                try:
                    sheet_start = time.time()
                    df = pd.read_excel(file_path, sheet_name=sheet_name)
                    sheet_time = time.time() - sheet_start
                    
                    result['sheet_info']['sheets_data'][sheet_name] = {
                        'rows': len(df),
                        'cols': len(df.columns),
                        'load_time': sheet_time,
                        'empty': len(df) == 0 or df.dropna(how='all').empty
                    }
                    
                except Exception as e:
                    result['sheet_info']['sheets_data'][sheet_name] = {
                        'error': str(e)
                    }
            
            total_time = time.time() - start_time
            result['timing']['total_time'] = total_time
            result['success'] = True
            
            # Identifier la feuille avec le plus de donn√©es
            max_rows = 0
            best_sheet = None
            for sheet_name, data in result['sheet_info']['sheets_data'].items():
                if 'rows' in data and data['rows'] > max_rows:
                    max_rows = data['rows']
                    best_sheet = sheet_name
            
            if best_sheet:
                print(f"      ‚úÖ {total_time:.2f}s - {result['file_stats']['size_mb']}MB")
                print(f"         Meilleure feuille: '{best_sheet}' ({max_rows} lignes)")
                
                if total_time > 5:
                    print(f"      ‚ö†Ô∏è LENT: {total_time:.1f}s")
                    
                # Identifier les feuilles vides (souvent "page de garde")
                empty_sheets = [name for name, data in result['sheet_info']['sheets_data'].items() 
                              if data.get('empty', False)]
                if empty_sheets:
                    print(f"         Feuilles vides: {empty_sheets}")
            else:
                print(f"      ‚ö†Ô∏è Aucune feuille avec donn√©es trouv√©e")
                
        except Exception as e:
            result['error'] = str(e)
            result['timing']['total_time'] = time.time() - start_time
            print(f"      ‚ùå Erreur: {e}")
        
        results.append(result)
    
    return results


def check_database_performance():
    """V√©rifie les performances de la base de donn√©es"""
    print("\\nüîç Test performance base de donn√©es...")
    
    base_url = "http://127.0.0.1:8000"
    
    # Test des requ√™tes courantes
    test_queries = [
        ("/api/v1/clients/", "Clients"),
        ("/api/v1/dpgfs/", "DPGFs"), 
        ("/api/v1/lots/", "Lots"),
        ("/api/v1/sections/", "Sections"),
        ("/api/v1/elements/?limit=10", "√âl√©ments (10 premiers)")
    ]
    
    for endpoint, desc in test_queries:
        try:
            start = time.time()
            response = requests.get(f"{base_url}{endpoint}", timeout=30)
            query_time = time.time() - start
            
            if response.status_code == 200:
                data = response.json()
                count = len(data) if isinstance(data, list) else 1
                status = "‚úÖ" if query_time < 2.0 else "‚ö†Ô∏è" if query_time < 10.0 else "‚ùå"
                print(f"   {status} {desc}: {query_time:.3f}s ({count} enregistrements)")
                
                if query_time > 5:
                    print(f"      ‚ö†Ô∏è Requ√™te lente - possible probl√®me d'index")
                    
            else:
                print(f"   ‚ùå {desc}: HTTP {response.status_code}")
                
        except Exception as e:
            print(f"   ‚ùå {desc}: {e}")


def analyze_import_bottlenecks():
    """Analyse les goulots d'√©tranglement potentiels"""
    print("\\nüîç Analyse des goulots d'√©tranglement...")
    
    bottlenecks = []
    
    # 1. M√©moire syst√®me
    memory = psutil.virtual_memory()
    if memory.percent > 80:
        bottlenecks.append("‚ö†Ô∏è M√©moire syst√®me √©lev√©e ({:.1f}%)".format(memory.percent))
    else:
        print(f"   ‚úÖ M√©moire syst√®me: {memory.percent:.1f}%")
    
    # 2. Espace disque
    disk = psutil.disk_usage('.')
    disk_percent = (disk.used / disk.total) * 100
    if disk_percent > 90:
        bottlenecks.append("‚ö†Ô∏è Espace disque faible ({:.1f}%)".format(disk_percent))
    else:
        print(f"   ‚úÖ Espace disque: {disk_percent:.1f}%")
    
    # 3. Processus Python actifs
    python_processes = []
    for proc in psutil.process_iter(['pid', 'name', 'cpu_percent', 'memory_percent']):
        try:
            if 'python' in proc.info['name'].lower():
                python_processes.append(proc.info)
        except:
            pass
    
    if len(python_processes) > 5:
        bottlenecks.append(f"‚ö†Ô∏è Nombreux processus Python actifs ({len(python_processes)})")
    else:
        print(f"   ‚úÖ Processus Python: {len(python_processes)}")
    
    # 4. Analyse des logs r√©cents
    log_files = ['dpgf_import.log', 'celery_tasks.log']
    for log_file in log_files:
        if os.path.exists(log_file):
            try:
                with open(log_file, 'r', encoding='utf-8', errors='ignore') as f:
                    lines = f.readlines()
                    if len(lines) > 1000:
                        recent_lines = lines[-100:]  # 100 derni√®res lignes
                        error_count = sum(1 for line in recent_lines if 'ERROR' in line or 'ERREUR' in line)
                        if error_count > 5:
                            bottlenecks.append(f"‚ö†Ô∏è Nombreuses erreurs r√©centes dans {log_file} ({error_count})")
                        else:
                            print(f"   ‚úÖ Log {log_file}: {error_count} erreurs r√©centes")
            except Exception as e:
                print(f"   ‚ö†Ô∏è Impossible de lire {log_file}: {e}")
    
    return bottlenecks


def generate_recommendations(api_results, file_results, bottlenecks):
    """G√©n√®re des recommandations d'optimisation"""
    print("\\nüí° RECOMMANDATIONS D'OPTIMISATION")
    print("=" * 50)
    
    recommendations = []
    
    # API Performance
    if not api_results['connectivity']:
        recommendations.append("üîß API non accessible - v√©rifier que l'API FastAPI est d√©marr√©e")
    
    slow_endpoints = [ep for ep, time in api_results.get('response_times', {}).items() if time > 2]
    if slow_endpoints:
        recommendations.append(f"üîß Endpoints lents d√©tect√©s: {slow_endpoints}")
        recommendations.append("   ‚Üí Ajouter des index sur les colonnes fr√©quemment requ√™t√©es")
        recommendations.append("   ‚Üí Impl√©menter la pagination pour les gros r√©sultats")
    
    # File Processing
    if file_results:
        slow_files = [r for r in file_results if r['success'] and r['timing']['total_time'] > 10]
        if slow_files:
            recommendations.append(f"üîß {len(slow_files)} fichier(s) lent(s) √† traiter (>10s)")
            recommendations.append("   ‚Üí Augmenter les timeouts dans l'orchestrateur")
            recommendations.append("   ‚Üí Impl√©menter un processing par chunks plus petit")
        
        # Analyser les feuilles multiples
        multi_sheet_files = [r for r in file_results if r['success'] and r['sheet_info']['sheet_count'] > 3]
        if multi_sheet_files:
            recommendations.append(f"üîß {len(multi_sheet_files)} fichier(s) avec nombreuses feuilles")
            recommendations.append("   ‚Üí Optimiser la d√©tection de la feuille de donn√©es")
    
    # System Bottlenecks
    for bottleneck in bottlenecks:
        recommendations.append(f"üîß {bottleneck}")
    
    # General Recommendations
    recommendations.extend([
        "",
        "üìã OPTIMISATIONS G√âN√âRALES:",
        "‚Ä¢ Impl√©menter un retry automatique avec backoff exponentiel",
        "‚Ä¢ Ajouter des m√©triques de monitoring (temps par √©tape)",
        "‚Ä¢ Utiliser un pool de workers pour le processing parall√®le",
        "‚Ä¢ Mettre en cache les patterns de classification Gemini",
        "‚Ä¢ Impl√©menter une queue Redis/Celery pour les gros imports"
    ])
    
    for rec in recommendations:
        print(rec)


def main():
    """Analyse compl√®te des performances"""
    print("üîç ANALYSE DE PERFORMANCE DPGF")
    print("=" * 50)
    
    # 1. Test API
    api_results = test_api_performance()
    
    # 2. Analyse fichiers locaux
    file_results = analyze_local_test_files()
    
    # 3. Test performance base de donn√©es
    check_database_performance()
    
    # 4. Analyse des goulots d'√©tranglement
    bottlenecks = analyze_import_bottlenecks()
    
    # 5. G√©n√©rer les recommandations
    generate_recommendations(api_results, file_results, bottlenecks)
    
    print("\\n" + "=" * 50)
    print("‚úÖ Analyse termin√©e")


if __name__ == "__main__":
    main()
